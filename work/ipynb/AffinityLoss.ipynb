{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorchの環境構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pyhocon\n",
    "import torch\n",
    "import argparse\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import glob\n",
    "import os, re, json\n",
    "import matplotlib.pyplot as plt\n",
    "import jsonlines\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 応答の対話行為推定モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url_repo = \"data/corpus/*\"\n",
    "data_url_dir = glob.glob(data_url_repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPUをdevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_token = '<EOS>'\n",
    "BOS_token = '<BOS>'\n",
    "file_pattern = re.compile(r'^sw\\_([a-z]+?)\\_([0-9]+?)\\.jsonlines$')\n",
    "\n",
    "swda_tagu = {\n",
    "    '<Uninterpretable>': ['abandoned_or_turn-exit/uninterpretable', 'non-verbal'],\n",
    "    '<Statement>': ['statement-non-opinion', 'statement-opinion', 'other_answers', '3rd-party-talk', 'self-talk', 'offers,_options_commits', 'collaborative_completion'],\n",
    "    '<Question>': ['q', 'yes-no-question', 'wh-question', 'declarative_yes-no-question', 'backchannel_in_question_form', 'open-question', 'rhetorical-questions', 'signal-non-understanding', 'or-clause', 'tag-question', 'declarative_wh-question'],\n",
    "    '<Directive>': ['action-directive'],\n",
    "    '<Greeting>': ['conventional-opening', 'conventional-closing'],\n",
    "    '<Apology>': ['apology', 'no_answers', 'reject', 'negative_non-no_answers', 'dispreferred_answers', 'dispreferred_answers'],\n",
    "    '<Agreement>': ['agree/accept', 'maybe/accept-part', 'thanking'],\n",
    "    '<Understanding>': ['acknowledge_(backchannel)', 'summarize/reformulate', 'appreciation', 'response_acknowledgement', 'affirmative_non-yes_answers', 'yes_answers'],\n",
    "    '<Other>': ['other', 'hedge', 'quotation', 'repeat-phrase', 'hold_before_answer/agreement', 'downplayer']\n",
    "}\n",
    "\n",
    "daily_tagu = {1: \"inform\", 2: \"question\", 3: \"directive\", 4: \"commissive\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 対話行為のID化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DA_to_ID:\n",
    "    \n",
    "    def __init__(self, config, X_DA, Y_DA, name):\n",
    "        self.word2id = None\n",
    "        self.id2word = None\n",
    "        self.config = config\n",
    "        self.X_DA = X_DA\n",
    "        self.Y_DA = Y_DA\n",
    "        self.name = name\n",
    "        self.construct()\n",
    "        \n",
    "    def construct(self):\n",
    "#        vocab = {'<PAD>': 0}\n",
    "        vocab = {}\n",
    "        vocab_count = {}\n",
    "        \n",
    "        for x,y in zip(self.X_DA, self.Y_DA):\n",
    "            for token in x:\n",
    "                if token in vocab_count:\n",
    "                    vocab_count[token] += 1\n",
    "                else:\n",
    "                    vocab_count[token] = 1\n",
    "                    \n",
    "            for token in y:\n",
    "                if token in vocab_count:\n",
    "                    vocab_count[token] += 1\n",
    "                else:\n",
    "                    vocab_count[token] = 1\n",
    "                    \n",
    "        for k, _ in sorted(vocab_count.items(), key=lambda x: -x[1]):\n",
    "            vocab[k] = len(vocab)\n",
    "            if len(vocab) >= self.config[self.name]['MAX_VOCAB']: break\n",
    "        self.word2id = vocab\n",
    "        self.id2word = {v : k for k, v in vocab.items()}\n",
    "        return vocab\n",
    "        \n",
    "    def tokenize(self, X_tensor, Y_tensor):\n",
    "        X_Tensor = [[self.word2id[token] for token in sentence] for sentence in X_tensor]\n",
    "        Y_Tensor = [[self.word2id[token] for token in sentence] for sentence in Y_tensor]\n",
    "        return X_Tensor, Y_Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 発話のID化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UTT_to_ID:\n",
    "    \n",
    "    def __init__(self, config, X_UTT, Y_UTT, name):\n",
    "        self.word2id = None\n",
    "        self.id2word = None\n",
    "        self.config = config\n",
    "        self.X_UTT = X_UTT\n",
    "        self.Y_UTT = Y_UTT\n",
    "        self.name = name\n",
    "        self.construct()\n",
    "        \n",
    "    def construct(self):\n",
    "        \n",
    "        vocab = {'<UNK>': 0, '<EOS>': 1, '<BOS>': 2, '<UttPAD>': 3, '<ConvPAD>': 4}\n",
    "        vocab_count = {}\n",
    "        \n",
    "        for x,y in zip(self.X_UTT, self.Y_UTT):\n",
    "            for seq in x:\n",
    "                for word in seq:\n",
    "                    if word in vocab_count:\n",
    "                        vocab_count[word] += 1\n",
    "                    else:\n",
    "                        vocab_count[word] = 1\n",
    "            for seq in y:\n",
    "                for word in seq:\n",
    "                    if word in vocab_count:\n",
    "                        vocab_count[word] += 1\n",
    "                    else:\n",
    "                        vocab_count[word] = 1\n",
    "                        \n",
    "        for k, _ in sorted(vocab_count.items(), key=lambda x: -x[1]):\n",
    "            vocab[k] = len(vocab)\n",
    "            if len(vocab) >= self.config[self.name]['UTT_MAX_VOCAB']: break\n",
    "        self.word2id = vocab\n",
    "        self.id2word = {v : k for k, v in vocab.items()}\n",
    "\n",
    "        return vocab\n",
    "        \n",
    "    def tokenize(self, X_tensor, Y_tensor):\n",
    "        \n",
    "        X_Tensor = [[[self.word2id[token] if token in self.word2id else self.word2id['<UNK>'] for token in seq] for seq in dialogue] for dialogue in X_tensor]\n",
    "        Y_Tensor = [[[self.word2id[token] if token in self.word2id else self.word2id['<UNK>'] for token in seq] for seq in dialogue] for dialogue in Y_tensor]\n",
    "        return X_Tensor, Y_Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### トレーニングデータ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_traindata(config, name):\n",
    "    files = [f for f in os.listdir(config[name]['train_path']) if file_pattern.match(f)]\n",
    "    # print(\"files:\" , files)\n",
    "    da_x, da_y, utt_x, utt_y, turn = [], [], [], [], []\n",
    "    # 1file 1conversation\n",
    "    for filename in files:\n",
    "        # print(os.path.join(config['train_path'], filename))\n",
    "        with open(os.path.join(config[name]['train_path'], filename), 'r') as f:\n",
    "            data = f.read().split('\\n')\n",
    "            data.remove('')\n",
    "            da_seq, utt_seq, turn_seq = [], [], []\n",
    "            # 1line 1turn\n",
    "            for idx, line in enumerate(data, 1):\n",
    "                jsondata = json.loads(line)\n",
    "                # single-turn multi dialogue case\n",
    "                if config[name]['multi_dialogue']:\n",
    "                    for da, utt in zip(jsondata['DA'], jsondata['sentence']):\n",
    "                        da_seq.append(da)\n",
    "                        utt_seq.append(utt.split(' '))\n",
    "                        turn_seq.append(0)\n",
    "                    if config[name]['turn']:\n",
    "                        da_seq.append('<turn>')\n",
    "                        utt_seq.append('<turn>')\n",
    "                    turn_seq[-1] = 1\n",
    "                # single-turn single dialogue case\n",
    "                else:\n",
    "                    da_seq.append(jsondata['DA'][-1])\n",
    "                    utt_seq.append(jsondata['sentence'][-1].split(' '))\n",
    "            da_seq = [easy_damsl(da) for da in da_seq]\n",
    "        \n",
    "            \n",
    "            # assert len(turn_seq) == len(da_seq), '{} != {}'.format(len(turn_seq), len(da_seq))\n",
    "        if config[name]['state']:\n",
    "            for i in range(max(1, len(da_seq) - 1 - config[name]['window_size'])):\n",
    "                da_x.append(da_seq[i:min(len(da_seq)-1, i + config[name]['window_size'])])\n",
    "                da_y.append(da_seq[1 + i:min(len(da_seq), 1 + i + config[name]['window_size'])])\n",
    "                utt_x.append(utt_seq[i:min(len(da_seq)-1, i + config[name]['window_size'])])\n",
    "                utt_y.append(utt_seq[1 + i:min(len(da_seq), 1 + i + config[name]['window_size'])])\n",
    "                turn.append(turn_seq[i:min(len(da_seq), i + config[name]['window_size'])])\n",
    "        else:\n",
    "            da_x.append(da_seq[:-1])\n",
    "            da_y.append(da_seq[1:])\n",
    "            utt_x.append(utt_seq[:-1])\n",
    "            utt_y.append(utt_seq[1:])\n",
    "            turn.append(turn_seq[:-1])\n",
    "    assert len(da_x) == len(da_y), 'Unexpect length da_posts and da_cmnts'\n",
    "    assert len(utt_x) == len(utt_y), 'Unexpect length utt_posts and utt_cmnts'\n",
    "    # assert len(turn) == len(da_posts)\n",
    "    \n",
    "    return da_x, da_y, utt_x, utt_y, turn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### タグ付"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def easy_damsl(tag):\n",
    "    easy_tag = [k for k, v in swda_tagu.items() if tag in v]\n",
    "    return easy_tag[0] if not len(easy_tag) < 1 else tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データの割合(8:1:1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(x, y, turn):\n",
    "    split_size = round(len(x) / 10)\n",
    "    if split_size == 0: split_size = 1\n",
    "    X_train, Y_train, Tturn = x[split_size * 2:], y[split_size * 2:], turn[split_size * 2:]\n",
    "    X_valid, Y_valid, Vturn = x[split_size: split_size * 2], y[split_size: split_size * 2], turn[split_size: split_size * 2]\n",
    "    X_test, Y_test, Testturn = x[:split_size], y[:split_size], turn[:split_size]\n",
    "    assert len(X_train) == len(Y_train), 'Unexpect to separate train data'\n",
    "    return X_train, Y_train, X_valid, Y_valid, X_test, Y_test, Tturn, Vturn, Testturn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_env(name):\n",
    "    config = pyhocon.ConfigFactory.parse_file('./dialogue.conf')\n",
    "    config['log_dirs'] = os.path.join(config[name]['log_dir'])\n",
    "    if not os.path.exists(config['log_dirs']):\n",
    "        os.mkdir(config['log_dirs'])\n",
    "     \n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データ取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DAdata(config, name):\n",
    "    posts, cmnts, _, _, turn = create_traindata(config, name)\n",
    "    X_train, Y_train, X_valid, Y_valid, X_test, Y_test, Tturn, Vturn, Testturn = separate_data(posts, cmnts, turn)\n",
    "    return X_train, Y_train, X_valid, Y_valid, X_test, Y_test, Tturn, Vturn, Testturn\n",
    "\n",
    "def create_Uttdata(config, name):\n",
    "    _, _, posts, cmnts, turn = create_traindata(config, name)\n",
    "    X_train, Y_train, X_valid, Y_valid, X_test, Y_test, _, _, _ = separate_data(posts, cmnts, turn)\n",
    "    return X_train, Y_train, X_valid, Y_valid, X_test, Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(mode_name, utt_vocab, da_vocab, config, device, lr, loss_func):\n",
    "    \n",
    "    if model_name == \"CmbAttention\":\n",
    "        if loss_func == \"ce\":\n",
    "            model = CmbAttentionModel(utt_vocab, da_vocab, config, device).to(device)\n",
    "            opt = optim.Adam(model.parameters(), lr)\n",
    "            loss_function = nn.CrossEntropyLoss(reduction='mean')\n",
    "        \n",
    "        elif loss_func == \"affinity\":\n",
    "            model = CmbAttentionModel_affinity(utt_vocab, da_vocab, config, device).to(device)\n",
    "            opt = optim.Adam(model.parameters(), lr)\n",
    "            loss_function = Affinity_Loss(0.75, len(da_vocab.word2id))\n",
    "    else:\n",
    "        model = None\n",
    "        opt = None\n",
    "        loss_function = None\n",
    "        \n",
    "    return model, opt, loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish preparing dataset...\n"
     ]
    }
   ],
   "source": [
    "# write = SummaryWriter(\"./logs\")\n",
    "\n",
    "model_name = 'CmbAttention'\n",
    "config = initialize_env(model_name)\n",
    "\n",
    "XDA_train, YDA_train, XDA_valid, YDA_valid, _, _, Tturn, Vturn, _ = create_DAdata(config, model_name)\n",
    "XUtt_train, YUtt_train, XUtt_valid, YUtt_valid, _, _ = create_Uttdata(config, model_name)\n",
    "\n",
    "DA_vocab = DA_to_ID(config, XDA_train+XDA_valid, YDA_train+YDA_valid, model_name)\n",
    "Utt_vocab = UTT_to_ID(config, XUtt_train+XUtt_valid, YUtt_train+YUtt_valid, model_name)\n",
    "\n",
    "XDA_train, YDA_train = DA_vocab.tokenize(XDA_train, YDA_train)\n",
    "XDA_valid, YDA_valid = DA_vocab.tokenize(XDA_valid, YDA_valid)\n",
    "XUtt_train, YUtt_train = Utt_vocab.tokenize(XUtt_train, YUtt_train)\n",
    "XUtt_valid, YUtt_valid = Utt_vocab.tokenize(XUtt_valid, YUtt_valid)\n",
    "\n",
    "print('Finish preparing dataset...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validation(X_valid, Y_valid, XU_valid, YU_valid, model, device, config, turn):\n",
    "\n",
    "    total_loss = 0\n",
    "    idx = 0\n",
    "    \n",
    "    for seq_idx in range(len(X_valid)):\n",
    "        print('\\r{}/{} conversation evaluating'.format(seq_idx+1, len(X_valid)), end='')\n",
    "        utter_hidden, context_hidden, da_hidden = model.initDAHidden(1)\n",
    "        X_seq = X_valid[seq_idx]\n",
    "        Y_seq = Y_valid[seq_idx]\n",
    "        turn[seq_idx] = turn[seq_idx] + [0] * (len(X_seq) - len(turn[seq_idx]))\n",
    "        turn_seq = turn[seq_idx]\n",
    "        XU_seq = XU_valid[seq_idx]\n",
    "        YU_seq = YU_valid[seq_idx]\n",
    "\n",
    "        assert len(X_seq) == len(Y_seq), 'Unexpect sequence len in evaluate {} != {}'.format(len(X_seq), len(Y_seq))\n",
    "        \n",
    "        for i in range(0, len(X_seq)):\n",
    "            X_tensor = torch.tensor([[X_seq[i]]]).to(device)\n",
    "            Y_tensor = torch.tensor([[Y_seq[i]]]).to(device)\n",
    "            turn_tensor = torch.tensor([[turn_seq[i]]]).to(device)\n",
    "            turn_tensor = turn_tensor.float()\n",
    "            turn_tensor = turn_tensor.unsqueeze(1)   \n",
    "            XU_tensor = torch.tensor([XU_seq[i]]).to(device)\n",
    "            \n",
    "            output, utter_hidden, context_hidden, da_hidden = model.validtion(XU_tensor, X_tensor, Y_tensor, None, utter_hidden, context_hidden, da_hidden, turn_tensor)\n",
    "            loss = loss_func(output, Y_tensor)\n",
    "            total_loss += loss\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer finish\n"
     ]
    }
   ],
   "source": [
    "_, _, _, _, XDA_test, YDA_test, _, _, DAturn = create_DAdata(config, model_name)\n",
    "_, _, _, _, XUtt_test, YUtt_test = create_Uttdata(config, model_name)\n",
    "\n",
    "XDA_test, YDA_test = DA_vocab.tokenize(XDA_test, YDA_test)\n",
    "XUtt_test, _ = Utt_vocab.tokenize(XUtt_test, YUtt_test)\n",
    "\n",
    "print('tokenizer finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def ModelTest():\n",
    "    result=[]\n",
    "    for seq_idx in range(0, len(XDA_test)):\n",
    "        print('\\r{}/{} conversation evaluating'.format(seq_idx+1, len(XDA_test)), end='')\n",
    "        XDA_seq = XDA_test[seq_idx]\n",
    "        YDA_seq = YDA_test[seq_idx]\n",
    "        DAturn[seq_idx] = DAturn[seq_idx] + [0] * (len(XDA_seq) - len(DAturn[seq_idx]))\n",
    "        DAturn_seq = DAturn[seq_idx]\n",
    "        XUtt_seq = XUtt_test[seq_idx]\n",
    "\n",
    "        pred_seq = []\n",
    "        true_seq = []\n",
    "        utter_hidden, context_hidden, da_hidden = models.initDAHidden(1)\n",
    "\n",
    "        for i in range(0, len(XDA_seq)):\n",
    "            XDA_tensor = torch.tensor([[XDA_seq[i]]]).to(device)\n",
    "            YDA_tensor = torch.tensor(YDA_seq[i]).to(device)\n",
    "            DAturn_tensor = torch.tensor([[DAturn_seq[i]]]).to(device)\n",
    "            DAturn_tensor = DAturn_tensor.float()\n",
    "            DAturn_tensor = DAturn_tensor.unsqueeze(1)\n",
    "            XUtt_tensor = torch.tensor([XUtt_seq[i]]).to(device)\n",
    "\n",
    "            output, utter_hidden, context_hidden, da_hidden, att_weights = models.testing(XUtt_tensor, XDA_tensor, None, utter_hidden, context_hidden, da_hidden, DAturn_tensor)\n",
    "                # output, utter_hidden, context_hidden, da_hidden = da_predict_model.prediction(XU_tensor, X_tensor, None, utter_hidden, context_hidden, da_hidden, turn_tensor)\n",
    "\n",
    "            pred_idx = torch.argmax(output)\n",
    "            pred_seq.append(pred_idx.item())\n",
    "            true_seq.append(YDA_tensor.item())\n",
    "            utter_list = [Utt_vocab.id2word[word] for word in XUtt_seq[i]]\n",
    "\n",
    "                # if ((552<=seq_idx and seq_idx<=557)  or (653<=seq_idx and seq_idx<=658) or (5508<=seq_idx and seq_idx<=5512) or  (8542<=seq_idx and seq_idx<=8547) or (8656<=seq_idx and seq_idx<=8661)):\n",
    "                #     visualize_attention_weights(att_weights.squeeze(0), seq_idx, i, experiment, utter_list)\n",
    "    #         if (da_vocab.id2word[true_seq[i]] == '<Apology>'):\n",
    "    #             visualize_attention_weights(att_weights.squeeze(0), seq_idx, i, experiment, utter_list)\n",
    "\n",
    "        result.append({'true': true_seq,\n",
    "                        'true_detok': [DA_vocab.id2word[token] for token in true_seq],\n",
    "                        'pred': pred_seq,\n",
    "                        'pred_detok': [DA_vocab.id2word[token] for token in pred_seq],\n",
    "                        'UttSeq': [[Utt_vocab.id2word[word] for word in sentence] for sentence in XUtt_seq],\n",
    "                        'seq_detok': [DA_vocab.id2word[label] for label in XDA_seq]})\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_average(y_true, y_pred):\n",
    "    p = precision_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    r = recall_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    f = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    acc = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    print('p: {} | r: {} | f: {} | acc: {}'.format(p, r, f, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル(Cmb Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CmbAttentionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, utt_vocab, da_vocab, config, device):\n",
    "        super(CmbAttentionModel, self).__init__()\n",
    "        \n",
    "        self.utter_encoder = UtteraceEncoder(len(utt_vocab.word2id), config['CmbAttention']['UTT_EMBED'], config['CmbAttention']['UTT_HIDDEN'])\n",
    "\n",
    "        self.context_encoder = RNNContextAwareEncoder(config['CmbAttention']['CON_EMBED'], config['CmbAttention']['CON_HIDDEN'])\n",
    "\n",
    "        self.da_encoder = RNNDAAwareEncoder(len(utt_vocab.word2id), config['CmbAttention']['DA_EMBED'], config['CmbAttention']['DA_HIDDEN'])\n",
    "\n",
    "        self.decoder = DenceDecoder(config['CmbAttention']['DA_HIDDEN'] + config['CmbAttention']['CON_HIDDEN'], config['CmbAttention']['DA_EMBED'], len(da_vocab.word2id))\n",
    "        \n",
    "#         self.weights = torch.tensor([1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]).cuda()\n",
    "\n",
    "#         self.cross_entropy_loss = nn.CrossEntropyLoss(reduction='mean', weight=self.weights)\n",
    "\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "#         self.affinity_loss = Affinity_Loss(0.75)\n",
    "\n",
    "        self.n_classes = len(da_vocab.word2id)\n",
    "\n",
    "#         self.affinity_cluster = ClusteringAffinity(n_classes=self.n_classes, n_centers, sigma, feat_dim)\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, X_utter, X_da, Y_da, mask, utter_hidden, context_hidden, da_hidden, turn):\n",
    "\n",
    "        utter_output, utter_weights = self.utter_encoder(X_utter, mask)\n",
    "\n",
    "        turn_output = torch.cat((utter_output, turn), dim=2)\n",
    "\n",
    "        context_output, context_weights, context_hidden = self.context_encoder(turn_output, mask, context_hidden)\n",
    "\n",
    "        da_output, da_weights, da_hidden = self.da_encoder(X_da, mask, da_hidden)\n",
    "\n",
    "        x_output = torch.cat((context_output, da_output), dim=2)\n",
    "\n",
    "        dence_output = self.decoder(x_output)\n",
    "\n",
    "        output = dence_output.squeeze(1)  # (batch_size, da_dim)\n",
    "\n",
    "        loss = self.cross_entropy_loss(output, Y_da)\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        return loss.item(), utter_hidden, context_hidden, da_hidden\n",
    "    \n",
    "    def validtion(self, X_utter, X_da, Y_da, mask, utter_hidden, context_hidden, da_hidden, turn):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            utter_output, utter_weights = self.utter_encoder(X_utter, mask)\n",
    "\n",
    "            turn_output = torch.cat((utter_output, turn), dim=2)\n",
    "\n",
    "            context_output, context_weights, context_hidden = self.context_encoder(turn_output, mask, context_hidden)\n",
    "\n",
    "            da_output, da_weights, da_hidden = self.da_encoder(X_da, mask, da_hidden)\n",
    "\n",
    "            x_output = torch.cat((context_output, da_output), dim=2)\n",
    "\n",
    "            dence_output = self.decoder(x_output)\n",
    "\n",
    "            output = dence_output.squeeze(1)  # (batch_size, da_dim)\n",
    "            \n",
    "            Y_da = Y_da.squeeze(0)\n",
    "            \n",
    "            loss = self.affinity_loss(output, Y_da)\n",
    "\n",
    "\n",
    "        return loss.item(), utter_hidden, context_hidden, da_hidden\n",
    "\n",
    "\n",
    "    def testing(self, X_utter, X_da, mask, utter_hidden, context_hidden, da_hidden, turn):\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            utter_output, utter_weights = self.utter_encoder(X_utter, mask)\n",
    "\n",
    "            turn_output = torch.cat((utter_output, turn), dim=2)\n",
    "\n",
    "            context_output, context_weights, context_hidden = self.context_encoder(turn_output, mask, context_hidden)\n",
    "\n",
    "            da_output, da_weights, da_hidden = self.da_encoder(X_da, mask, da_hidden)\n",
    "\n",
    "            x_output = torch.cat((context_output, da_output), dim=2)\n",
    "\n",
    "            dence_output = self.decoder(x_output)\n",
    "\n",
    "            output = dence_output.squeeze(1)  # (batch_size, da_dim)\n",
    "\n",
    "        return output, utter_hidden, context_hidden, da_hidden, utter_weights\n",
    "\n",
    "\n",
    "    def initDAHidden(self, batch_size):\n",
    "        return self.utter_encoder.initHidden(batch_size, self.device), self.context_encoder.initHidden(batch_size, self.device), self.da_encoder.initHidden(batch_size, self.device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル(Cmb Attention) ver AffinityLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CmbAttentionModel_affinity(nn.Module):\n",
    "    \n",
    "    def __init__(self, utt_vocab, da_vocab, config, device):\n",
    "        super(CmbAttentionModel_affinity, self).__init__()\n",
    "        \n",
    "        self.utter_encoder = UtteraceEncoder(len(utt_vocab.word2id), config['CmbAttention']['UTT_EMBED'], config['CmbAttention']['UTT_HIDDEN'])\n",
    "\n",
    "        self.context_encoder = RNNContextAwareEncoder(config['CmbAttention']['CON_EMBED'], config['CmbAttention']['CON_HIDDEN'])\n",
    "\n",
    "        self.da_encoder = RNNDAAwareEncoder(len(utt_vocab.word2id), config['CmbAttention']['DA_EMBED'], config['CmbAttention']['DA_HIDDEN'])\n",
    "\n",
    "        self.decoder = DenceDecoder(config['CmbAttention']['DA_HIDDEN'] + config['CmbAttention']['CON_HIDDEN'], config['CmbAttention']['DA_EMBED'], len(da_vocab.word2id))\n",
    "        \n",
    "        self.n_classes = len(da_vocab.word2id)\n",
    "\n",
    "        self.affinity_cluster = ClusteringAffinity(n_classes=self.n_classes, n_centers=1, sigma=90.0, feat_dim=9)\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, X_utter, X_da, Y_da, mask, utter_hidden, context_hidden, da_hidden, turn):\n",
    "\n",
    "        utter_output, utter_weights = self.utter_encoder(X_utter, mask)\n",
    "\n",
    "        turn_output = torch.cat((utter_output, turn), dim=2)\n",
    "\n",
    "        context_output, context_weights, context_hidden = self.context_encoder(turn_output, mask, context_hidden)\n",
    "\n",
    "        da_output, da_weights, da_hidden = self.da_encoder(X_da, mask, da_hidden)\n",
    "\n",
    "        x_output = torch.cat((context_output, da_output), dim=2)\n",
    "\n",
    "        dence_output = self.decoder(x_output)\n",
    "\n",
    "        output = dence_output.squeeze(1)  # (batch_size, da_dim)\n",
    "\n",
    "#         Y_da = torch.eye(self.n_classes)[Y_da[:,-1]]\n",
    "        \n",
    "#         output = output.to(self.device)\n",
    "#         Y_da = Y_da.to(self.device)\n",
    "\n",
    "        output = self.affinity_cluster(output)\n",
    "\n",
    "#         loss.backward(retain_graph=True)\n",
    "\n",
    "        return output, utter_hidden, context_hidden, da_hidden\n",
    "    \n",
    "    def validtion(self, X_utter, X_da, Y_da, mask, utter_hidden, context_hidden, da_hidden, turn):\n",
    "\n",
    "        utter_output, utter_weights = self.utter_encoder(X_utter, mask)\n",
    "\n",
    "        turn_output = torch.cat((utter_output, turn), dim=2)\n",
    "\n",
    "        context_output, context_weights, context_hidden = self.context_encoder(turn_output, mask, context_hidden)\n",
    "\n",
    "        da_output, da_weights, da_hidden = self.da_encoder(X_da, mask, da_hidden)\n",
    "\n",
    "        x_output = torch.cat((context_output, da_output), dim=2)\n",
    "\n",
    "        dence_output = self.decoder(x_output)\n",
    "\n",
    "        output = dence_output.squeeze(1)  # (batch_size, da_dim)\n",
    "            \n",
    "        output = self.affinity_cluster(output)\n",
    "\n",
    "        return output, utter_hidden, context_hidden, da_hidden\n",
    "\n",
    "\n",
    "    def testing(self, X_utter, X_da, mask, utter_hidden, context_hidden, da_hidden, turn):\n",
    "\n",
    "        utter_output, utter_weights = self.utter_encoder(X_utter, mask)\n",
    "\n",
    "        turn_output = torch.cat((utter_output, turn), dim=2)\n",
    "\n",
    "        context_output, context_weights, context_hidden = self.context_encoder(turn_output, mask, context_hidden)\n",
    "\n",
    "        da_output, da_weights, da_hidden = self.da_encoder(X_da, mask, da_hidden)\n",
    "\n",
    "        x_output = torch.cat((context_output, da_output), dim=2)\n",
    "\n",
    "        dence_output = self.decoder(x_output)\n",
    "\n",
    "        output = dence_output.squeeze(1)  # (batch_size, da_dim)\n",
    "\n",
    "        return output, utter_hidden, context_hidden, da_hidden, utter_weights\n",
    "\n",
    "\n",
    "    def initDAHidden(self, batch_size):\n",
    "        return self.utter_encoder.initHidden(batch_size, self.device), self.context_encoder.initHidden(batch_size, self.device), self.da_encoder.initHidden(batch_size, self.device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, w_model):\n",
    "        super(WordEmbedding, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.linear = nn.Linear(embed_size, w_model)\n",
    "\n",
    "    def forward(self, x_word):\n",
    "        return torch.tanh(self.linear(self.word_embedding(x_word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DA Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, da_size, embed_size, d_model):\n",
    "        super(DAEmbedding, self).__init__()\n",
    "        self.da_embedding = nn.Embedding(da_size, embed_size)\n",
    "        self.linear = nn.Linear(embed_size, d_model)\n",
    "\n",
    "    def forward(self, x_da):\n",
    "        return torch.tanh(self.linear(self.da_embedding(x_da)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model):\n",
    "        super(Attention, self).__init__()\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        self.d_k = d_model\n",
    "       \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # 全結合層で特徴量を変換\n",
    "        k = self.k_linear(k)\n",
    "        q = self.q_linear(q)\n",
    "        v = self.v_linear(v)\n",
    "\n",
    "        # Attentionの値を計算する\n",
    "        # 各値を足し算すると大きくなりすぎるので、root(d_k)で割って調整\n",
    "        weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # ここでmaskを計算\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            weights = weights.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # softmaxで規格化をする\n",
    "        attention_weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        # AttentionをValueとかけ算\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "\n",
    "        # 全結合層で特徴量を変換\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, hidden_size, att_size):\n",
    "        super(ContextAttention, self).__init__()\n",
    "        self.q_linear = nn.Linear(att_size, att_size)\n",
    "        self.v_linear = nn.Linear(att_size, att_size)\n",
    "        self.k_linear = nn.Linear(att_size, att_size)\n",
    "\n",
    "        self.fc_1 = nn.Linear(d_model, d_model)\n",
    "        self.fc_3 = nn.Linear(hidden_size, d_model, bias=True)\n",
    "        self.fc_2 = nn.Linear(d_model, att_size)\n",
    "\n",
    "        self.fc_out = nn.Linear(att_size, hidden_size, bias=True)\n",
    "        self.d_k = att_size\n",
    "\n",
    "    def forward(self, x, mask, hidden):\n",
    "        \n",
    "        x = self.fc_2(torch.tanh(self.fc_1(x) + self.fc_3(hidden)))\n",
    "\n",
    "        q = self.q_linear(x)\n",
    "        v = self.v_linear(x)\n",
    "        k = self.k_linear(x)\n",
    "\n",
    "        weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # ここでmaskを計算\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            weights = weights.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attention_weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        att_output = torch.matmul(attention_weights, v)\n",
    "\n",
    "        output = self.fc_out(att_output)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.linear_1(x)\n",
    "\n",
    "        x = self.dropout(F.relu(x))\n",
    "\n",
    "        x = self.linear_2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositinalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len, dropout=0.1):\n",
    "        super(PositinalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenceDecoder(nn.Module):\n",
    "    def __init__(self, da_hidden, da_embed_size, da_input_size):\n",
    "        super(DenceDecoder, self).__init__()\n",
    "        self.he = nn.Linear(da_hidden, da_embed_size)\n",
    "        self.ey = nn.Linear(da_embed_size, da_input_size)\n",
    "\n",
    "    def forward(self, hidden):\n",
    "        pred = self.ey(torch.tanh(self.he(hidden)))\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-Attention => RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNContextAwareEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_dim, d_model):\n",
    "        super(RNNContextAwareEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(self.d_model+1, self.d_model)\n",
    "        self.rnn = nn.GRU(self.d_model, self.d_model, batch_first=True)\n",
    "        self.attention = ContextAttention(self.d_model, self.d_model, self.d_model)\n",
    "        self.ffn = FeedForward(self.d_model, self.d_model)\n",
    "\n",
    "    def forward(self, x, mask, hidden):\n",
    "\n",
    "        lin_output = self.linear(x)\n",
    "\n",
    "        att_output, att_weights = self.attention(lin_output, mask, hidden.transpose(0,1))        \n",
    "\n",
    "        rnn_output, rnn_hidden = self.rnn(att_output, hidden)\n",
    "\n",
    "        ffn_output = self.ffn(rnn_output)\n",
    "\n",
    "        return ffn_output, att_weights, rnn_hidden\n",
    "\n",
    "    def initHidden(self, batch_size, device):\n",
    "        return torch.zeros(1, batch_size, self.d_model).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DA Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-Attention => RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDAAwareEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, da_size, emb_dim, d_model):\n",
    "        super(RNNDAAwareEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = DAEmbedding(da_size, emb_dim, self.d_model)\n",
    "        self.rnn = nn.GRU(self.d_model, self.d_model, batch_first=True)\n",
    "        # self.attention = ContextAwareAttention(self.d_model, self.d_model, self.d_model)\n",
    "        self.attention = ContextAttention(self.d_model, self.d_model, self.d_model)\n",
    "        self.ffn = FeedForward(self.d_model, emb_dim)\n",
    "\n",
    "    def forward(self, X_da, mask, hidden):\n",
    "\n",
    "        emb_output = self.embedding(X_da)\n",
    "\n",
    "        att_output, att_weights = self.attention(emb_output, mask, hidden.transpose(0,1))        \n",
    "\n",
    "        rnn_output, rnn_hidden = self.rnn(att_output, hidden)\n",
    "\n",
    "        ffn_output = self.ffn(rnn_output)\n",
    "\n",
    "        return ffn_output, att_weights, rnn_hidden\n",
    "\n",
    "    def initHidden(self, batch_size, device):\n",
    "        return torch.zeros(1, batch_size, self.d_model).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utterance Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PE => Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UtteraceEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_dim, d_model):\n",
    "        super(UtteraceEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = WordEmbedding(vocab_size, emb_dim, self.d_model)\n",
    "        self.pe = PositinalEncoding(self.d_model, 200)\n",
    "        self.att = Attention(self.d_model)\n",
    "        self.ffn = FeedForward(d_model, emb_dim)\n",
    "        \n",
    "    def forward(self, x_utter, mask):\n",
    "\n",
    "        emb_output = self.embedding(x_utter)\n",
    "\n",
    "        pos_output = self.pe(emb_output)\n",
    "\n",
    "        att_output, att_weights = self.att(pos_output, pos_output, pos_output, mask)\n",
    "\n",
    "        ffn_output = self.ffn(att_output)\n",
    "\n",
    "        seq_len = ffn_output.size()[1]\n",
    "\n",
    "        avg_output = F.avg_pool2d(ffn_output, (seq_len, 1)) # => (128, 1, 512)\n",
    "\n",
    "        return avg_output, att_weights  # 発話ベクトル(128, 1, 512)\n",
    "\n",
    "    def initHidden(self, batch_size, device):\n",
    "        return torch.zeros(1, batch_size, self.d_model).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affinity Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringAffinity(nn.Module):\n",
    "    def __init__(self,n_classes, n_centers, sigma, feat_dim, init_weight=True):\n",
    "        super(ClusteringAffinity,self).__init__()\n",
    "        self.n_classes=n_classes\n",
    "        self.n_centers=n_centers\n",
    "        self.feat_dim=feat_dim\n",
    "        self.sigma=sigma\n",
    "        self.centers=nn.Parameter(torch.randn(self.n_classes,self.n_centers,self.feat_dim),requires_grad=True)\n",
    "        # self.my_registered_parameter=nn.ParameterList([self.centers])\n",
    "        if init_weight:\n",
    "            self.__init_weight()\n",
    "\n",
    "    # init the centers\n",
    "    def __init_weight(self):\n",
    "        nn.init.kaiming_normal_(self.centers)\n",
    "\n",
    "\n",
    "    def forward(self,f):\n",
    "        \n",
    "        f_expand = f.unsqueeze(1).unsqueeze(1)\n",
    "        w_expand = self.centers.unsqueeze(0)\n",
    "        fw_norm = torch.sum((f_expand-w_expand)**2,-1)\n",
    "        distance = torch.exp(-fw_norm/self.sigma)\n",
    "        distance = torch.max(distance,-1)[0]\n",
    "        # self.centers.zeros_grad()\n",
    "        #Regularization\n",
    "        mc = self.n_centers*self.n_classes\n",
    "        w_reshape = self.centers.view(mc,self.feat_dim)\n",
    "        w_reshape_expand1 = w_reshape.unsqueeze(0)\n",
    "        w_reshape_expand2 = w_reshape.unsqueeze(1)\n",
    "        w_norm_mat = torch.sum((w_reshape_expand2-w_reshape_expand1)**2,-1)\n",
    "        w_norm_upper = self.upper_triangle(w_norm_mat)\n",
    "        mu = 2.0/(mc**2-mc)*w_norm_upper.sum()\n",
    "        residuals = self.upper_triangle((w_norm_upper-mu)**2)\n",
    "        rw = 2.0/(mc**2-mc)*residuals.sum()\n",
    "        # rw=residuals.sum()\n",
    "        batch_size = f.size(0)\n",
    "        rw_broadcast = torch.ones((batch_size,1)).to(device)*rw\n",
    "        output = torch.cat((distance,rw_broadcast),dim=-1)\n",
    "        return output\n",
    "\n",
    "    def upper_triangle(self,metrix):\n",
    "        upper=torch.triu(metrix)\n",
    "        diagonal=torch.diag(metrix)\n",
    "        diagonal_mask=torch.sign(torch.abs(torch.diag(diagonal)))\n",
    "        return upper*(1.0-diagonal_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affinity_Loss(nn.Module):\n",
    "    def __init__(self,lambd,n_classes):\n",
    "        super(Affinity_Loss,self).__init__()\n",
    "        self.lamda=lambd\n",
    "        self.n_classes = n_classes + 1\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        \n",
    "        target = torch.eye(self.n_classes)[target[:,-1]]\n",
    "        \n",
    "        onehot = target[:,:-1]\n",
    "        \n",
    "        distance = output[:,:-1]\n",
    "        \n",
    "        rw = torch.mean(output[:,-1])\n",
    "         \n",
    "        d_fi_wyi = torch.sum(onehot*distance, -1).unsqueeze(1)\n",
    "        \n",
    "        losses = torch.clamp(self.lamda + distance - d_fi_wyi, min=0)\n",
    "        \n",
    "        L_mm = torch.sum(losses*(1.0-onehot),-1)/target.size(0)\n",
    "        \n",
    "        loss = torch.sum(L_mm + rw, -1)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルトレーニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = config[model_name]['lr']\n",
    "batch_size = config[model_name]['BATCH_SIZE']\n",
    "plot_train_losses = []\n",
    "plot_valid_losses = []\n",
    "loss_name = \"affinity\"\n",
    "    \n",
    "print_total_loss = 0\n",
    "plot_total_loss = 0\n",
    "plot_total_acc = 0\n",
    "_valid_loss = None\n",
    "\n",
    "models, optims, loss_func = select_model(model_name, Utt_vocab, DA_vocab, config, device, lr, loss_name)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start TRAINING\n",
      "Epoch 1 start\n",
      "Conversation 66089/160028 training..."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-f63c88272aa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mYUtt_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutter_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mda_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXUtt_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXda_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYda_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutter_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mda_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mturn_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYda_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-2274a1e8d4ab>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X_utter, X_da, Y_da, mask, utter_hidden, context_hidden, da_hidden, turn)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mcontext_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mturn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mda_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mda_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mda_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mda_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_da\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mda_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mda_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-8e88a09a3e36>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X_da, mask, hidden)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0memb_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_da\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0matt_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mrnn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matt_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-9220cbf1a342>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask, hidden)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1e9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0matt_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"start TRAINING\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "config[model_name]['EPOCH'] = 30\n",
    "\n",
    "for epoch in range(config[model_name]['EPOCH']):\n",
    "    \n",
    "    tmp_time = time.time()\n",
    "    print('Epoch {} start'.format(epoch+1))\n",
    "    index = [i for i in range(len(XDA_train))]\n",
    "    random.shuffle(index)\n",
    "    idx=0\n",
    "    \n",
    "    while idx < len(index):\n",
    "        step_size = min(batch_size, len(index)-idx)\n",
    "        batch_idx = index[idx:idx+step_size]\n",
    "        optims.zero_grad()\n",
    "        utter_hidden, context_hidden, da_hidden = models.initDAHidden(step_size)\n",
    "        \n",
    "        print('\\rConversation {}/{} training...'.format(idx + step_size, len(XDA_train)), end='')\n",
    "        Xda_seq = [XDA_train[seq_idx] for seq_idx in batch_idx]\n",
    "        Yda_seq = [YDA_train[seq_idx] for seq_idx in batch_idx]\n",
    "        turn_seq = [Tturn[seq_idx] for seq_idx in batch_idx]\n",
    "        max_conv_len = max(len(s) for s in Xda_seq) \n",
    "        \n",
    "        ## \n",
    "        XUtt_seq = [XUtt_train[seq_idx] for seq_idx in batch_idx]\n",
    "        YUtt_seq = [YUtt_train[seq_idx] for seq_idx in batch_idx]\n",
    "            \n",
    "        for i in range(len(XUtt_seq)):\n",
    "            XUtt_seq[i] = XUtt_seq[i] + [[Utt_vocab.word2id['<ConvPAD>']]] * (max_conv_len - len(XUtt_seq[i]))\n",
    "            YUtt_seq[i] = YUtt_seq[i] + [[Utt_vocab.word2id['<ConvPAD>']]] * (max_conv_len - len(YUtt_seq[i]))\n",
    "                \n",
    "        for ci in range(len(Xda_seq)):\n",
    "\n",
    "            turn_seq[ci] = turn_seq[ci] + [0] * (max_conv_len - len(turn_seq[ci]))\n",
    "            Xda_seq[ci] = Xda_seq[ci] + [-1] * (max_conv_len - len(Xda_seq[ci]))\n",
    "            Yda_seq[ci] = Yda_seq[ci] + [-1] * (max_conv_len - len(Yda_seq[ci]))\n",
    "            \n",
    "        for i in range(0, max_conv_len):\n",
    "            last = True if i == max_conv_len - 1 else False\n",
    "            \n",
    "            Xda_tensor = torch.tensor([[X[i]] for X in Xda_seq]).to(device)\n",
    "            Yda_tensor = torch.tensor([[Y[i]] for Y in Yda_seq]).to(device)\n",
    "            turn_tensor = torch.tensor([[t[i]] for t in turn_seq]).to(device)\n",
    "            turn_tensor = turn_tensor.float()\n",
    "            turn_tensor = turn_tensor.unsqueeze(1)    \n",
    "                \n",
    "            max_seq_len = max(len(XU[i]) + 1 for XU in XUtt_seq)\n",
    "            for ci in range(len(XUtt_seq)):\n",
    "                XUtt_seq[ci][i] = XUtt_seq[ci][i] + [Utt_vocab.word2id['<UttPAD>']] * (max_seq_len - len(XUtt_seq[ci][i]))\n",
    "                YUtt_seq[ci][i] = YUtt_seq[ci][i] + [Utt_vocab.word2id['<UttPAD>']] * (max_seq_len - len(YUtt_seq[ci][i]))\n",
    "            XUtt_tensor = torch.tensor([XU[i] for XU in XUtt_seq]).to(device)\n",
    "            YUtt_tensor = None\n",
    "\n",
    "            output, utter_hidden, context_hidden, da_hidden = models(XUtt_tensor, Xda_tensor, Yda_tensor, None, utter_hidden, context_hidden, da_hidden, turn_tensor)\n",
    "            loss = loss_func(output.to(device), Yda_tensor.to(device))\n",
    "            \n",
    "            print_total_loss += loss\n",
    "                \n",
    "            if last:\n",
    "                optims.step()\n",
    "                \n",
    "        idx += step_size\n",
    "        \n",
    "    valid_loss = validation(XDA_valid, YDA_valid, XUtt_valid, YUtt_valid, models, device, config, Vturn)\n",
    "    \n",
    "    \n",
    "    def save_model(filename):\n",
    "        torch.save(models.state_dict(), os.path.join(config[model_name]['log_dir'], config[model_name]['SAVE_NAME'] + \"_\" + str(config[model_name]['window_size']) + \".model\".format(filename)))\n",
    "        \n",
    "    print(\"steps %d\\tloss %.4f\\tvalid loss %.4f | exec time %.4f\" % (epoch+1, print_total_loss, valid_loss, time.time()-tmp_time))\n",
    "    print_total_loss = 0\n",
    "    \n",
    "    if _valid_loss == None:\n",
    "        save_model(\"model_save\")\n",
    "        print(\"Model Saved\")\n",
    "        _valid_loss = valid_loss\n",
    "    else:\n",
    "        if valid_loss<_valid_loss:\n",
    "            _valid_loss = valid_loss\n",
    "            save_model(\"model_save\")\n",
    "            print(\"Model Saved\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, _ = select_model(model_name, Utt_vocab, DA_vocab, config, device, lr)\n",
    "models.load_state_dict(torch.load(os.path.join(config[model_name]['log_dir'], config[model_name]['SAVE_NAME'] + \"_\" + str(config[model_name]['window_size']) + \".model\".format('model_save'))))\n",
    "\n",
    "print('model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ModelTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = [label for line in result for label in line['true']]\n",
    "pred = [label for line in result for label in line['pred']]\n",
    "\n",
    "calc_average(y_true=true, y_pred=pred)\n",
    "f = f1_score(y_true=true, y_pred=pred, average=None)\n",
    "r = recall_score(y_true=true, y_pred=pred, average=None)\n",
    "p = precision_score(y_true=true, y_pred=pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
