{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Cost Sensitive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pyhocon\n",
    "import torch\n",
    "import argparse\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import glob\n",
    "import os, re, json\n",
    "import matplotlib.pyplot as plt\n",
    "import jsonlines\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ データ処理\n",
    "    + データのロード\n",
    "    + データの前処理\n",
    "    + データの分割\n",
    "+ モデルの構築\n",
    "    + Cmb Attentionモデル\n",
    "    + 損失関数の定義\n",
    "        + tf-idf形式のコスト損失関数の構築\n",
    "        \n",
    "+ モデルの訓練\n",
    "    + モデルの学習\n",
    "    + モデルの精度評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データのディレクトリを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url_repo = \"data/corpus/*\"\n",
    "data_url_dir = glob.glob(data_url_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_token = '<EOS>'\n",
    "BOS_token = '<BOS>'\n",
    "file_pattern = re.compile(r'^sw\\_([a-z]+?)\\_([0-9]+?)\\.jsonlines$')\n",
    "\n",
    "swda_tagu = {\n",
    "    '<Uninterpretable>': ['abandoned_or_turn-exit/uninterpretable', 'non-verbal'],\n",
    "    '<Statement>': ['statement-non-opinion', 'statement-opinion', 'other_answers', '3rd-party-talk', 'self-talk', 'offers,_options_commits', 'collaborative_completion'],\n",
    "    '<Question>': ['q', 'yes-no-question', 'wh-question', 'declarative_yes-no-question', 'backchannel_in_question_form', 'open-question', 'rhetorical-questions', 'signal-non-understanding', 'or-clause', 'tag-question', 'declarative_wh-question'],\n",
    "    '<Directive>': ['action-directive'],\n",
    "    '<Greeting>': ['conventional-opening', 'conventional-closing'],\n",
    "    '<Apology>': ['apology', 'no_answers', 'reject', 'negative_non-no_answers', 'dispreferred_answers', 'dispreferred_answers'],\n",
    "    '<Agreement>': ['agree/accept', 'maybe/accept-part', 'thanking'],\n",
    "    '<Understanding>': ['acknowledge_(backchannel)', 'summarize/reformulate', 'appreciation', 'response_acknowledgement', 'affirmative_non-yes_answers', 'yes_answers'],\n",
    "    '<Other>': ['other', 'hedge', 'quotation', 'repeat-phrase', 'hold_before_answer/agreement', 'downplayer']\n",
    "}\n",
    "\n",
    "daily_tagu = {1: \"inform\", 2: \"question\", 3: \"directive\", 4: \"commissive\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 対話行為のID化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DA_to_ID:\n",
    "    \n",
    "    def __init__(self, config, X_DA, Y_DA, name):\n",
    "        self.word2id = None\n",
    "        self.id2word = None\n",
    "        self.config = config\n",
    "        self.X_DA = X_DA\n",
    "        self.Y_DA = Y_DA\n",
    "        self.name = name\n",
    "        self.construct()\n",
    "        \n",
    "    def construct(self):\n",
    "#        vocab = {'<PAD>': 0}\n",
    "        vocab = {}\n",
    "        vocab_count = {}\n",
    "        \n",
    "        for x,y in zip(self.X_DA, self.Y_DA):\n",
    "            for token in x:\n",
    "                if token in vocab_count:\n",
    "                    vocab_count[token] += 1\n",
    "                else:\n",
    "                    vocab_count[token] = 1\n",
    "                    \n",
    "            for token in y:\n",
    "                if token in vocab_count:\n",
    "                    vocab_count[token] += 1\n",
    "                else:\n",
    "                    vocab_count[token] = 1\n",
    "                    \n",
    "        for k, _ in sorted(vocab_count.items(), key=lambda x: -x[1]):\n",
    "            vocab[k] = len(vocab)\n",
    "            if len(vocab) >= self.config[self.name]['MAX_VOCAB']: break\n",
    "        self.word2id = vocab\n",
    "        self.id2word = {v : k for k, v in vocab.items()}\n",
    "        return vocab\n",
    "        \n",
    "    def tokenize(self, X_tensor, Y_tensor):\n",
    "        X_Tensor = [[self.word2id[token] for token in sentence] for sentence in X_tensor]\n",
    "        Y_Tensor = [[self.word2id[token] for token in sentence] for sentence in Y_tensor]\n",
    "        return X_Tensor, Y_Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 発話のID化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UTT_to_ID:\n",
    "    \n",
    "    def __init__(self, config, X_UTT, Y_UTT, name):\n",
    "        self.word2id = None\n",
    "        self.id2word = None\n",
    "        self.config = config\n",
    "        self.X_UTT = X_UTT\n",
    "        self.Y_UTT = Y_UTT\n",
    "        self.name = name\n",
    "        self.construct()\n",
    "        \n",
    "    def construct(self):\n",
    "        \n",
    "        vocab = {'<UNK>': 0, '<EOS>': 1, '<BOS>': 2, '<UttPAD>': 3, '<ConvPAD>': 4}\n",
    "        vocab_count = {}\n",
    "        \n",
    "        for x,y in zip(self.X_UTT, self.Y_UTT):\n",
    "            for seq in x:\n",
    "                for word in seq:\n",
    "                    if word in vocab_count:\n",
    "                        vocab_count[word] += 1\n",
    "                    else:\n",
    "                        vocab_count[word] = 1\n",
    "            for seq in y:\n",
    "                for word in seq:\n",
    "                    if word in vocab_count:\n",
    "                        vocab_count[word] += 1\n",
    "                    else:\n",
    "                        vocab_count[word] = 1\n",
    "                        \n",
    "        for k, _ in sorted(vocab_count.items(), key=lambda x: -x[1]):\n",
    "            vocab[k] = len(vocab)\n",
    "            if len(vocab) >= self.config[self.name]['UTT_MAX_VOCAB']: break\n",
    "        self.word2id = vocab\n",
    "        self.id2word = {v : k for k, v in vocab.items()}\n",
    "\n",
    "        return vocab\n",
    "        \n",
    "    def tokenize(self, X_tensor, Y_tensor):\n",
    "        \n",
    "        X_Tensor = [[[self.word2id[token] if token in self.word2id else self.word2id['<UNK>'] for token in seq] for seq in dialogue] for dialogue in X_tensor]\n",
    "        Y_Tensor = [[[self.word2id[token] if token in self.word2id else self.word2id['<UNK>'] for token in seq] for seq in dialogue] for dialogue in Y_tensor]\n",
    "        return X_Tensor, Y_Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニングデータ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_traindata(config, name):\n",
    "    files = [f for f in os.listdir(config[name]['train_path']) if file_pattern.match(f)]\n",
    "    # print(\"files:\" , files)\n",
    "    da_x, da_y, utt_x, utt_y, turn = [], [], [], [], []\n",
    "    # 1file 1conversation\n",
    "    for filename in files:\n",
    "        # print(os.path.join(config['train_path'], filename))\n",
    "        with open(os.path.join(config[name]['train_path'], filename), 'r') as f:\n",
    "            data = f.read().split('\\n')\n",
    "            data.remove('')\n",
    "            da_seq, utt_seq, turn_seq = [], [], []\n",
    "            # 1line 1turn\n",
    "            for idx, line in enumerate(data, 1):\n",
    "                jsondata = json.loads(line)\n",
    "                # single-turn multi dialogue case\n",
    "                if config[name]['multi_dialogue']:\n",
    "                    for da, utt in zip(jsondata['DA'], jsondata['sentence']):\n",
    "                        da_seq.append(da)\n",
    "                        utt_seq.append(utt.split(' '))\n",
    "                        turn_seq.append(0)\n",
    "                    if config[name]['turn']:\n",
    "                        da_seq.append('<turn>')\n",
    "                        utt_seq.append('<turn>')\n",
    "                    turn_seq[-1] = 1\n",
    "                # single-turn single dialogue case\n",
    "                else:\n",
    "                    da_seq.append(jsondata['DA'][-1])\n",
    "                    utt_seq.append(jsondata['sentence'][-1].split(' '))\n",
    "            da_seq = [easy_damsl(da) for da in da_seq]\n",
    "        \n",
    "            \n",
    "            # assert len(turn_seq) == len(da_seq), '{} != {}'.format(len(turn_seq), len(da_seq))\n",
    "        if config[name]['state']:\n",
    "            for i in range(max(1, len(da_seq) - 1 - config[name]['window_size'])):\n",
    "                da_x.append(da_seq[i:min(len(da_seq)-1, i + config[name]['window_size'])])\n",
    "                da_y.append(da_seq[1 + i:min(len(da_seq), 1 + i + config[name]['window_size'])])\n",
    "                utt_x.append(utt_seq[i:min(len(da_seq)-1, i + config[name]['window_size'])])\n",
    "                utt_y.append(utt_seq[1 + i:min(len(da_seq), 1 + i + config[name]['window_size'])])\n",
    "                turn.append(turn_seq[i:min(len(da_seq), i + config[name]['window_size'])])\n",
    "        else:\n",
    "            da_x.append(da_seq[:-1])\n",
    "            da_y.append(da_seq[1:])\n",
    "            utt_x.append(utt_seq[:-1])\n",
    "            utt_y.append(utt_seq[1:])\n",
    "            turn.append(turn_seq[:-1])\n",
    "    assert len(da_x) == len(da_y), 'Unexpect length da_posts and da_cmnts'\n",
    "    assert len(utt_x) == len(utt_y), 'Unexpect length utt_posts and utt_cmnts'\n",
    "    # assert len(turn) == len(da_posts)\n",
    "    \n",
    "    return da_x, da_y, utt_x, utt_y, turn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### タグ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def easy_damsl(tag):\n",
    "    easy_tag = [k for k, v in swda_tagu.items() if tag in v]\n",
    "    return easy_tag[0] if not len(easy_tag) < 1 else tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(x, y, turn):\n",
    "    split_size = round(len(x) / 10)\n",
    "    if split_size == 0: split_size = 1\n",
    "    X_train, Y_train, Tturn = x[split_size * 2:], y[split_size * 2:], turn[split_size * 2:]\n",
    "    X_valid, Y_valid, Vturn = x[split_size: split_size * 2], y[split_size: split_size * 2], turn[split_size: split_size * 2]\n",
    "    X_test, Y_test, Testturn = x[:split_size], y[:split_size], turn[:split_size]\n",
    "    assert len(X_train) == len(Y_train), 'Unexpect to separate train data'\n",
    "    return X_train, Y_train, X_valid, Y_valid, X_test, Y_test, Tturn, Vturn, Testturn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練の準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_env(name):\n",
    "    config = pyhocon.ConfigFactory.parse_file('./dialogue.conf')\n",
    "    config['log_dirs'] = os.path.join(config[name]['log_dir'])\n",
    "    if not os.path.exists(config['log_dirs']):\n",
    "        os.mkdir(config['log_dirs'])\n",
    "     \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DAdata(config, name):\n",
    "    posts, cmnts, _, _, turn = create_traindata(config, name)\n",
    "    X_train, Y_train, X_valid, Y_valid, X_test, Y_test, Tturn, Vturn, Testturn = separate_data(posts, cmnts, turn)\n",
    "    return X_train, Y_train, X_valid, Y_valid, X_test, Y_test, Tturn, Vturn, Testturn\n",
    "\n",
    "def create_Uttdata(config, name):\n",
    "    _, _, posts, cmnts, turn = create_traindata(config, name)\n",
    "    X_train, Y_train, X_valid, Y_valid, X_test, Y_test, _, _, _ = separate_data(posts, cmnts, turn)\n",
    "    return X_train, Y_train, X_valid, Y_valid, X_test, Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish preparing dataset...\n"
     ]
    }
   ],
   "source": [
    "write = SummaryWriter(\"./logs\")\n",
    "\n",
    "model_name = 'CmbAttention'\n",
    "# loss_name = \"Affinity\"\n",
    "loss_name = \"CE\"\n",
    "# loss_name = \"TFIDF\"\n",
    "config = initialize_env(model_name+loss_name)\n",
    "\n",
    "XDA_train, YDA_train, XDA_valid, YDA_valid, _, _, Tturn, Vturn, _ = create_DAdata(config, model_name+loss_name)\n",
    "XUtt_train, YUtt_train, XUtt_valid, YUtt_valid, _, _ = create_Uttdata(config, model_name+loss_name)\n",
    "\n",
    "DA_vocab = DA_to_ID(config, XDA_train+XDA_valid, YDA_train+YDA_valid, model_name+loss_name)\n",
    "Utt_vocab = UTT_to_ID(config, XUtt_train+XUtt_valid, YUtt_train+YUtt_valid, model_name+loss_name)\n",
    "\n",
    "XDA_train, YDA_train = DA_vocab.tokenize(XDA_train, YDA_train)\n",
    "XDA_valid, YDA_valid = DA_vocab.tokenize(XDA_valid, YDA_valid)\n",
    "XUtt_train, YUtt_train = Utt_vocab.tokenize(XUtt_train, YUtt_train)\n",
    "XUtt_valid, YUtt_valid = Utt_vocab.tokenize(XUtt_valid, YUtt_valid)\n",
    "\n",
    "print('Finish preparing dataset...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cmb Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CmbAttentionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_name, utt_vocab, da_vocab, config, device):\n",
    "        super(CmbAttentionModel, self).__init__()\n",
    "        \n",
    "        self.utter_encoder = UtteraceEncoder(len(utt_vocab.word2id), config[model_name]['UTT_EMBED'], config[model_name]['UTT_HIDDEN'])\n",
    "\n",
    "        self.context_encoder = RNNContextAwareEncoder(config[model_name]['CON_EMBED'], config[model_name]['CON_HIDDEN'])\n",
    "\n",
    "        self.da_encoder = RNNDAAwareEncoder(len(utt_vocab.word2id), config[model_name]['DA_EMBED'], config[model_name]['DA_HIDDEN'])\n",
    "\n",
    "        self.de_encoder = DenceEncoder(config[model_name]['DA_HIDDEN'] + config[model_name]['CON_HIDDEN'], config[model_name]['DA_EMBED'], len(da_vocab.word2id))\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, X_utter, X_da, Y_da, mask, utter_hidden, context_hidden, da_hidden, turn):\n",
    "\n",
    "        utter_output, utter_weights = self.utter_encoder(X_utter, mask)\n",
    "\n",
    "        turn_output = torch.cat((utter_output, turn), dim=2)\n",
    "\n",
    "        context_output, context_weights, context_hidden = self.context_encoder(turn_output, mask, context_hidden)\n",
    "\n",
    "        da_output, da_weights, da_hidden = self.da_encoder(X_da, mask, da_hidden)\n",
    "\n",
    "        x_output = torch.cat((context_output, da_output), dim=2)\n",
    "\n",
    "        dence_output = self.de_encoder(x_output)\n",
    "\n",
    "        output = dence_output.squeeze(1)  # (batch_size, da_dim)\n",
    "        \n",
    "        Y_da = Y_da.squeeze()\n",
    "        \n",
    "        return output, utter_hidden, context_hidden, da_hidden\n",
    "    \n",
    "    def validtion(self, X_utter, X_da, Y_da, mask, utter_hidden, context_hidden, da_hidden, turn):\n",
    "\n",
    "        utter_output, utter_weights = self.utter_encoder(X_utter, mask)\n",
    "\n",
    "        turn_output = torch.cat((utter_output, turn), dim=2)\n",
    "\n",
    "        context_output, context_weights, context_hidden = self.context_encoder(turn_output, mask, context_hidden)\n",
    "\n",
    "        da_output, da_weights, da_hidden = self.da_encoder(X_da, mask, da_hidden)\n",
    "\n",
    "        x_output = torch.cat((context_output, da_output), dim=2)\n",
    "\n",
    "        dence_output = self.de_encoder(x_output)\n",
    "\n",
    "        output = dence_output.squeeze(1)  # (batch_size, da_dim)\n",
    "            \n",
    "        Y_da = Y_da.squeeze(0)\n",
    "\n",
    "        return output, utter_hidden, context_hidden, da_hidden, Y_da\n",
    "\n",
    "\n",
    "    def prediction(self, X_utter, X_da, mask, utter_hidden, context_hidden, da_hidden, turn):\n",
    "\n",
    "        utter_output, utter_weights = self.utter_encoder(X_utter, mask)\n",
    "\n",
    "        turn_output = torch.cat((utter_output, turn), dim=2)\n",
    "\n",
    "        context_output, context_weights, context_hidden = self.context_encoder(turn_output, mask, context_hidden)\n",
    "\n",
    "        da_output, da_weights, da_hidden = self.da_encoder(X_da, mask, da_hidden)\n",
    "\n",
    "        x_output = torch.cat((context_output, da_output), dim=2)\n",
    "\n",
    "        dence_output = self.de_encoder(x_output)\n",
    "\n",
    "        output = dence_output.squeeze(1)  # (batch_size, da_dim)\n",
    "\n",
    "        return output, utter_hidden, context_hidden, da_hidden, utter_weights\n",
    "\n",
    "\n",
    "    def initDAHidden(self, batch_size):\n",
    "        return self.utter_encoder.initHidden(batch_size, self.device), self.context_encoder.initHidden(batch_size, self.device), self.da_encoder.initHidden(batch_size, self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, w_model):\n",
    "        super(WordEmbedding, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.linear = nn.Linear(embed_size, w_model)\n",
    "\n",
    "    def forward(self, x_word):\n",
    "        return torch.tanh(self.linear(self.word_embedding(x_word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, da_size, embed_size, d_model):\n",
    "        super(DAEmbedding, self).__init__()\n",
    "        self.da_embedding = nn.Embedding(da_size, embed_size)\n",
    "        self.linear = nn.Linear(embed_size, d_model)\n",
    "\n",
    "    def forward(self, x_da):\n",
    "        return torch.tanh(self.linear(self.da_embedding(x_da)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model):\n",
    "        super(Attention, self).__init__()\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        self.d_k = d_model\n",
    "       \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # 全結合層で特徴量を変換\n",
    "        k = self.k_linear(k)\n",
    "        q = self.q_linear(q)\n",
    "        v = self.v_linear(v)\n",
    "\n",
    "        # Attentionの値を計算する\n",
    "        # 各値を足し算すると大きくなりすぎるので、root(d_k)で割って調整\n",
    "        weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # ここでmaskを計算\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            weights = weights.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # softmaxで規格化をする\n",
    "        attention_weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        # AttentionをValueとかけ算\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "\n",
    "        # 全結合層で特徴量を変換\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, hidden_size, att_size):\n",
    "        super(ContextAttention, self).__init__()\n",
    "        self.q_linear = nn.Linear(att_size, att_size)\n",
    "        self.v_linear = nn.Linear(att_size, att_size)\n",
    "        self.k_linear = nn.Linear(att_size, att_size)\n",
    "\n",
    "        self.fc_1 = nn.Linear(d_model, d_model)\n",
    "        self.fc_3 = nn.Linear(hidden_size, d_model, bias=True)\n",
    "        self.fc_2 = nn.Linear(d_model, att_size)\n",
    "\n",
    "        self.fc_out = nn.Linear(att_size, hidden_size, bias=True)\n",
    "        self.d_k = att_size\n",
    "\n",
    "    def forward(self, x, mask, hidden):\n",
    "        \n",
    "        x = self.fc_2(torch.tanh(self.fc_1(x) + self.fc_3(hidden)))\n",
    "\n",
    "        q = self.q_linear(x)\n",
    "        v = self.v_linear(x)\n",
    "        k = self.k_linear(x)\n",
    "\n",
    "        weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # ここでmaskを計算\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            weights = weights.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attention_weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        att_output = torch.matmul(attention_weights, v)\n",
    "\n",
    "        output = self.fc_out(att_output)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.linear_1(x)\n",
    "\n",
    "        x = self.dropout(F.relu(x))\n",
    "\n",
    "        x = self.linear_2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositinalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len, dropout=0.1):\n",
    "        super(PositinalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenceEncoder(nn.Module):\n",
    "    def __init__(self, da_hidden, da_embed_size, da_input_size):\n",
    "        super(DenceEncoder, self).__init__()\n",
    "        self.he = nn.Linear(da_hidden, da_embed_size)\n",
    "        self.ey = nn.Linear(da_embed_size, da_input_size)\n",
    "\n",
    "    def forward(self, hidden):\n",
    "        pred = self.ey(torch.tanh(self.he(hidden)))\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNContextAwareEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_dim, d_model):\n",
    "        super(RNNContextAwareEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(self.d_model+1, self.d_model)\n",
    "        self.rnn = nn.GRU(self.d_model, self.d_model, batch_first=True)\n",
    "        self.attention = ContextAttention(self.d_model, self.d_model, self.d_model)\n",
    "        self.ffn = FeedForward(self.d_model, self.d_model)\n",
    "\n",
    "    def forward(self, x, mask, hidden):\n",
    "\n",
    "        lin_output = self.linear(x)\n",
    "\n",
    "        att_output, att_weights = self.attention(lin_output, mask, hidden.transpose(0,1))        \n",
    "\n",
    "        rnn_output, rnn_hidden = self.rnn(att_output, hidden)\n",
    "\n",
    "        ffn_output = self.ffn(rnn_output)\n",
    "\n",
    "        return ffn_output, att_weights, rnn_hidden\n",
    "\n",
    "    def initHidden(self, batch_size, device):\n",
    "        return torch.zeros(1, batch_size, self.d_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDAAwareEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, da_size, emb_dim, d_model):\n",
    "        super(RNNDAAwareEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = DAEmbedding(da_size, emb_dim, self.d_model)\n",
    "        self.rnn = nn.GRU(self.d_model, self.d_model, batch_first=True)\n",
    "        # self.attention = ContextAwareAttention(self.d_model, self.d_model, self.d_model)\n",
    "        self.attention = ContextAttention(self.d_model, self.d_model, self.d_model)\n",
    "        self.ffn = FeedForward(self.d_model, emb_dim)\n",
    "\n",
    "    def forward(self, X_da, mask, hidden):\n",
    "\n",
    "        emb_output = self.embedding(X_da)\n",
    "\n",
    "        att_output, att_weights = self.attention(emb_output, mask, hidden.transpose(0,1))        \n",
    "\n",
    "        rnn_output, rnn_hidden = self.rnn(att_output, hidden)\n",
    "\n",
    "        ffn_output = self.ffn(rnn_output)\n",
    "\n",
    "        return ffn_output, att_weights, rnn_hidden\n",
    "\n",
    "    def initHidden(self, batch_size, device):\n",
    "        return torch.zeros(1, batch_size, self.d_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UtteraceEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_dim, d_model):\n",
    "        super(UtteraceEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = WordEmbedding(vocab_size, emb_dim, self.d_model)\n",
    "        self.pe = PositinalEncoding(self.d_model, 200)\n",
    "        self.att = Attention(self.d_model)\n",
    "        self.ffn = FeedForward(d_model, emb_dim)\n",
    "        \n",
    "    def forward(self, x_utter, mask):\n",
    "\n",
    "        emb_output = self.embedding(x_utter)\n",
    "\n",
    "        pos_output = self.pe(emb_output)\n",
    "\n",
    "        att_output, att_weights = self.att(pos_output, pos_output, pos_output, mask)\n",
    "\n",
    "        ffn_output = self.ffn(att_output)\n",
    "\n",
    "        seq_len = ffn_output.size()[1]\n",
    "\n",
    "        avg_output = F.avg_pool2d(ffn_output, (seq_len, 1)) # => (128, 1, 512)\n",
    "\n",
    "        return avg_output, att_weights  # 発話ベクトル(128, 1, 512)\n",
    "\n",
    "    def initHidden(self, batch_size, device):\n",
    "        return torch.zeros(1, batch_size, self.d_model).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClacTF(DA_num, dialogue_len):\n",
    "    return DA_num/dialogue_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClacIDF(sum_num, dialogue_num):\n",
    "    return np.log(sum_num/(dialogue_num+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_weight=[1]*9\n",
    "count_dialogue=[0]*9\n",
    "for i in range(9):\n",
    "    for j in range(len(XDA_train)):\n",
    "        if i in XDA_train[j]:\n",
    "            count_dialogue[i] += 1\n",
    "#         count_num[i] += XDA_train[j].count(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_num = [0]*9\n",
    "for i in range(9):\n",
    "    for j in XDA_train:\n",
    "        if i in j:\n",
    "            means_num[i]+=1\n",
    "            classes_weight[i] += ClacIDF(len(XDA_train), count_dialogue[i])*ClacTF(j.count(i), len(j))\n",
    "            \n",
    "            \n",
    "for i in range(9):\n",
    "    classes_weight[i]/=means_num[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, dialogue_size, batch_size):\n",
    "        super(TFIDFLoss, self).__init__()\n",
    "        self.dialogue_size = dialogue_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def forward(self, output, target):\n",
    "        return \n",
    "        \n",
    "    def clacTF(DA_num):\n",
    "        return DA_num/self.dialogue_size\n",
    "    \n",
    "    def clacIDF(dialogue_num):\n",
    "        return np.log(self.batch_size/(dialogue_num+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights=torch.tensor(classes_weight, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(mode_name, utt_vocab, da_vocab, config, device, lr, loss_func):\n",
    "    \n",
    "    if model_name == \"CmbAttention\":\n",
    "        \n",
    "        if loss_func == \"CE\":\n",
    "            model = CmbAttentionModel(model_name+loss_func, utt_vocab, da_vocab, config, device).to(device)\n",
    "            opt = optim.Adam(model.parameters(), lr)\n",
    "            loss_function = nn.CrossEntropyLoss(reduction='mean')\n",
    "        \n",
    "        elif loss_func == \"Affinity\":\n",
    "            model = CmbAttentionModel_affinity(model_name+loss_func, utt_vocab, da_vocab, config, device).to(device)\n",
    "            opt = optim.Adam(model.parameters(), lr)\n",
    "            loss_function = Affinity_Loss(0.95, len(da_vocab.word2id), device)\n",
    "            \n",
    "        elif loss_func == \"TFIDF\":\n",
    "            model = CmbAttentionModel(model_name+loss_func, utt_vocab, da_vocab, config, device).to(device)\n",
    "            opt = optim.Adam(model.parameters(), lr)\n",
    "            loss_function = nn.CrossEntropyLoss(reduction='mean', weight=class_weights).to(device)\n",
    "            \n",
    "    else:\n",
    "        model = None\n",
    "        opt = None\n",
    "        loss_function = None\n",
    "        \n",
    "    return model, opt, loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = config[model_name+loss_name]['lr']\n",
    "batch_size = config[model_name+loss_name]['BATCH_SIZE']\n",
    "plot_train_losses = []\n",
    "plot_valid_losses = []\n",
    "# loss_name = \"Affinity\"\n",
    "# loss_name = \"CE\"\n",
    "# loss_name = \"TFIDF\"\n",
    "    \n",
    "print_total_loss = 0\n",
    "plot_total_loss = 0\n",
    "plot_total_acc = 0\n",
    "_valid_loss = None\n",
    "\n",
    "models, optims, loss_func = select_model(model_name, Utt_vocab, DA_vocab, config, device, lr, loss_name)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validation(X_valid, Y_valid, XU_valid, YU_valid, model, device, config, turn):\n",
    "\n",
    "    total_loss = 0\n",
    "    idx = 0\n",
    "    \n",
    "    for seq_idx in range(len(X_valid)):\n",
    "        print('\\r{}/{} conversation evaluating'.format(seq_idx+1, len(X_valid)), end='')\n",
    "        utter_hidden, context_hidden, da_hidden = model.initDAHidden(1)\n",
    "        X_seq = X_valid[seq_idx]\n",
    "        Y_seq = Y_valid[seq_idx]\n",
    "        turn[seq_idx] = turn[seq_idx] + [0] * (len(X_seq) - len(turn[seq_idx]))\n",
    "        turn_seq = turn[seq_idx]\n",
    "        XU_seq = XU_valid[seq_idx]\n",
    "        YU_seq = YU_valid[seq_idx]\n",
    "\n",
    "        assert len(X_seq) == len(Y_seq), 'Unexpect sequence len in evaluate {} != {}'.format(len(X_seq), len(Y_seq))\n",
    "        \n",
    "        for i in range(0, len(X_seq)):\n",
    "            X_tensor = torch.tensor([[X_seq[i]]]).to(device)\n",
    "            Y_tensor = torch.tensor([[Y_seq[i]]]).to(device)\n",
    "            turn_tensor = torch.tensor([[turn_seq[i]]]).to(device)\n",
    "            turn_tensor = turn_tensor.float()\n",
    "            turn_tensor = turn_tensor.unsqueeze(1)   \n",
    "            XU_tensor = torch.tensor([XU_seq[i]]).to(device)\n",
    "            \n",
    "            output, utter_hidden, context_hidden, da_hidden, Y_tensor = model.validtion(XU_tensor, X_tensor, Y_tensor, None, utter_hidden, context_hidden, da_hidden, turn_tensor)\n",
    "            loss = loss_func(output, Y_tensor)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        total_loss/=5\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start TRAINING\n",
      "Epoch 1 start\n",
      "20004/20004 conversation evaluatingsteps 1\tloss 1.2765\tvalid loss 1.9334 | exec time 556.3386\n",
      "Model Saved\n",
      "Epoch 2 start\n",
      "20004/20004 conversation evaluatingsteps 2\tloss 1.4217\tvalid loss 1.7572 | exec time 622.0164\n",
      "Model Saved\n",
      "Epoch 3 start\n",
      "586/20004 conversation evaluatingng..."
     ]
    }
   ],
   "source": [
    "print(\"start TRAINING\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "config[model_name+loss_name]['EPOCH'] = 30\n",
    "\n",
    "for epoch in range(config[model_name+loss_name]['EPOCH']):\n",
    "    \n",
    "    tmp_time = time.time()\n",
    "    print('Epoch {} start'.format(epoch+1))\n",
    "    index = [i for i in range(len(XDA_train))]\n",
    "    random.shuffle(index)\n",
    "    idx=0\n",
    "    \n",
    "    while idx < len(index):\n",
    "        step_size = min(batch_size, len(index)-idx)\n",
    "        batch_idx = index[idx:idx+step_size]\n",
    "        utter_hidden, context_hidden, da_hidden = models.initDAHidden(step_size)\n",
    "        \n",
    "        print('\\rConversation {}/{} training...'.format(idx + step_size, len(XDA_train)), end='')\n",
    "        Xda_seq = [XDA_train[seq_idx] for seq_idx in batch_idx]\n",
    "        Yda_seq = [YDA_train[seq_idx] for seq_idx in batch_idx]\n",
    "        turn_seq = [Tturn[seq_idx] for seq_idx in batch_idx]\n",
    "        max_conv_len = max(len(s) for s in Xda_seq) \n",
    "        \n",
    "        ## \n",
    "        XUtt_seq = [XUtt_train[seq_idx] for seq_idx in batch_idx]\n",
    "        YUtt_seq = [YUtt_train[seq_idx] for seq_idx in batch_idx]\n",
    "            \n",
    "        for i in range(len(XUtt_seq)):\n",
    "            XUtt_seq[i] = XUtt_seq[i] + [[Utt_vocab.word2id['<ConvPAD>']]] * (max_conv_len - len(XUtt_seq[i]))\n",
    "            YUtt_seq[i] = YUtt_seq[i] + [[Utt_vocab.word2id['<ConvPAD>']]] * (max_conv_len - len(YUtt_seq[i]))\n",
    "                \n",
    "        for ci in range(len(Xda_seq)):\n",
    "\n",
    "            turn_seq[ci] = turn_seq[ci] + [0] * (max_conv_len - len(turn_seq[ci]))\n",
    "            Xda_seq[ci] = Xda_seq[ci] + [-1] * (max_conv_len - len(Xda_seq[ci]))\n",
    "            Yda_seq[ci] = Yda_seq[ci] + [-1] * (max_conv_len - len(Yda_seq[ci]))\n",
    "            \n",
    "        for i in range(0, max_conv_len):\n",
    "            last = True if i == max_conv_len - 1 else False\n",
    "            \n",
    "            Xda_tensor = torch.tensor([[X[i]] for X in Xda_seq]).to(device)\n",
    "            Yda_tensor = torch.tensor([[Y[i]] for Y in Yda_seq]).to(device)\n",
    "            turn_tensor = torch.tensor([[t[i]] for t in turn_seq]).to(device)\n",
    "            turn_tensor = turn_tensor.float()\n",
    "            turn_tensor = turn_tensor.unsqueeze(1)    \n",
    "                \n",
    "            max_seq_len = max(len(XU[i]) + 1 for XU in XUtt_seq)\n",
    "            for ci in range(len(XUtt_seq)):\n",
    "                XUtt_seq[ci][i] = XUtt_seq[ci][i] + [Utt_vocab.word2id['<UttPAD>']] * (max_seq_len - len(XUtt_seq[ci][i]))\n",
    "                YUtt_seq[ci][i] = YUtt_seq[ci][i] + [Utt_vocab.word2id['<UttPAD>']] * (max_seq_len - len(YUtt_seq[ci][i]))\n",
    "            XUtt_tensor = torch.tensor([XU[i] for XU in XUtt_seq]).to(device)\n",
    "            YUtt_tensor = None\n",
    "\n",
    "            output, utter_hidden, context_hidden, da_hidden = models(XUtt_tensor, Xda_tensor, Yda_tensor, None, utter_hidden, context_hidden, da_hidden, turn_tensor)\n",
    "            Yda_tensor = Yda_tensor.squeeze(1)\n",
    "            loss = loss_func(output, Yda_tensor)\n",
    "            optims.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            print_total_loss += loss.item()\n",
    "                \n",
    "            if last:\n",
    "                optims.step()\n",
    "        print_total_loss/=max_conv_len      \n",
    "        idx += step_size\n",
    "        \n",
    "    valid_loss = validation(XDA_valid, YDA_valid, XUtt_valid, YUtt_valid, models, device, config, Vturn)\n",
    "    \n",
    "    \n",
    "    def save_model(filename):\n",
    "        torch.save(models.state_dict(), os.path.join(config[model_name+loss_name]['log_dir'], config[model_name+loss_name]['SAVE_NAME'] + \"_\" + str(config[model_name+loss_name]['window_size']) + \".model\".format(filename)))\n",
    "    \n",
    "    print_total_loss/(max_conv_len*idx)\n",
    "    print(\"steps %d\\tloss %.4f\\tvalid loss %.4f | exec time %.4f\" % (epoch+1, print_total_loss, valid_loss, time.time()-tmp_time))\n",
    "    print_total_loss = 0\n",
    "    \n",
    "    if _valid_loss == None:\n",
    "        save_model(\"model_save\")\n",
    "        print(\"Model Saved\")\n",
    "        _valid_loss = valid_loss\n",
    "    else:\n",
    "        if valid_loss<_valid_loss:\n",
    "            _valid_loss = valid_loss\n",
    "            save_model(\"model_save\")\n",
    "            print(\"Model Saved\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, _, XDA_test, YDA_test, _, _, DAturn = create_DAdata(config, model_name)\n",
    "_, _, _, _, XUtt_test, YUtt_test = create_Uttdata(config, model_name)\n",
    "\n",
    "XDA_test, YDA_test = DA_vocab.tokenize(XDA_test, YDA_test)\n",
    "XUtt_test, _ = Utt_vocab.tokenize(XUtt_test, YUtt_test)\n",
    "\n",
    "print('tokenizer finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, _, _ = select_model(model_name, Utt_vocab, DA_vocab, config, device, lr, loss_name)\n",
    "models.load_state_dict(torch.load(os.path.join(config[model_name+loss_name]['log_dir'], config[model_name+loss_name]['SAVE_NAME'] + \"_\" + str(config[model_name+loss_name]['window_size']) + \".model\".format('model_save'))))\n",
    "\n",
    "print('model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "\n",
    "def ModelTest():\n",
    "    result=[]\n",
    "    for seq_idx in range(0, len(XDA_test)):\n",
    "        print('\\r{}/{} conversation evaluating'.format(seq_idx+1, len(XDA_test)), end='')\n",
    "        XDA_seq = XDA_test[seq_idx]\n",
    "        YDA_seq = YDA_test[seq_idx]\n",
    "        DAturn[seq_idx] = DAturn[seq_idx] + [0] * (len(XDA_seq) - len(DAturn[seq_idx]))\n",
    "        DAturn_seq = DAturn[seq_idx]\n",
    "        XUtt_seq = XUtt_test[seq_idx]\n",
    "\n",
    "        pred_seq = []\n",
    "        true_seq = []\n",
    "        utter_hidden, context_hidden, da_hidden = models.initDAHidden(1)\n",
    "\n",
    "        for i in range(0, len(XDA_seq)):\n",
    "            XDA_tensor = torch.tensor([[XDA_seq[i]]]).to(device)\n",
    "            YDA_tensor = torch.tensor(YDA_seq[i]).to(device)\n",
    "            DAturn_tensor = torch.tensor([[DAturn_seq[i]]]).to(device)\n",
    "            DAturn_tensor = DAturn_tensor.float()\n",
    "            DAturn_tensor = DAturn_tensor.unsqueeze(1)\n",
    "            XUtt_tensor = torch.tensor([XUtt_seq[i]]).to(device)\n",
    "\n",
    "            output, utter_hidden, context_hidden, da_hidden, att_weights = models.prediction(XUtt_tensor, XDA_tensor, None, utter_hidden, context_hidden, da_hidden, DAturn_tensor)\n",
    "                # output, utter_hidden, context_hidden, da_hidden = da_predict_model.prediction(XU_tensor, X_tensor, None, utter_hidden, context_hidden, da_hidden, turn_tensor)\n",
    "\n",
    "            pred_idx = torch.argmax(output)\n",
    "            pred_seq.append(pred_idx.item())\n",
    "            true_seq.append(YDA_tensor.item())\n",
    "            utter_list = [Utt_vocab.id2word[word] for word in XUtt_seq[i]]\n",
    "\n",
    "        result.append({'true': true_seq,\n",
    "                        'true_detok': [DA_vocab.id2word[token] for token in true_seq],\n",
    "                        'pred': pred_seq,\n",
    "                        'pred_detok': [DA_vocab.id2word[token] for token in pred_seq],\n",
    "                        'UttSeq': [[Utt_vocab.id2word[word] for word in sentence] for sentence in XUtt_seq],\n",
    "                        'seq_detok': [DA_vocab.id2word[label] for label in XDA_seq]})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_average(y_true, y_pred):\n",
    "    p = precision_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    r = recall_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    f = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    acc = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    print('p: {} | r: {} | f: {} | acc: {}'.format(p, r, f, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ModelTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = [label for line in result for label in line['true']]\n",
    "pred = [label for line in result for label in line['pred']]\n",
    "\n",
    "calc_average(y_true=true, y_pred=pred)\n",
    "f = f1_score(y_true=true, y_pred=pred, average=None)\n",
    "r = recall_score(y_true=true, y_pred=pred, average=None)\n",
    "p = precision_score(y_true=true, y_pred=pred, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p: 0.08928320438668537 | r: 0.2204979571885202 | f: 0.08984448174391911 | acc: 0.14053189362127574\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p: 0.08912725942812572 | r: 0.22063442894175095 | f: 0.08982748395954142 | acc: 0.14128174365126975"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p: 0.08920915274971167 | r: 0.22057195853621478 | f: 0.08991179627013274 | acc: 0.1412617476504699"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2回目の調査結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ p: 0.09872020361658407 | r: 0.24682752100587319 | f: 0.1299960965830519 | acc: 0.20530893821235752\n",
    "               \n",
    "+ p: 0.09936403344872773 | r: 0.24819802586828738 | f: 0.13074154261395307 | acc: 0.20596880623875224\n",
    "\n",
    "+ p: 0.0991076895807943 | r: 0.2471875496588724 | f: 0.13046067696477437 | acc: 0.20617876424715056"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(128, 9, requires_grad=True)\n",
    "print(inputs.size())\n",
    "target = torch.empty(128, dtype=torch.long).random_(5)\n",
    "print(target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "output = loss(inputs, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
