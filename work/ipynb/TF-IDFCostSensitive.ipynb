{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Cost Sensitive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pyhocon\n",
    "import torch\n",
    "import argparse\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import glob\n",
    "import os, re, json\n",
    "import matplotlib.pyplot as plt\n",
    "import jsonlines\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ データ処理\n",
    "    + データのロード\n",
    "    + データの前処理\n",
    "    + データの分割\n",
    "+ モデルの構築\n",
    "    + Cmb Attentionモデル\n",
    "    + 損失関数の定義\n",
    "        + tf-idf形式のコスト損失関数の構築\n",
    "        \n",
    "+ モデルの訓練\n",
    "    + モデルの学習\n",
    "    + モデルの精度評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データのディレクトリを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url_repo = \"data/corpus/*\"\n",
    "data_url_dir = glob.glob(data_url_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_token = '<EOS>'\n",
    "BOS_token = '<BOS>'\n",
    "file_pattern = re.compile(r'^sw\\_([a-z]+?)\\_([0-9]+?)\\.jsonlines$')\n",
    "\n",
    "swda_tagu = {\n",
    "    '<Uninterpretable>': ['abandoned_or_turn-exit/uninterpretable', 'non-verbal'],\n",
    "    '<Statement>': ['statement-non-opinion', 'statement-opinion', 'other_answers', '3rd-party-talk', 'self-talk', 'offers,_options_commits', 'collaborative_completion'],\n",
    "    '<Question>': ['q', 'yes-no-question', 'wh-question', 'declarative_yes-no-question', 'backchannel_in_question_form', 'open-question', 'rhetorical-questions', 'signal-non-understanding', 'or-clause', 'tag-question', 'declarative_wh-question'],\n",
    "    '<Directive>': ['action-directive'],\n",
    "    '<Greeting>': ['conventional-opening', 'conventional-closing'],\n",
    "    '<Apology>': ['apology', 'no_answers', 'reject', 'negative_non-no_answers', 'dispreferred_answers', 'dispreferred_answers'],\n",
    "    '<Agreement>': ['agree/accept', 'maybe/accept-part', 'thanking'],\n",
    "    '<Understanding>': ['acknowledge_(backchannel)', 'summarize/reformulate', 'appreciation', 'response_acknowledgement', 'affirmative_non-yes_answers', 'yes_answers'],\n",
    "    '<Other>': ['other', 'hedge', 'quotation', 'repeat-phrase', 'hold_before_answer/agreement', 'downplayer']\n",
    "}\n",
    "\n",
    "daily_tagu = {1: \"inform\", 2: \"question\", 3: \"directive\", 4: \"commissive\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 対話行為のID化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DA_to_ID:\n",
    "    \n",
    "    def __init__(self, config, X_DA, Y_DA, name):\n",
    "        self.word2id = None\n",
    "        self.id2word = None\n",
    "        self.config = config\n",
    "        self.X_DA = X_DA\n",
    "        self.Y_DA = Y_DA\n",
    "        self.name = name\n",
    "        self.construct()\n",
    "        \n",
    "    def construct(self):\n",
    "#        vocab = {'<PAD>': 0}\n",
    "        vocab = {}\n",
    "        vocab_count = {}\n",
    "        \n",
    "        for x,y in zip(self.X_DA, self.Y_DA):\n",
    "            for token in x:\n",
    "                if token in vocab_count:\n",
    "                    vocab_count[token] += 1\n",
    "                else:\n",
    "                    vocab_count[token] = 1\n",
    "                    \n",
    "            for token in y:\n",
    "                if token in vocab_count:\n",
    "                    vocab_count[token] += 1\n",
    "                else:\n",
    "                    vocab_count[token] = 1\n",
    "                    \n",
    "        for k, _ in sorted(vocab_count.items(), key=lambda x: -x[1]):\n",
    "            vocab[k] = len(vocab)\n",
    "            if len(vocab) >= self.config[self.name]['MAX_VOCAB']: break\n",
    "        self.word2id = vocab\n",
    "        self.id2word = {v : k for k, v in vocab.items()}\n",
    "        return vocab\n",
    "        \n",
    "    def tokenize(self, X_tensor, Y_tensor):\n",
    "        X_Tensor = [[self.word2id[token] for token in sentence] for sentence in X_tensor]\n",
    "        Y_Tensor = [[self.word2id[token] for token in sentence] for sentence in Y_tensor]\n",
    "        return X_Tensor, Y_Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 発話のID化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UTT_to_ID:\n",
    "    \n",
    "    def __init__(self, config, X_UTT, Y_UTT, name):\n",
    "        self.word2id = None\n",
    "        self.id2word = None\n",
    "        self.config = config\n",
    "        self.X_UTT = X_UTT\n",
    "        self.Y_UTT = Y_UTT\n",
    "        self.name = name\n",
    "        self.construct()\n",
    "        \n",
    "    def construct(self):\n",
    "        \n",
    "        vocab = {'<UNK>': 0, '<EOS>': 1, '<BOS>': 2, '<UttPAD>': 3, '<ConvPAD>': 4}\n",
    "        vocab_count = {}\n",
    "        \n",
    "        for x,y in zip(self.X_UTT, self.Y_UTT):\n",
    "            for seq in x:\n",
    "                for word in seq:\n",
    "                    if word in vocab_count:\n",
    "                        vocab_count[word] += 1\n",
    "                    else:\n",
    "                        vocab_count[word] = 1\n",
    "            for seq in y:\n",
    "                for word in seq:\n",
    "                    if word in vocab_count:\n",
    "                        vocab_count[word] += 1\n",
    "                    else:\n",
    "                        vocab_count[word] = 1\n",
    "                        \n",
    "        for k, _ in sorted(vocab_count.items(), key=lambda x: -x[1]):\n",
    "            vocab[k] = len(vocab)\n",
    "            if len(vocab) >= self.config[self.name]['UTT_MAX_VOCAB']: break\n",
    "        self.word2id = vocab\n",
    "        self.id2word = {v : k for k, v in vocab.items()}\n",
    "\n",
    "        return vocab\n",
    "        \n",
    "    def tokenize(self, X_tensor, Y_tensor):\n",
    "        \n",
    "        X_Tensor = [[[self.word2id[token] if token in self.word2id else self.word2id['<UNK>'] for token in seq] for seq in dialogue] for dialogue in X_tensor]\n",
    "        Y_Tensor = [[[self.word2id[token] if token in self.word2id else self.word2id['<UNK>'] for token in seq] for seq in dialogue] for dialogue in Y_tensor]\n",
    "        return X_Tensor, Y_Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニングデータ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_traindata(config, name):\n",
    "    files = [f for f in os.listdir(config[name]['train_path']) if file_pattern.match(f)]\n",
    "    # print(\"files:\" , files)\n",
    "    da_x, da_y, utt_x, utt_y, turn = [], [], [], [], []\n",
    "    # 1file 1conversation\n",
    "    for filename in files:\n",
    "        # print(os.path.join(config['train_path'], filename))\n",
    "        with open(os.path.join(config[name]['train_path'], filename), 'r') as f:\n",
    "            data = f.read().split('\\n')\n",
    "            data.remove('')\n",
    "            da_seq, utt_seq, turn_seq = [], [], []\n",
    "            # 1line 1turn\n",
    "            for idx, line in enumerate(data, 1):\n",
    "                jsondata = json.loads(line)\n",
    "                # single-turn multi dialogue case\n",
    "                if config[name]['multi_dialogue']:\n",
    "                    for da, utt in zip(jsondata['DA'], jsondata['sentence']):\n",
    "                        da_seq.append(da)\n",
    "                        utt_seq.append(utt.split(' '))\n",
    "                        turn_seq.append(0)\n",
    "                    if config[name]['turn']:\n",
    "                        da_seq.append('<turn>')\n",
    "                        utt_seq.append('<turn>')\n",
    "                    turn_seq[-1] = 1\n",
    "                # single-turn single dialogue case\n",
    "                else:\n",
    "                    da_seq.append(jsondata['DA'][-1])\n",
    "                    utt_seq.append(jsondata['sentence'][-1].split(' '))\n",
    "            da_seq = [easy_damsl(da) for da in da_seq]\n",
    "        \n",
    "            \n",
    "            # assert len(turn_seq) == len(da_seq), '{} != {}'.format(len(turn_seq), len(da_seq))\n",
    "        if config[name]['state']:\n",
    "            for i in range(max(1, len(da_seq) - 1 - config[name]['window_size'])):\n",
    "                da_x.append(da_seq[i:min(len(da_seq)-1, i + config[name]['window_size'])])\n",
    "                da_y.append(da_seq[1 + i:min(len(da_seq), 1 + i + config[name]['window_size'])])\n",
    "                utt_x.append(utt_seq[i:min(len(da_seq)-1, i + config[name]['window_size'])])\n",
    "                utt_y.append(utt_seq[1 + i:min(len(da_seq), 1 + i + config[name]['window_size'])])\n",
    "                turn.append(turn_seq[i:min(len(da_seq), i + config[name]['window_size'])])\n",
    "        else:\n",
    "            da_x.append(da_seq[:-1])\n",
    "            da_y.append(da_seq[1:])\n",
    "            utt_x.append(utt_seq[:-1])\n",
    "            utt_y.append(utt_seq[1:])\n",
    "            turn.append(turn_seq[:-1])\n",
    "    assert len(da_x) == len(da_y), 'Unexpect length da_posts and da_cmnts'\n",
    "    assert len(utt_x) == len(utt_y), 'Unexpect length utt_posts and utt_cmnts'\n",
    "    # assert len(turn) == len(da_posts)\n",
    "    \n",
    "    return da_x, da_y, utt_x, utt_y, turn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### タグ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def easy_damsl(tag):\n",
    "    easy_tag = [k for k, v in swda_tagu.items() if tag in v]\n",
    "    return easy_tag[0] if not len(easy_tag) < 1 else tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(x, y, turn):\n",
    "    split_size = round(len(x) / 10)\n",
    "    if split_size == 0: split_size = 1\n",
    "    X_train, Y_train, Tturn = x[split_size * 2:], y[split_size * 2:], turn[split_size * 2:]\n",
    "    X_valid, Y_valid, Vturn = x[split_size: split_size * 2], y[split_size: split_size * 2], turn[split_size: split_size * 2]\n",
    "    X_test, Y_test, Testturn = x[:split_size], y[:split_size], turn[:split_size]\n",
    "    assert len(X_train) == len(Y_train), 'Unexpect to separate train data'\n",
    "    return X_train, Y_train, X_valid, Y_valid, X_test, Y_test, Tturn, Vturn, Testturn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練の準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_env(name):\n",
    "    config = pyhocon.ConfigFactory.parse_file('./dialogue.conf')\n",
    "    config['log_dirs'] = os.path.join(config[name]['log_dir'])\n",
    "    if not os.path.exists(config['log_dirs']):\n",
    "        os.mkdir(config['log_dirs'])\n",
    "     \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DAdata(config, name):\n",
    "    posts, cmnts, _, _, turn = create_traindata(config, name)\n",
    "    X_train, Y_train, X_valid, Y_valid, X_test, Y_test, Tturn, Vturn, Testturn = separate_data(posts, cmnts, turn)\n",
    "    return X_train, Y_train, X_valid, Y_valid, X_test, Y_test, Tturn, Vturn, Testturn\n",
    "\n",
    "def create_Uttdata(config, name):\n",
    "    _, _, posts, cmnts, turn = create_traindata(config, name)\n",
    "    X_train, Y_train, X_valid, Y_valid, X_test, Y_test, _, _, _ = separate_data(posts, cmnts, turn)\n",
    "    return X_train, Y_train, X_valid, Y_valid, X_test, Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'CmbAttention'\n",
    "# loss_name = \"Affinity\"\n",
    "# loss_name = \"CE\"\n",
    "# loss_name = \"LogAll\"\n",
    "# loss_name = \"CE_cost\"\n",
    "# loss_name = \"TFIDF_AllAdd\"\n",
    "loss_name = \"TFIDF_Allweight\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish preparing dataset...\n"
     ]
    }
   ],
   "source": [
    "write = SummaryWriter(\"./logs\")\n",
    "config = initialize_env(model_name+loss_name)\n",
    "\n",
    "XDA_train, YDA_train, XDA_valid, YDA_valid, _, _, Tturn, Vturn, _ = create_DAdata(config, model_name+loss_name)\n",
    "XUtt_train, YUtt_train, XUtt_valid, YUtt_valid, _, _ = create_Uttdata(config, model_name+loss_name)\n",
    "\n",
    "DA_vocab = DA_to_ID(config, XDA_train+XDA_valid, YDA_train+YDA_valid, model_name+loss_name)\n",
    "Utt_vocab = UTT_to_ID(config, XUtt_train+XUtt_valid, YUtt_train+YUtt_valid, model_name+loss_name)\n",
    "\n",
    "XDA_train, YDA_train = DA_vocab.tokenize(XDA_train, YDA_train)\n",
    "XDA_valid, YDA_valid = DA_vocab.tokenize(XDA_valid, YDA_valid)\n",
    "XUtt_train, YUtt_train = Utt_vocab.tokenize(XUtt_train, YUtt_train)\n",
    "XUtt_valid, YUtt_valid = Utt_vocab.tokenize(XUtt_valid, YUtt_valid)\n",
    "\n",
    "print('Finish preparing dataset...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cmb Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CmbAttentionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_name, utt_vocab, da_vocab, config, device):\n",
    "        super(CmbAttentionModel, self).__init__()\n",
    "        \n",
    "        self.utter_encoder = UtteraceEncoder(len(utt_vocab.word2id), config[model_name]['UTT_EMBED'], config[model_name]['UTT_HIDDEN'])\n",
    "\n",
    "        self.context_encoder = RNNContextAwareEncoder(config[model_name]['CON_EMBED'], config[model_name]['CON_HIDDEN'])\n",
    "\n",
    "        self.da_encoder = RNNDAAwareEncoder(len(utt_vocab.word2id), config[model_name]['DA_EMBED'], config[model_name]['DA_HIDDEN'])\n",
    "\n",
    "        self.de_encoder = DenceEncoder(config[model_name]['DA_HIDDEN'] + config[model_name]['CON_HIDDEN'], config[model_name]['DA_EMBED'], len(da_vocab.word2id))\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, X_utter, X_da, Y_da, mask, utter_hidden, context_hidden, da_hidden, turn):\n",
    "\n",
    "        utter_output, utter_weights = self.utter_encoder(X_utter, mask)\n",
    "\n",
    "        turn_output = torch.cat((utter_output, turn), dim=2)\n",
    "\n",
    "        context_output, context_weights, context_hidden = self.context_encoder(turn_output, mask, context_hidden)\n",
    "\n",
    "        da_output, da_weights, da_hidden = self.da_encoder(X_da, mask, da_hidden)\n",
    "\n",
    "        x_output = torch.cat((context_output, da_output), dim=2)\n",
    "\n",
    "        dence_output = self.de_encoder(x_output)\n",
    "\n",
    "        output = dence_output.squeeze(1)  # (batch_size, da_dim)\n",
    "        \n",
    "        Y_da = Y_da.squeeze()\n",
    "        \n",
    "        return output, utter_hidden, context_hidden, da_hidden\n",
    "    \n",
    "    def validtion(self, X_utter, X_da, Y_da, mask, utter_hidden, context_hidden, da_hidden, turn):\n",
    "\n",
    "        utter_output, utter_weights = self.utter_encoder(X_utter, mask)\n",
    "\n",
    "        turn_output = torch.cat((utter_output, turn), dim=2)\n",
    "\n",
    "        context_output, context_weights, context_hidden = self.context_encoder(turn_output, mask, context_hidden)\n",
    "\n",
    "        da_output, da_weights, da_hidden = self.da_encoder(X_da, mask, da_hidden)\n",
    "\n",
    "        x_output = torch.cat((context_output, da_output), dim=2)\n",
    "\n",
    "        dence_output = self.de_encoder(x_output)\n",
    "\n",
    "        output = dence_output.squeeze(1)  # (batch_size, da_dim)\n",
    "            \n",
    "        Y_da = Y_da.squeeze(0)\n",
    "\n",
    "        return output, utter_hidden, context_hidden, da_hidden, Y_da\n",
    "\n",
    "\n",
    "    def prediction(self, X_utter, X_da, mask, utter_hidden, context_hidden, da_hidden, turn):\n",
    "\n",
    "        utter_output, utter_weights = self.utter_encoder(X_utter, mask)\n",
    "\n",
    "        turn_output = torch.cat((utter_output, turn), dim=2)\n",
    "\n",
    "        context_output, context_weights, context_hidden = self.context_encoder(turn_output, mask, context_hidden)\n",
    "\n",
    "        da_output, da_weights, da_hidden = self.da_encoder(X_da, mask, da_hidden)\n",
    "\n",
    "        x_output = torch.cat((context_output, da_output), dim=2)\n",
    "\n",
    "        dence_output = self.de_encoder(x_output)\n",
    "\n",
    "        output = dence_output.squeeze(1)  # (batch_size, da_dim)\n",
    "\n",
    "        return output, utter_hidden, context_hidden, da_hidden, utter_weights\n",
    "\n",
    "\n",
    "    def initDAHidden(self, batch_size):\n",
    "        return self.utter_encoder.initHidden(batch_size, self.device), self.context_encoder.initHidden(batch_size, self.device), self.da_encoder.initHidden(batch_size, self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, w_model):\n",
    "        super(WordEmbedding, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.linear = nn.Linear(embed_size, w_model)\n",
    "\n",
    "    def forward(self, x_word):\n",
    "        return torch.tanh(self.linear(self.word_embedding(x_word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, da_size, embed_size, d_model):\n",
    "        super(DAEmbedding, self).__init__()\n",
    "        self.da_embedding = nn.Embedding(da_size, embed_size)\n",
    "        self.linear = nn.Linear(embed_size, d_model)\n",
    "\n",
    "    def forward(self, x_da):\n",
    "        return torch.tanh(self.linear(self.da_embedding(x_da)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model):\n",
    "        super(Attention, self).__init__()\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        self.d_k = d_model\n",
    "       \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # 全結合層で特徴量を変換\n",
    "        k = self.k_linear(k)\n",
    "        q = self.q_linear(q)\n",
    "        v = self.v_linear(v)\n",
    "\n",
    "        # Attentionの値を計算する\n",
    "        # 各値を足し算すると大きくなりすぎるので、root(d_k)で割って調整\n",
    "        weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # ここでmaskを計算\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            weights = weights.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # softmaxで規格化をする\n",
    "        attention_weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        # AttentionをValueとかけ算\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "\n",
    "        # 全結合層で特徴量を変換\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, hidden_size, att_size):\n",
    "        super(ContextAttention, self).__init__()\n",
    "        self.q_linear = nn.Linear(att_size, att_size)\n",
    "        self.v_linear = nn.Linear(att_size, att_size)\n",
    "        self.k_linear = nn.Linear(att_size, att_size)\n",
    "\n",
    "        self.fc_1 = nn.Linear(d_model, d_model)\n",
    "        self.fc_3 = nn.Linear(hidden_size, d_model, bias=True)\n",
    "        self.fc_2 = nn.Linear(d_model, att_size)\n",
    "\n",
    "        self.fc_out = nn.Linear(att_size, hidden_size, bias=True)\n",
    "        self.d_k = att_size\n",
    "\n",
    "    def forward(self, x, mask, hidden):\n",
    "        \n",
    "        x = self.fc_2(torch.tanh(self.fc_1(x) + self.fc_3(hidden)))\n",
    "\n",
    "        q = self.q_linear(x)\n",
    "        v = self.v_linear(x)\n",
    "        k = self.k_linear(x)\n",
    "\n",
    "        weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # ここでmaskを計算\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            weights = weights.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attention_weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        att_output = torch.matmul(attention_weights, v)\n",
    "\n",
    "        output = self.fc_out(att_output)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.linear_1(x)\n",
    "\n",
    "        x = self.dropout(F.relu(x))\n",
    "\n",
    "        x = self.linear_2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositinalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len, dropout=0.1):\n",
    "        super(PositinalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenceEncoder(nn.Module):\n",
    "    def __init__(self, da_hidden, da_embed_size, da_input_size):\n",
    "        super(DenceEncoder, self).__init__()\n",
    "        self.he = nn.Linear(da_hidden, da_embed_size)\n",
    "        self.ey = nn.Linear(da_embed_size, da_input_size)\n",
    "\n",
    "    def forward(self, hidden):\n",
    "        pred = self.ey(torch.tanh(self.he(hidden)))\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNContextAwareEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_dim, d_model):\n",
    "        super(RNNContextAwareEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(self.d_model+1, self.d_model)\n",
    "        self.rnn = nn.GRU(self.d_model, self.d_model, batch_first=True)\n",
    "        self.attention = ContextAttention(self.d_model, self.d_model, self.d_model)\n",
    "        self.ffn = FeedForward(self.d_model, self.d_model)\n",
    "\n",
    "    def forward(self, x, mask, hidden):\n",
    "\n",
    "        lin_output = self.linear(x)\n",
    "\n",
    "        att_output, att_weights = self.attention(lin_output, mask, hidden.transpose(0,1))        \n",
    "\n",
    "        rnn_output, rnn_hidden = self.rnn(att_output, hidden)\n",
    "\n",
    "        ffn_output = self.ffn(rnn_output)\n",
    "\n",
    "        return ffn_output, att_weights, rnn_hidden\n",
    "\n",
    "    def initHidden(self, batch_size, device):\n",
    "        return torch.zeros(1, batch_size, self.d_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDAAwareEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, da_size, emb_dim, d_model):\n",
    "        super(RNNDAAwareEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = DAEmbedding(da_size, emb_dim, self.d_model)\n",
    "        self.rnn = nn.GRU(self.d_model, self.d_model, batch_first=True)\n",
    "        self.attention = ContextAttention(self.d_model, self.d_model, self.d_model)\n",
    "        self.ffn = FeedForward(self.d_model, emb_dim)\n",
    "\n",
    "    def forward(self, X_da, mask, hidden):\n",
    "\n",
    "        emb_output = self.embedding(X_da)\n",
    "\n",
    "        att_output, att_weights = self.attention(emb_output, mask, hidden.transpose(0,1))        \n",
    "\n",
    "        rnn_output, rnn_hidden = self.rnn(att_output, hidden)\n",
    "\n",
    "        ffn_output = self.ffn(rnn_output)\n",
    "\n",
    "        return ffn_output, att_weights, rnn_hidden\n",
    "\n",
    "    def initHidden(self, batch_size, device):\n",
    "        return torch.zeros(1, batch_size, self.d_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UtteraceEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_dim, d_model):\n",
    "        super(UtteraceEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = WordEmbedding(vocab_size, emb_dim, self.d_model)\n",
    "        self.pe = PositinalEncoding(self.d_model, 200)\n",
    "        self.att = Attention(self.d_model)\n",
    "        self.ffn = FeedForward(d_model, emb_dim)\n",
    "        \n",
    "    def forward(self, x_utter, mask):\n",
    "\n",
    "        emb_output = self.embedding(x_utter)\n",
    "\n",
    "        pos_output = self.pe(emb_output)\n",
    "\n",
    "        att_output, att_weights = self.att(pos_output, pos_output, pos_output, mask)\n",
    "\n",
    "        ffn_output = self.ffn(att_output)\n",
    "\n",
    "        seq_len = ffn_output.size()[1]\n",
    "\n",
    "        avg_output = F.avg_pool2d(ffn_output, (seq_len, 1)) # => (128, 1, 512)\n",
    "\n",
    "        return avg_output, att_weights  # 発話ベクトル(128, 1, 512)\n",
    "\n",
    "    def initHidden(self, batch_size, device):\n",
    "        return torch.zeros(1, batch_size, self.d_model).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Loss Fcuntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllDataTFIDF(nn.Module):\n",
    "    def __init__(self, tfidf, device):\n",
    "        super(AllDataTFIDF, self).__init__()\n",
    "        self.tfidf = tfidf\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='mean')\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, logit, target):\n",
    "        ce_loss = self.ce(logit, target)\n",
    "        pred = torch.argmax(logit.squeeze(1), dim=1).to(self.device)\n",
    "        cost_loss = Add_tfidf_cost(pred, target.to(self.device), self.tfidf)\n",
    "        return ce_loss + torch.tensor(cost_loss)\n",
    "    \n",
    "def Add_tfidf_cost(pred, target, tfidf):\n",
    "    loss=[0]\n",
    "    for i, p in enumerate(pred):\n",
    "        if target[i]!=p:\n",
    "            loss.append(tfidf[target[i]])\n",
    "    return np.array(loss).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF ALL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_Allweight=[\n",
    "    0.008819631900111391,\n",
    "    0.0848148499389655,\n",
    "    0.2120586174920262,\n",
    "    0.3158002502173657,\n",
    "    0.4848457517317563,\n",
    "    0.7863592650818836,\n",
    "    1.2701063502450178,\n",
    "    0.9210760201244663,\n",
    "    1.0213459579190332\n",
    "]\n",
    "TFIDF_Allweights=torch.tensor(TFIDF_Allweight, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Sensitive Loss (割合重視)　=> Cost Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_matrix = [\n",
    "    [0, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "    [0.6606467194974978, 0, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "    [1.4345745909708165, 0.7739278714733187, 0, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "    [2.0795002276998646, 1.418853508202367, 0.6449256367290482, 0, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "    [2.927104615492692, 2.2664578959951944, 1.4925300245218756, 0.8476043877928274, 0, 0.01, 0.01, 0.01, 0.01],\n",
    "    [4.628593334154884, 3.9679466146573863, 3.1940187431840674, 2.549093106455019, 1.7014887186621916, 0, 0.01, 0.01, 0.01],\n",
    "    [5.2347708574106395, 4.574124137913142, 3.800196266439823, 3.1552706297107744, 2.3076662419179472, 0.6061775232557551, 0, 0.01, 0.01],\n",
    "    [5.372836213185386, 4.712189493687888, 3.9382616222145694, 3.293335985485521, 2.4457315976926934, 0.7442428790305019, 0.13806535577474677,\n",
    "  0, 0.01],\n",
    "    [5.663684515195216, 5.003037795697718, 4.2291099242243995, 3.5841842874953516, 2.7365798997025244, 1.0350911810403323, 0.4289136577845772,\n",
    "  0.2908483020098305, 0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostMatrixLoss(nn.Module):\n",
    "    def __init__(self, cost_matrix):\n",
    "        super(CostMatrixLoss, self).__init__()\n",
    "        self.loss_ = nn.CrossEntropyLoss(reduction='none')\n",
    "        self.cost_matrix = cost_matrix\n",
    "        \n",
    "    def forward(self, logit, target):\n",
    "        ce_output = self.loss_(logit, target)\n",
    "        ce_output = torch.unsqueeze(ce_output,0)\n",
    "        pred = torch.argmax(logit.squeeze(1), dim=1)\n",
    "        cost_mat = torch.tensor([self.cost_matrix[target[i]][pred[i]] for i in range(len(pred))])\n",
    "        cost_output = torch.matmul(output, cost_mat)\n",
    "        return cost_output/len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostSensitiveLoss(nn.Module):\n",
    "    def __init__(self, device, cost_matrix):\n",
    "        super(CostSensitiveLoss, self).__init__()\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='mean')\n",
    "        self.device = device\n",
    "        self.matrix = cost_matrix\n",
    "        \n",
    "    def forward(self, logit, target):\n",
    "        ce_loss = self.ce(logit, target)\n",
    "        pred = torch.argmax(logit.squeeze(1), dim=1).to(self.device)\n",
    "        cost = CostMatrix(pred, target).to(self.device)\n",
    "        return ce_loss+torch.tensor(cost)\n",
    "    \n",
    "    \n",
    "def CostMatrix(inputs, targets):\n",
    "    cost_loss=[]\n",
    "    for i, v in enumerate(inputs):\n",
    "        cost_loss.append(class_matrix[v][targets[i]])\n",
    "    return np.array(loss).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Weight: log(全体/タグ数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_log_all_weight = [\n",
    "    0.6711497989840916,\n",
    "    1.3317965184815894,\n",
    "    2.105724389954908,\n",
    "    2.750650026683956,\n",
    "    3.598254414476784,\n",
    "    5.299743133138976,\n",
    "    5.905920656394731,\n",
    "    6.043986012169477,\n",
    "    6.334834314179308\n",
    "]\n",
    "class_log_all_weights = torch.tensor(class_log_all_weight, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(mode_name, utt_vocab, da_vocab, config, device, lr, loss_func):\n",
    "    \n",
    "    if model_name == \"CmbAttention\":\n",
    "        \n",
    "        if loss_func == \"CE\":\n",
    "            model = CmbAttentionModel(model_name+loss_func, utt_vocab, da_vocab, config, device).to(device)\n",
    "            opt = optim.Adam(model.parameters(), lr)\n",
    "            loss_function = nn.CrossEntropyLoss(reduction='mean', weight=torch.tensor([1.0, 1.0, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5]).cuda())\n",
    "        \n",
    "        elif loss_func == \"Affinity\":\n",
    "            model = CmbAttentionModel_affinity(model_name+loss_func, utt_vocab, da_vocab, config, device).to(device)\n",
    "            opt = optim.Adam(model.parameters(), lr)\n",
    "            loss_function = Affinity_Loss(0.95, len(da_vocab.word2id), device)\n",
    "            \n",
    "        elif loss_func == \"LogAll\":\n",
    "            model = CmbAttentionModel(model_name+loss_func, utt_vocab, da_vocab, config, device).to(device)\n",
    "            opt = optim.Adam(model.parameters(), lr)\n",
    "            loss_function = nn.CrossEntropyLoss(reduction='mean', weight=class_log_all_weights).to(device)\n",
    "            \n",
    "        elif loss_func == \"TFIDF_Allweight\":\n",
    "            model = CmbAttentionModel(model_name+loss_func, utt_vocab, da_vocab, config, device).to(device)\n",
    "            opt = optim.Adam(model.parameters(), lr)\n",
    "            loss_function = nn.CrossEntropyLoss(reduction='mean', weight=TFIDF_Allweights).to(device)\n",
    "            \n",
    "        elif loss_func == \"CE_cost\":\n",
    "            model = CmbAttentionModel(model_name+loss_func, utt_vocab, da_vocab, config, device).to(device)\n",
    "            opt = optim.Adam(model.parameters(), lr)\n",
    "            loss_func = CostMatrixLoss(device, class_matrix)\n",
    "            \n",
    "        elif loss_func == \"TFIDF_AllAdd\":\n",
    "            model = CmbAttentionModel(model_name+loss_func, utt_vocab, da_vocab, config, device).to(device)\n",
    "            opt = optim.Adam(model.parameters(), lr)\n",
    "            loss_function = AllDataTFIDF(TFIDF_Allweight, device)\n",
    "            \n",
    "        elif loss_func == \"TFIDF_Update\":\n",
    "            model = CmbAttentionModel(model_name+loss_func, utt_vocab, da_vocab, config, device).to(device)\n",
    "            opt = optim.Adam(model.parameters(), lr)\n",
    "            loss_function = AllDataTFIDF(TFIDF_Allweight, device)\n",
    "    else:\n",
    "        model = None\n",
    "        opt = None\n",
    "        loss_function = None\n",
    "        \n",
    "    return model, opt, loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = config[model_name+loss_name]['lr']\n",
    "batch_size = config[model_name+loss_name]['BATCH_SIZE']\n",
    "plot_train_losses = []\n",
    "plot_valid_losses = []\n",
    "    \n",
    "print_total_loss = 0\n",
    "plot_total_loss = 0\n",
    "plot_total_acc = 0\n",
    "_valid_loss = None\n",
    "\n",
    "models, optims, loss_func = select_model(model_name, Utt_vocab, DA_vocab, config, device, lr, loss_name)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validation(X_valid, Y_valid, XU_valid, YU_valid, model, device, config, turn):\n",
    "\n",
    "    total_loss = 0\n",
    "    idx = 0\n",
    "    \n",
    "    for seq_idx in range(len(X_valid)):\n",
    "        print('\\r{}/{} conversation evaluating'.format(seq_idx+1, len(X_valid)), end='')\n",
    "        utter_hidden, context_hidden, da_hidden = model.initDAHidden(1)\n",
    "        X_seq = X_valid[seq_idx]\n",
    "        Y_seq = Y_valid[seq_idx]\n",
    "        turn[seq_idx] = turn[seq_idx] + [0] * (len(X_seq) - len(turn[seq_idx]))\n",
    "        turn_seq = turn[seq_idx]\n",
    "        XU_seq = XU_valid[seq_idx]\n",
    "        YU_seq = YU_valid[seq_idx]\n",
    "\n",
    "        assert len(X_seq) == len(Y_seq), 'Unexpect sequence len in evaluate {} != {}'.format(len(X_seq), len(Y_seq))\n",
    "        \n",
    "        for i in range(0, len(X_seq)):\n",
    "            X_tensor = torch.tensor([[X_seq[i]]]).to(device)\n",
    "            Y_tensor = torch.tensor([[Y_seq[i]]]).to(device)\n",
    "            turn_tensor = torch.tensor([[turn_seq[i]]]).to(device)\n",
    "            turn_tensor = turn_tensor.float()\n",
    "            turn_tensor = turn_tensor.unsqueeze(1)   \n",
    "            XU_tensor = torch.tensor([XU_seq[i]]).to(device)\n",
    "            \n",
    "            output, utter_hidden, context_hidden, da_hidden, Y_tensor = model.validtion(XU_tensor, X_tensor, Y_tensor, None, utter_hidden, context_hidden, da_hidden, turn_tensor)\n",
    "            loss = loss_func(output, Y_tensor)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        total_loss/=5\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CmbAttentionTFIDF_Allweight start TRAINING\n",
      "Epoch 1 start\n",
      "20004/20004 conversation evaluatingsteps 1\tloss 2.1479\tvalid loss 3.0551 | exec time 378.6428\n",
      "Model Saved\n",
      "Epoch 2 start\n",
      "20004/20004 conversation evaluatingsteps 2\tloss 2.2862\tvalid loss 3.1016 | exec time 375.4774\n",
      "Epoch 3 start\n",
      "20004/20004 conversation evaluatingsteps 3\tloss 2.2409\tvalid loss 3.0984 | exec time 377.3165\n",
      "Epoch 4 start\n",
      "20004/20004 conversation evaluatingsteps 4\tloss 2.0238\tvalid loss 3.1208 | exec time 375.9397\n",
      "Epoch 5 start\n",
      "20004/20004 conversation evaluatingsteps 5\tloss 2.2564\tvalid loss 2.9874 | exec time 377.9896\n",
      "Model Saved\n",
      "Epoch 6 start\n",
      "20004/20004 conversation evaluatingsteps 6\tloss 1.7989\tvalid loss 2.7087 | exec time 378.7932\n",
      "Model Saved\n",
      "Epoch 7 start\n",
      "20004/20004 conversation evaluatingsteps 7\tloss 1.9068\tvalid loss 2.8487 | exec time 377.9352\n",
      "Epoch 8 start\n",
      "20004/20004 conversation evaluatingsteps 8\tloss 2.0168\tvalid loss 3.4941 | exec time 377.2676\n",
      "Epoch 9 start\n",
      "20004/20004 conversation evaluatingsteps 9\tloss 2.1279\tvalid loss 2.9846 | exec time 377.7234\n",
      "Epoch 10 start\n",
      "8945/20004 conversation evaluatingg..."
     ]
    }
   ],
   "source": [
    "print('{} start TRAINING'.format(model_name+loss_name))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "config[model_name+loss_name]['EPOCH'] = 30\n",
    "\n",
    "for epoch in range(config[model_name+loss_name]['EPOCH']):\n",
    "    \n",
    "    tmp_time = time.time()\n",
    "    print('Epoch {} start'.format(epoch+1))\n",
    "    index = [i for i in range(len(XDA_train))]\n",
    "    random.shuffle(index)\n",
    "    idx=0\n",
    "    \n",
    "    while idx < len(index):\n",
    "        step_size = min(batch_size, len(index)-idx)\n",
    "        batch_idx = index[idx:idx+step_size]\n",
    "        utter_hidden, context_hidden, da_hidden = models.initDAHidden(step_size)\n",
    "        \n",
    "        print('\\rConversation {}/{} training...'.format(idx + step_size, len(XDA_train)), end='')\n",
    "        Xda_seq = [XDA_train[seq_idx] for seq_idx in batch_idx]\n",
    "        Yda_seq = [YDA_train[seq_idx] for seq_idx in batch_idx]\n",
    "        turn_seq = [Tturn[seq_idx] for seq_idx in batch_idx]\n",
    "        max_conv_len = max(len(s) for s in Xda_seq) \n",
    "        \n",
    "        ## \n",
    "        XUtt_seq = [XUtt_train[seq_idx] for seq_idx in batch_idx]\n",
    "        YUtt_seq = [YUtt_train[seq_idx] for seq_idx in batch_idx]\n",
    "            \n",
    "        for i in range(len(XUtt_seq)):\n",
    "            XUtt_seq[i] = XUtt_seq[i] + [[Utt_vocab.word2id['<ConvPAD>']]] * (max_conv_len - len(XUtt_seq[i]))\n",
    "            YUtt_seq[i] = YUtt_seq[i] + [[Utt_vocab.word2id['<ConvPAD>']]] * (max_conv_len - len(YUtt_seq[i]))\n",
    "                \n",
    "        for ci in range(len(Xda_seq)):\n",
    "\n",
    "            turn_seq[ci] = turn_seq[ci] + [0] * (max_conv_len - len(turn_seq[ci]))\n",
    "            Xda_seq[ci] = Xda_seq[ci] + [-1] * (max_conv_len - len(Xda_seq[ci]))\n",
    "            Yda_seq[ci] = Yda_seq[ci] + [-1] * (max_conv_len - len(Yda_seq[ci]))\n",
    "            \n",
    "        for i in range(0, max_conv_len):\n",
    "            last = True if i == max_conv_len - 1 else False\n",
    "            \n",
    "            Xda_tensor = torch.tensor([[X[i]] for X in Xda_seq]).to(device)\n",
    "            Yda_tensor = torch.tensor([[Y[i]] for Y in Yda_seq]).to(device)\n",
    "            turn_tensor = torch.tensor([[t[i]] for t in turn_seq]).to(device)\n",
    "            turn_tensor = turn_tensor.float()\n",
    "            turn_tensor = turn_tensor.unsqueeze(1)    \n",
    "                \n",
    "            max_seq_len = max(len(XU[i]) + 1 for XU in XUtt_seq)\n",
    "            for ci in range(len(XUtt_seq)):\n",
    "                XUtt_seq[ci][i] = XUtt_seq[ci][i] + [Utt_vocab.word2id['<UttPAD>']] * (max_seq_len - len(XUtt_seq[ci][i]))\n",
    "                YUtt_seq[ci][i] = YUtt_seq[ci][i] + [Utt_vocab.word2id['<UttPAD>']] * (max_seq_len - len(YUtt_seq[ci][i]))\n",
    "            XUtt_tensor = torch.tensor([XU[i] for XU in XUtt_seq]).to(device)\n",
    "            YUtt_tensor = None\n",
    "\n",
    "            output, utter_hidden, context_hidden, da_hidden = models(XUtt_tensor, Xda_tensor, Yda_tensor, None, utter_hidden, context_hidden, da_hidden, turn_tensor)\n",
    "            Yda_tensor=Yda_tensor.squeeze(1)\n",
    "            loss = loss_func(output.to(device), Yda_tensor.to(device))\n",
    "            optims.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            print_total_loss += loss.item()\n",
    "                \n",
    "            if last:\n",
    "                optims.step()\n",
    "        print_total_loss/=max_conv_len  \n",
    "        \n",
    "        idx += step_size\n",
    "        \n",
    "    valid_loss = validation(XDA_valid, YDA_valid, XUtt_valid, YUtt_valid, models, device, config, Vturn)\n",
    "    \n",
    "    \n",
    "    def save_model(filename):\n",
    "        torch.save(models.state_dict(), os.path.join(config[model_name+loss_name]['log_dir'], config[model_name+loss_name]['SAVE_NAME'] + \"_\" + str(config[model_name+loss_name]['window_size']) + \".model\".format(filename)))\n",
    "        \n",
    "    print(\"steps %d\\tloss %.4f\\tvalid loss %.4f | exec time %.4f\" % (epoch+1, print_total_loss, valid_loss, time.time()-tmp_time))\n",
    "    print_total_loss = 0\n",
    "    \n",
    "    if _valid_loss == None:\n",
    "        save_model(\"model_save\")\n",
    "        print(\"Model Saved\")\n",
    "        _valid_loss = valid_loss\n",
    "    else:\n",
    "        if valid_loss<_valid_loss:\n",
    "            _valid_loss = valid_loss\n",
    "            save_model(\"model_save\")\n",
    "            print(\"Model Saved\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer finish\n"
     ]
    }
   ],
   "source": [
    "_, _, _, _, XDA_test, YDA_test, _, _, DAturn = create_DAdata(config, model_name+loss_name)\n",
    "_, _, _, _, XUtt_test, YUtt_test = create_Uttdata(config, model_name+loss_name)\n",
    "\n",
    "XDA_test, YDA_test = DA_vocab.tokenize(XDA_test, YDA_test)\n",
    "XUtt_test, _ = Utt_vocab.tokenize(XUtt_test, YUtt_test)\n",
    "\n",
    "print('tokenizer finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "models, _, _ = select_model(model_name, Utt_vocab, DA_vocab, config, device, lr, loss_name)\n",
    "models.load_state_dict(torch.load(os.path.join(config[model_name+loss_name]['log_dir'], config[model_name+loss_name]['SAVE_NAME'] + \"_\" + str(config[model_name+loss_name]['window_size']) + \".model\".format('model_save'))))\n",
    "\n",
    "print('model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "\n",
    "def ModelTest():\n",
    "    result=[]\n",
    "    for seq_idx in range(0, len(XDA_test)):\n",
    "        print('\\r{}/{} conversation evaluating'.format(seq_idx+1, len(XDA_test)), end='')\n",
    "        XDA_seq = XDA_test[seq_idx]\n",
    "        YDA_seq = YDA_test[seq_idx]\n",
    "        DAturn[seq_idx] = DAturn[seq_idx] + [0] * (len(XDA_seq) - len(DAturn[seq_idx]))\n",
    "        DAturn_seq = DAturn[seq_idx]\n",
    "        XUtt_seq = XUtt_test[seq_idx]\n",
    "\n",
    "        pred_seq = []\n",
    "        true_seq = []\n",
    "        utter_hidden, context_hidden, da_hidden = models.initDAHidden(1)\n",
    "\n",
    "        for i in range(0, len(XDA_seq)):\n",
    "            XDA_tensor = torch.tensor([[XDA_seq[i]]]).to(device)\n",
    "            YDA_tensor = torch.tensor(YDA_seq[i]).to(device)\n",
    "            DAturn_tensor = torch.tensor([[DAturn_seq[i]]]).to(device)\n",
    "            DAturn_tensor = DAturn_tensor.float()\n",
    "            DAturn_tensor = DAturn_tensor.unsqueeze(1)\n",
    "            XUtt_tensor = torch.tensor([XUtt_seq[i]]).to(device)\n",
    "\n",
    "            output, utter_hidden, context_hidden, da_hidden, att_weights = models.prediction(XUtt_tensor, XDA_tensor, None, utter_hidden, context_hidden, da_hidden, DAturn_tensor)\n",
    "                # output, utter_hidden, context_hidden, da_hidden = da_predict_model.prediction(XU_tensor, X_tensor, None, utter_hidden, context_hidden, da_hidden, turn_tensor)\n",
    "\n",
    "            pred_idx = torch.argmax(output)\n",
    "            pred_seq.append(pred_idx.item())\n",
    "            true_seq.append(YDA_tensor.item())\n",
    "            utter_list = [Utt_vocab.id2word[word] for word in XUtt_seq[i]]\n",
    "\n",
    "        result.append({'true': true_seq,\n",
    "                        'true_detok': [DA_vocab.id2word[token] for token in true_seq],\n",
    "                        'pred': pred_seq,\n",
    "                        'pred_detok': [DA_vocab.id2word[token] for token in pred_seq],\n",
    "                        'UttSeq': [[Utt_vocab.id2word[word] for word in sentence] for sentence in XUtt_seq],\n",
    "                        'seq_detok': [DA_vocab.id2word[label] for label in XDA_seq]})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_average(y_true, y_pred):\n",
    "    p = precision_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    r = recall_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    f = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    acc = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    print('p: {} | r: {} | f: {} | acc: {}'.format(p, r, f, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20004/20004 conversation evaluating"
     ]
    }
   ],
   "source": [
    "result = ModelTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: 0.23352820874615576 | r: 0.2788918285455712 | f: 0.13576326585873455 | acc: 0.23300339932013597\n"
     ]
    }
   ],
   "source": [
    "true = [label for line in result for label in line['true']]\n",
    "pred = [label for line in result for label in line['pred']]\n",
    "\n",
    "calc_average(y_true=true, y_pred=pred)\n",
    "f = f1_score(y_true=true, y_pred=pred, average=None)\n",
    "r = recall_score(y_true=true, y_pred=pred, average=None)\n",
    "p = precision_score(y_true=true, y_pred=pred, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "調査結果 (weight操作あり(多数派1, 少数派2.5))\n",
    "+ CE weight\n",
    "    + p: 0.24818453133519863 | r: 0.20607521870644477 | f: 0.20802163700276752 | acc: 0.5632773445310938\n",
    "    + Statement> 0.71834421195035\n",
    "    + Understanding> 0.5381366307541626\n",
    "    + Uninterpretable> 0.19706556625401192\n",
    "    + Question> 0.024621480118862317\n",
    "    + Agreement> 0.059600614439324115\n",
    "    + Apology> 0.0\n",
    "    + Greeting> 0.33442622950819667\n",
    "    + Other> 0.0\n",
    "    + Directive> 0.0\n",
    "    \n",
    "    + p: 0.2208616507361535 | r: 0.20739232257066276 | f: 0.20530872105709908 | acc: 0.5402319536092781\n",
    "    + Statement> 0.713659725208346\n",
    "    + Understanding> 0.5119440279860069\n",
    "    + Uninterpretable> 0.23632643151943952\n",
    "    + Question> 0.04073820915926179\n",
    "    + Agreement> 0.08844765342960288\n",
    "    + Apology> 0.05831533477321814\n",
    "    + Greeting> 0.19834710743801653\n",
    "    + Other> 0.0\n",
    "    + Directive> 0.0\n",
    "    \n",
    "    + p: 0.22583136491006917 | r: 0.20164583940949257 | f: 0.19933769306971447 | acc: 0.5272445510897821\n",
    "    + Statement> 0.7129001270490534\n",
    "    + Understanding> 0.489025049558479\n",
    "    + Uninterpretable> 0.24705947625388372\n",
    "    + Question> 0.01017050553395154\n",
    "    + Agreement> 0.0694519804666305\n",
    "    + Apology> 0.0\n",
    "    + Greeting> 0.2654320987654321\n",
    "    + Other> 0.0\n",
    "    + Directive> 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### CE_cost (Cost Matrix)\n",
    "1回目\n",
    "+ p: 0.19701369707289088 \n",
    "+ r: 0.18496287108708176 \n",
    "+ f: 0.16902785521707672 \n",
    "+ acc: 0.5407718456308738\n",
    "+ Statement> 0.6820972495088409\n",
    "+ Understanding> 0.5250899257634123\n",
    "+ Uninterpretable> 0.019918246005202526\n",
    "+ Question> 0.0\n",
    "+ Agreement> 0.0014571948998178506\n",
    "+ Apology> 0.0\n",
    "+ Greeting> 0.28571428571428575\n",
    "+ Other> 0.0\n",
    "+ Directive> 0.0\n",
    "\n",
    "2回目\n",
    "+ p: 0.20648173193318442 \n",
    "+ r: 0.1874203755705327 \n",
    "+ f: 0.17165307997135495 \n",
    "+ acc: 0.5417316536692661\n",
    "+ Statement> 0.6833902603644694\n",
    "+ Understanding> 0.5254842908552615\n",
    "+ Uninterpretable> 0.019365410397735736\n",
    "+ Question> 0.0\n",
    "+ Agreement> 0.0029122679286494356\n",
    "+ Apology> 0.0\n",
    "+ Greeting> 0.3137254901960784\n",
    "+ Other> 0.0\n",
    "+ Directive> 0.0\n",
    "\n",
    "3回目\n",
    "+ p: 0.19553881707447446 \n",
    "+ r: 0.1842547378287682 \n",
    "+ f: 0.16758062586436398 \n",
    "+ acc: 0.5416616676664667\n",
    "+ Statement> 0.6831065179576384\n",
    "+ Understanding> 0.5259693037853561\n",
    "+ Uninterpretable> 0.020653789004457652\n",
    "+ Question> 0.0\n",
    "+ Agreement> 0.0021802325581395353\n",
    "+ Apology> 0.0\n",
    "+ Greeting> 0.2763157894736842\n",
    "+ Other> 0.0\n",
    "+ Directive> 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### TF-IDF ADD\n",
    "\n",
    "1回目\n",
    "+ p: 0.24024057427900639 \n",
    "+ r: 0.19567713365528064 \n",
    "+ f: 0.1841594871204228 \n",
    "+ acc: 0.5776944611077784\n",
    "\n",
    "2回目\n",
    "+ p: 0.24339163384492135 \n",
    "+ r: 0.197476632150292 \n",
    "+ f: 0.1860118815380931 \n",
    "+ acc: 0.5782843431313738\n",
    "\n",
    "3回目\n",
    "+ p: 0.24072101610115867 \n",
    "+ r: 0.1976486300940167 \n",
    "+ f: 0.18606180160630123 \n",
    "+ acc: 0.5775844831033793"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p: 0.2300734955693666 | r: 0.27594903920220787 | f: 0.13336721319773936 | acc: 0.23256348730253948qq+ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Statement> 3.9041911491986646e-05\n",
      "<Understanding> 0.6374199486137209\n",
      "<Uninterpretable> 0.3766359670382938\n",
      "<Question> 0.1779542269967305\n",
      "<Agreement> 0.16035307098197868\n",
      "<Apology> 0.5086042065009561\n",
      "<Greeting> 0.587378640776699\n",
      "<Other> 0.0\n",
      "<Directive> 0.03515625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(DA_vocab.id2word[idx], score) for idx, score in zip(sorted(set(true)),r)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogAll\n",
    "+ p: 0.22366663225067787 \n",
    "+ r: 0.2289095697286461 \n",
    "+ f: 0.21175668482817597 \n",
    "+ acc: 0.5251349730053989\n",
    "\n",
    "+ Statement> 0.5828762176196146\n",
    "+ Understanding> 0.7913870460559114\n",
    "+ Uninterpretable> 0.11076102762966554\n",
    "+ Question> 0.0535575276350615\n",
    "+ Agreement> 0.061051857300478116\n",
    "+ Apology> 0.16443594646271512\n",
    "+ Greeting> 0.2961165048543689\n",
    "+ Other> 0.0\n",
    "+ Directive> 0.0\n",
    "\n",
    "+ p: 0.22449542189480265 \n",
    "+ r: 0.2272148944899485 \n",
    "+ f: 0.21171216641043833 \n",
    "+ acc: 0.5252249550089982\n",
    "\n",
    "+ p: 0.22382434825256262 \n",
    "+ r: 0.22902392483260078 \n",
    "+ f: 0.21174164927044187 \n",
    "+ acc: 0.5255048990201959"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF-all Add\n",
    "+ p: 0.23289269243585706 \n",
    "+ r: 0.19465300793384677 \n",
    "+ f: 0.182717083718074 \n",
    "+ acc: 0.5775144971005799"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF weight\n",
    "\n",
    "+ p: 0.09689848389010275 \n",
    "+ r: 0.23841193688235723 \n",
    "+ f: 0.12855223355309808 \n",
    "+ acc: 0.20227954409118176\n",
    "\n",
    "+ p: 0.09676394159233768 \n",
    "+ r: 0.23825317197554727 \n",
    "+ f: 0.12840115908452857 \n",
    "+ acc: 0.2020895820835833\n",
    "\n",
    "<Statement> 0.0\n",
    "<Understanding> 0.5070368523986655\n",
    "<Uninterpretable> 0.4105671352399418\n",
    "<Question> 0.18511598941304686\n",
    "<Agreement> 0.13166605369621184\n",
    "<Apology> 0.5506692160611855\n",
    "<Greeting> 0.3592233009708738\n",
    "<Other> 0.0\n",
    "<Directive> 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
