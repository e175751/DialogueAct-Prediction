{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorchの環境構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pyhocon\n",
    "import torch\n",
    "import argparse\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import glob\n",
    "import os, re, json\n",
    "import matplotlib.pyplot as plt\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 応答の対話行為推定モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url_repo = \"data/corpus/*\"\n",
    "data_url_dir = glob.glob(data_url_repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPUをdevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_token = '<EOS>'\n",
    "BOS_token = '<BOS>'\n",
    "file_pattern = re.compile(r'^sw\\_([a-z]+?)\\_([0-9]+?)\\.jsonlines$')\n",
    "\n",
    "swda_tagu = {\n",
    "    '<Uninterpretable>': ['abandoned_or_turn-exit/uninterpretable', 'non-verbal'],\n",
    "    '<Statement>': ['statement-non-opinion', 'statement-opinion', 'other_answers', '3rd-party-talk', 'self-talk', 'offers,_options_commits', 'collaborative_completion'],\n",
    "    '<Question>': ['q', 'yes-no-question', 'wh-question', 'declarative_yes-no-question', 'backchannel_in_question_form', 'open-question', 'rhetorical-questions', 'signal-non-understanding', 'or-clause', 'tag-question', 'declarative_wh-question'],\n",
    "    '<Directive>': ['action-directive'],\n",
    "    '<Greeting>': ['conventional-opening', 'conventional-closing'],\n",
    "    '<Apology>': ['apology', 'no_answers', 'reject', 'negative_non-no_answers', 'dispreferred_answers', 'dispreferred_answers'],\n",
    "    '<Agreement>': ['agree/accept', 'maybe/accept-part', 'thanking'],\n",
    "    '<Understanding>': ['acknowledge_(backchannel)', 'summarize/reformulate', 'appreciation', 'response_acknowledgement', 'affirmative_non-yes_answers', 'yes_answers'],\n",
    "    '<Other>': ['other', 'hedge', 'quotation', 'repeat-phrase', 'hold_before_answer/agreement', 'downplayer']\n",
    "}\n",
    "\n",
    "daily_tagu = {1: \"inform\", 2: \"question\", 3: \"directive\", 4: \"commissive\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 対話行為のID化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DA_to_ID:\n",
    "    \n",
    "    def __init__(self, config, X_DA, Y_DA):\n",
    "        self.word2id = None\n",
    "        self.id2word = None\n",
    "        self.config = config\n",
    "        self.X_DA = X_DA\n",
    "        self.Y_DA = Y_DA\n",
    "        self.construct()\n",
    "        \n",
    "    def construct(self):\n",
    "        vocab = {'<PAD>': 0}\n",
    "        vocab_count = {}\n",
    "        \n",
    "        for x,y in zip(self.X_DA, self.Y_DA):\n",
    "            for token in x:\n",
    "                if token in vocab_count:\n",
    "                    vocab_count[token] += 1\n",
    "                else:\n",
    "                    vocab_count[token] = 1\n",
    "                    \n",
    "            for token in y:\n",
    "                if token in vocab_count:\n",
    "                    vocab_count[token] += 1\n",
    "                else:\n",
    "                    vocab_count[token] = 1\n",
    "                    \n",
    "        for k, _ in sorted(vocab_count.items(), key=lambda x: -x[1]):\n",
    "            vocab[k] = len(vocab)\n",
    "            if len(vocab) >= self.config['MAX_VOCAB']: break\n",
    "        self.word2id = vocab\n",
    "        self.id2word = {v : k for k, v in vocab.items()}\n",
    "        return vocab\n",
    "        \n",
    "    def tokenize(self, X_tensor, Y_tensor):\n",
    "        X_tensor = [[self.word2id[token] for token in sentence] for sentence in X_tensor]\n",
    "        Y_tensor = [[self.word2id[token] for token in sentence] for sentence in Y_tensor]\n",
    "        return X_tensor, Y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 発話のID化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UTT_to_ID:\n",
    "    \n",
    "    def __init__(self, config, X_UTT, Y_UTT):\n",
    "        self.word2id = None\n",
    "        self.id2word = None\n",
    "        self.config = config\n",
    "        self.X_UTT = X_UTT\n",
    "        self.Y_UTT = Y_UTT\n",
    "        self.construct()\n",
    "        \n",
    "    def construct(self):\n",
    "        \n",
    "        vocab = {'<UNK>': 0, '<EOS>': 1, '<BOS>': 2, '<UttPAD>': 3, '<ConvPAD>': 4}\n",
    "        vocab_count = {}\n",
    "        \n",
    "        for x,y in zip(self.X_UTT, self.Y_UTT):\n",
    "            for seq in x:\n",
    "                for word in seq:\n",
    "                    if word in vocab_count:\n",
    "                        vocab_count[word] += 1\n",
    "                    else:\n",
    "                        vocab_count[word] = 1\n",
    "            for seq in y:\n",
    "                for word in seq:\n",
    "                    if word in vocab_count:\n",
    "                        vocab_count[word] += 1\n",
    "                    else:\n",
    "                        vocab_count[word] = 1\n",
    "                        \n",
    "        for k, _ in sorted(vocab_count.items(), key=lambda x: -x[1]):\n",
    "            vocab[k] = len(vocab)\n",
    "            if len(vocab) >= self.config['UTT_MAX_VOCAB']: break\n",
    "        self.word2id = vocab\n",
    "        self.id2word = {v : k for k, v in vocab.items()}\n",
    "\n",
    "        return vocab\n",
    "        \n",
    "    def tokenize(self, X_tensor, Y_tensor):\n",
    "        \n",
    "        X_tensor = [[[self.word2id[token] if token in self.word2id else self.word2id['<UNK>'] for token in seq] for seq in dialogue] for dialogue in X_tensor]\n",
    "        Y_tensor = [[[self.word2id[token] if token in self.word2id else self.word2id['<UNK>'] for token in seq] for seq in dialogue] for dialogue in Y_tensor]\n",
    "        return X_tensor, Y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### トレーニングデータ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_traindata(config, name):\n",
    "    files = [f for f in os.listdir(config[name]['train_path']) if file_pattern.match(f)]\n",
    "    # print(\"files:\" , files)\n",
    "    da_x = []\n",
    "    da_y = []\n",
    "    utt_x = []\n",
    "    utt_y = []\n",
    "    turn = []\n",
    "    tasikame = []\n",
    "    # 1file 1conversation\n",
    "    for filename in files:\n",
    "        # print(os.path.join(config['train_path'], filename))\n",
    "        with open(os.path.join(config[name]['train_path'], filename), 'r') as f:\n",
    "            data = f.read().split('\\n')\n",
    "            # print(data)\n",
    "            data.remove('')\n",
    "            da_seq = []\n",
    "            utt_seq = []\n",
    "            turn_seq = []\n",
    "            # 1line 1turn\n",
    "            for idx, line in enumerate(data, 1):\n",
    "                jsondata = json.loads(line)\n",
    "                # single-turn multi dialogue case\n",
    "                if config[name]['multi_dialogue']:\n",
    "                    for da, utt in zip(jsondata['DA'], jsondata['sentence']):\n",
    "                        da_seq.append(da)\n",
    "                        tasikame.append(da)\n",
    "                        utt_seq.append(utt.split(' '))\n",
    "                        turn_seq.append(0)\n",
    "                    if not config[name]['turn']:\n",
    "                        da_seq.append('<turn>')\n",
    "                        utt_seq.append('<turn>')\n",
    "                    turn_seq[-1] = 1\n",
    "                # single-turn single dialogue case\n",
    "                else:\n",
    "                    da_seq.append(jsondata['DA'][-1])\n",
    "                    utt_seq.append(jsondata['sentence'][-1].split(' '))\n",
    "            da_seq = [easy_damsl(da) for da in da_seq]\n",
    "        \n",
    "            \n",
    "            # assert len(turn_seq) == len(da_seq), '{} != {}'.format(len(turn_seq), len(da_seq))\n",
    "        if config['state']:\n",
    "            for i in range(max(1, len(da_seq) - 1 - config[name]['window_size'])):\n",
    "                da_x.append(da_seq[i:min(len(da_seq)-1, i + config[name]['window_size'])])\n",
    "                da_y.append(da_seq[1 + i:min(len(da_seq), 1 + i + config[name]['window_size'])])\n",
    "                utt_x.append(utt_seq[i:min(len(da_seq)-1, i + config[name]['window_size'])])\n",
    "                utt_y.append(utt_seq[1 + i:min(len(da_seq), 1 + i + config[name]['window_size'])])\n",
    "                turn.append(turn_seq[i:min(len(da_seq), i + config[name]['window_size'])])\n",
    "        else:\n",
    "            da_x.append(da_seq[:-1])\n",
    "            da_y.append(da_seq[1:])\n",
    "            utt_x.append(utt_seq[:-1])\n",
    "            utt_y.append(utt_seq[1:])\n",
    "            turn.append(turn_seq[:-1])\n",
    "    assert len(da_x) == len(da_y), 'Unexpect length da_posts and da_cmnts'\n",
    "    assert len(utt_x) == len(utt_y), 'Unexpect length utt_posts and utt_cmnts'\n",
    "    # assert len(turn) == len(da_posts)\n",
    "    \n",
    "    return da_posts, da_cmnts, utt_posts, utt_cmnts, turn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### タグ付"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def easy_damsl(tag):\n",
    "    easy_tag = [k for k, v in swda_tagu.items() if tag in v]\n",
    "    return easy_tag[0] if not len(easy_tag) < 1 else tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データの割合(8:1:1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(x, y, turn):\n",
    "    split_size = round(len(x) / 10)\n",
    "    if split_size == 0: split_size = 1\n",
    "    X_train, Y_train, Tturn = x[split_size * 2:], y[split_size * 2:], turn[split_size * 2:]\n",
    "    X_valid, Y_valid, Vturn = x[split_size: split_size * 2], y[split_size: split_size * 2], turn[split_size: split_size * 2]\n",
    "    X_test, Y_test, Testturn = x[:split_size], y[:split_size], turn[:split_size]\n",
    "    assert len(X_train) == len(Y_train), 'Unexpect to separate train data'\n",
    "    return X_train, Y_train, X_valid, Y_valid, X_test, Y_test, Tturn, Vturn, Testturn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_env(name):\n",
    "    config = pyhocon.ConfigFactory.parse_file('./dialogue.conf')\n",
    "    config['log_dirs'] = os.path.join(config[name]['log_dir'])\n",
    "    if not os.path.exists(config['log_dirs']):\n",
    "        os.mkdir(config['log_dirs'])\n",
    "     \n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データ取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DAdata(config, name):\n",
    "    posts, cmnts, _, _, turn = create_traindata(config, name)\n",
    "    X_train, Y_train, X_valid, Y_valid, X_test, Y_test, Tturn, Vturn, Testturn = separate_data(posts, cmnts, turn)\n",
    "    return X_train, Y_train, X_valid, Y_valid, X_test, Y_test, Tturn, Vturn, Testturn\n",
    "\n",
    "def create_Uttdata(config, name):\n",
    "    _, _, posts, cmnts, turn = create_traindata(config, name)\n",
    "    X_train, Y_train, X_valid, Y_valid, X_test, Y_test, _, _, _ = separate_data(posts, cmnts, turn)\n",
    "    return X_train, Y_train, X_valid, Y_valid, X_test, Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(mode_name, utt_vocab, da_vocab, config, device, lr):\n",
    "    \n",
    "    if model_name == \"cmb_attention\":\n",
    "        model = CmbAttentionModel(utt_vocab, da_vocab, config, device).to(device)\n",
    "        opt = optim.Adam(da_predict_model.parameters(), lr)\n",
    "        \n",
    "    else:\n",
    "        model = None\n",
    "        opt = None\n",
    "        \n",
    "    return model, opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConfigMissingException",
     "evalue": "'No configuration setting found for key turn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConfigMissingException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-0d7c98ad75fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mXDA_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYDA_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXDA_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYDA_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_DAdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mXUtt_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYUtt_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXUtt_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYUtt_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_Uttdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-c070a77acccf>\u001b[0m in \u001b[0;36mcreate_DAdata\u001b[0;34m(config, name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_DAdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mposts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmnts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mturn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_traindata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTestturn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseparate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmnts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mturn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTestturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-61-268c15c288aa>\u001b[0m in \u001b[0;36mcreate_traindata\u001b[0;34m(config, name)\u001b[0m\n\u001b[1;32m     28\u001b[0m                         \u001b[0mutt_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                         \u001b[0mturn_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'turn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                         \u001b[0mda_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<turn>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                         \u001b[0mutt_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<turn>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyhocon/config_tree.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mUndefinedKey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyhocon/config_tree.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtree\u001b[0m \u001b[0mlocated\u001b[0m \u001b[0mat\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \"\"\"\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConfigTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUndefinedKey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyhocon/config_tree.py\u001b[0m in \u001b[0;36m_get\u001b[0;34m(self, key_path, key_index, default)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mUndefinedKey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mUndefinedKey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                 raise ConfigMissingException(\n\u001b[0m\u001b[1;32m    177\u001b[0m                     u\"No configuration setting found for key {key}\".format(key='.'.join(key_path[:key_index + 1])))\n\u001b[1;32m    178\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConfigMissingException\u001b[0m: 'No configuration setting found for key turn'"
     ]
    }
   ],
   "source": [
    "write = SummaryWriter(\"./logs\")\n",
    "\n",
    "model_name = 'CmbAttention'\n",
    "config = initialize_env(model_name)\n",
    "\n",
    "XDA_train, YDA_train, XDA_valid, YDA_valid, _, _, Tturn, Vturn, _ = create_DAdata(config, model_name)\n",
    "XUtt_train, YUtt_train, XUtt_valid, YUtt_valid, _, _ = create_Uttdata(config, model_name)\n",
    "\n",
    "DA_vocab = DA_to_ID(config, XDA_train+XDA_valid, YDA_train+YDA_valid)\n",
    "Utt_vocab = UTT_to_ID(config, XUtt_train+XUtt_valid, YUtt_train+YUtt_valid)\n",
    "\n",
    "XDA_train, YDA_train = DA_to_ID.tokenize(XDA_train, YDA_train)\n",
    "XDA_valid, YDA_valid = DA_to_ID.tokenize(XDA_valid, YDA_valid)\n",
    "XUtt_train, XUtt_valid = UTT_to_ID.tokenize(XUtt_train, YUtt_train)\n",
    "XUtt_valid, YUtt_valid = UTT_to_ID.tokenize(XUtt_valid, YUtt_valid)\n",
    "\n",
    "print('Finish preparing dataset...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-0c436571a504>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BATCH_SIZE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplot_train_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplot_valid_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "lr = config['lr']\n",
    "batch_size = config['BATCH_SIZE']\n",
    "plot_train_losses = []\n",
    "plot_valid_losses = []\n",
    "    \n",
    "print_total_loss = 0\n",
    "plot_total_loss = 0\n",
    "plot_total_acc = 0\n",
    "\n",
    "models, optims = select_model(experiment, utt_vocab, da_vocab, config, device, lr)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start TRAINING\")\n",
    "\n",
    "start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/model/'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(config['CmbAttention']['log_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル(Cmb Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CmbAttentionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, utt_vocab, da_vocab, config, device):\n",
    "        super(CmbAttentionModel, self).__init__()\n",
    "        \n",
    "        self.utter_encoder = UtteraceEncoder(len(utt_vocab.word2id), config['UTT_EMBED'], config['UTT_HIDDEN'])\n",
    "\n",
    "        self.context_encoder = RNNContextAwareEncoder(config['CON_EMBED'], config['CON_HIDDEN'])\n",
    "\n",
    "        self.da_encoder = RNNDAAwareEncoder(len(utt_vocab.word2id), config['DA_EMBED'], config['DA_HIDDEN'])\n",
    "\n",
    "        self.de_encoder = DenceEncoder(config['DA_HIDDEN'] + config['CON_HIDDEN'], config['DA_EMBED'], len(da_vocab.word2id))\n",
    "        \n",
    "        # self.weights = torch.tensor([1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]).cuda()\n",
    "\n",
    "        # self.cross_entropy_loss = nn.CrossEntropyLoss(reduction='mean', weight=self.weights)\n",
    "\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, X_utter, X_da, Y_da, mask, utter_hidden, context_hidden, da_hidden, turn):\n",
    "\n",
    "        utter_output, utter_weights = self.utter_encoder(X_utter, mask)\n",
    "\n",
    "        turn_output = torch.cat((utter_output, turn), dim=2)\n",
    "\n",
    "        context_output, context_weights, context_hidden = self.context_encoder(turn_output, mask, context_hidden)\n",
    "\n",
    "        da_output, da_weights, da_hidden = self.da_encoder(X_da, mask, da_hidden)\n",
    "\n",
    "        x_output = torch.cat((context_output, da_output), dim=2)\n",
    "\n",
    "        dence_output = self.de_encoder(x_output)\n",
    "\n",
    "        output = dence_output.squeeze(1)  # (batch_size, da_dim)\n",
    "        \n",
    "        Y_da = Y_da.squeeze()\n",
    "        \n",
    "        loss = self.cross_entropy_loss(output, Y_da)\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        return loss.item(), utter_hidden, context_hidden, da_hidden\n",
    "    \n",
    "    def evaluate(self, X_utter, X_da, Y_da, mask, utter_hidden, context_hidden, da_hidden, turn):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            utter_output, utter_weights = self.utter_encoder(X_utter, mask)\n",
    "\n",
    "            turn_output = torch.cat((utter_output, turn), dim=2)\n",
    "\n",
    "            context_output, context_weights, context_hidden = self.context_encoder(turn_output, mask, context_hidden)\n",
    "\n",
    "            da_output, da_weights, da_hidden = self.da_encoder(X_da, mask, da_hidden)\n",
    "\n",
    "            x_output = torch.cat((context_output, da_output), dim=2)\n",
    "\n",
    "            dence_output = self.de_encoder(x_output)\n",
    "\n",
    "            output = dence_output.squeeze(1)  # (batch_size, da_dim)\n",
    "            \n",
    "            Y_da = Y_da.squeeze(0)\n",
    "            \n",
    "            loss = self.cross_entropy_loss(output, Y_da)\n",
    "\n",
    "        return loss.item(), utter_hidden, context_hidden, da_hidden\n",
    "\n",
    "\n",
    "    def prediction(self, X_utter, X_da, mask, utter_hidden, context_hidden, da_hidden, turn):\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            utter_output, utter_weights = self.utter_encoder(X_utter, mask)\n",
    "\n",
    "            turn_output = torch.cat((utter_output, turn), dim=2)\n",
    "\n",
    "            context_output, context_weights, context_hidden = self.context_encoder(turn_output, mask, context_hidden)\n",
    "\n",
    "            da_output, da_weights, da_hidden = self.da_encoder(X_da, mask, da_hidden)\n",
    "\n",
    "            x_output = torch.cat((context_output, da_output), dim=2)\n",
    "\n",
    "            dence_output = self.de_encoder(x_output)\n",
    "\n",
    "            output = dence_output.squeeze(1)  # (batch_size, da_dim)\n",
    "\n",
    "        return output, utter_hidden, context_hidden, da_hidden, utter_weights\n",
    "\n",
    "\n",
    "    def initDAHidden(self, batch_size):\n",
    "        return self.utter_encoder.initHidden(batch_size, self.device), self.context_encoder.initHidden(batch_size, self.device), self.da_encoder.initHidden(batch_size, self.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, w_model):\n",
    "        super(WordEmbedding, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.linear = nn.Linear(embed_size, w_model)\n",
    "\n",
    "    def forward(self, x_word):\n",
    "        return torch.tanh(self.linear(self.word_embedding(x_word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DA Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, da_size, embed_size, d_model):\n",
    "        super(DAEmbedding, self).__init__()\n",
    "        self.da_embedding = nn.Embedding(da_size, embed_size)\n",
    "        self.linear = nn.Linear(embed_size, d_model)\n",
    "\n",
    "    def forward(self, x_da):\n",
    "        return torch.tanh(self.linear(self.da_embedding(x_da)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model):\n",
    "        super(Attention, self).__init__()\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        self.d_k = d_model\n",
    "       \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # 全結合層で特徴量を変換\n",
    "        k = self.k_linear(k)\n",
    "        q = self.q_linear(q)\n",
    "        v = self.v_linear(v)\n",
    "\n",
    "        # Attentionの値を計算する\n",
    "        # 各値を足し算すると大きくなりすぎるので、root(d_k)で割って調整\n",
    "        weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # ここでmaskを計算\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            weights = weights.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # softmaxで規格化をする\n",
    "        attention_weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        # AttentionをValueとかけ算\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "\n",
    "        # 全結合層で特徴量を変換\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, hidden_size, att_size):\n",
    "        super(ContextAttention, self).__init__()\n",
    "        self.q_linear = nn.Linear(att_size, att_size)\n",
    "        self.v_linear = nn.Linear(att_size, att_size)\n",
    "        self.k_linear = nn.Linear(att_size, att_size)\n",
    "\n",
    "        self.fc_1 = nn.Linear(d_model, d_model)\n",
    "        self.fc_3 = nn.Linear(hidden_size, d_model, bias=True)\n",
    "        self.fc_2 = nn.Linear(d_model, att_size)\n",
    "\n",
    "        self.fc_out = nn.Linear(att_size, hidden_size, bias=True)\n",
    "        self.d_k = att_size\n",
    "\n",
    "    def forward(self, x, mask, hidden):\n",
    "        \n",
    "        x = self.fc_2(torch.tanh(self.fc_1(x) + self.fc_3(hidden)))\n",
    "\n",
    "        q = self.q_linear(x)\n",
    "        v = self.v_linear(x)\n",
    "        k = self.k_linear(x)\n",
    "\n",
    "        weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # ここでmaskを計算\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            weights = weights.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attention_weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        att_output = torch.matmul(attention_weights, v)\n",
    "\n",
    "        output = self.fc_out(att_output)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.linear_1(x)\n",
    "\n",
    "        x = self.dropout(F.relu(x))\n",
    "\n",
    "        x = self.linear_2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositinalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len, dropout=0.1):\n",
    "        super(PositinalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenceDecoder(nn.Module):\n",
    "    def __init__(self, da_hidden, da_embed_size, da_input_size):\n",
    "        super(DenceDecoder, self).__init__()\n",
    "        self.he = nn.Linear(da_hidden, da_embed_size)\n",
    "        self.ey = nn.Linear(da_embed_size, da_input_size)\n",
    "\n",
    "    def forward(self, hidden):\n",
    "        pred = self.ey(torch.tanh(self.he(hidden)))\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-Attention => RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNContextAwareEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_dim, d_model):\n",
    "        super(RNNContextAwareEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(self.d_model+1, self.d_model)\n",
    "        self.rnn = nn.GRU(self.d_model, self.d_model, batch_first=True)\n",
    "        self.attention = ContextAttention(self.d_model, self.d_model, self.d_model)\n",
    "        self.ffn = FeedForward(self.d_model, self.d_model)\n",
    "\n",
    "    def forward(self, x, mask, hidden):\n",
    "\n",
    "        lin_output = self.linear(x)\n",
    "\n",
    "        att_output, att_weights = self.attention(lin_output, mask, hidden.transpose(0,1))        \n",
    "\n",
    "        rnn_output, rnn_hidden = self.rnn(att_output, hidden)\n",
    "\n",
    "        ffn_output = self.ffn(rnn_output)\n",
    "\n",
    "        return ffn_output, att_weights, rnn_hidden\n",
    "\n",
    "    def initHidden(self, batch_size, device):\n",
    "        return torch.zeros(1, batch_size, self.d_model).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DA Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-Attention => RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDAAwareEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, da_size, emb_dim, d_model):\n",
    "        super(RNNDAAwareEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = DAEmbedding(da_size, emb_dim, self.d_model)\n",
    "        self.rnn = nn.GRU(self.d_model, self.d_model, batch_first=True)\n",
    "        # self.attention = ContextAwareAttention(self.d_model, self.d_model, self.d_model)\n",
    "        self.attention = ContextAttention(self.d_model, self.d_model, self.d_model)\n",
    "        self.ffn = FeedForward(self.d_model, emb_dim)\n",
    "\n",
    "    def forward(self, X_da, mask, hidden):\n",
    "\n",
    "        emb_output = self.embedding(X_da)\n",
    "\n",
    "        att_output, att_weights = self.attention(emb_output, mask, hidden.transpose(0,1))        \n",
    "\n",
    "        rnn_output, rnn_hidden = self.rnn(att_output, hidden)\n",
    "\n",
    "        ffn_output = self.ffn(rnn_output)\n",
    "\n",
    "        return ffn_output, att_weights, rnn_hidden\n",
    "\n",
    "    def initHidden(self, batch_size, device):\n",
    "        return torch.zeros(1, batch_size, self.d_model).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utterance Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PE => Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UtteraceEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_dim, d_model):\n",
    "        super(UtteraceEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = WordEmbedding(vocab_size, emb_dim, self.d_model)\n",
    "        self.pe = PositinalEncoding(self.d_model, 200)\n",
    "        self.att = Attention(self.d_model)\n",
    "        self.ffn = FeedForward(d_model, emb_dim)\n",
    "        \n",
    "    def forward(self, x_utter, mask):\n",
    "\n",
    "        emb_output = self.embedding(x_utter)\n",
    "\n",
    "        pos_output = self.pe(emb_output)\n",
    "\n",
    "        att_output, att_weights = self.att(pos_output, pos_output, pos_output, mask)\n",
    "\n",
    "        ffn_output = self.ffn(att_output)\n",
    "\n",
    "        seq_len = ffn_output.size()[1]\n",
    "\n",
    "        avg_output = F.avg_pool2d(ffn_output, (seq_len, 1)) # => (128, 1, 512)\n",
    "\n",
    "        return avg_output, att_weights  # 発話ベクトル(128, 1, 512)\n",
    "\n",
    "    def initHidden(self, batch_size, device):\n",
    "        return torch.zeros(1, batch_size, self.d_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
