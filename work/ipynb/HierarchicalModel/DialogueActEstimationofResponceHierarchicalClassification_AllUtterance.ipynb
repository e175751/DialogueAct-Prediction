{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Classification SwDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pyhocon\n",
    "import torch\n",
    "import argparse\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn.functional import one_hot\n",
    "import numpy as np\n",
    "import glob\n",
    "import os, re, json\n",
    "import matplotlib.pyplot as plt\n",
    "import jsonlines\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_token = '<EOS>'\n",
    "BOS_token = '<BOS>'\n",
    "file_pattern = re.compile(r'^sw\\_([a-z]+?)\\_([0-9]+?)\\.jsonlines$')\n",
    "\n",
    "swda_tagu = {\n",
    "    '<Uninterpretable>': ['abandoned_or_turn-exit/uninterpretable', 'non-verbal'],\n",
    "    '<Statement>': ['statement-non-opinion', 'statement-opinion', 'other_answers', '3rd-party-talk', 'self-talk', 'offers,_options_commits', 'collaborative_completion'],\n",
    "    '<Question>': ['q', 'yes-no-question', 'wh-question', 'declarative_yes-no-question', 'backchannel_in_question_form', 'open-question', 'rhetorical-questions', 'signal-non-understanding', 'or-clause', 'tag-question', 'declarative_wh-question'],\n",
    "    '<Directive>': ['action-directive'],\n",
    "    '<Greeting>': ['conventional-opening', 'conventional-closing'],\n",
    "    '<Apology>': ['apology', 'no_answers', 'reject', 'negative_non-no_answers', 'dispreferred_answers', 'dispreferred_answers'],\n",
    "    '<Agreement>': ['agree/accept', 'maybe/accept-part', 'thanking'],\n",
    "    '<Understanding>': ['acknowledge_(backchannel)', 'summarize/reformulate', 'appreciation', 'response_acknowledgement', 'affirmative_non-yes_answers', 'yes_answers'],\n",
    "    '<Other>': ['other', 'hedge', 'quotation', 'repeat-phrase', 'hold_before_answer/agreement', 'downplayer']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 対話行為のID化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagu_to_ids = {\n",
    "    'None': -1,\n",
    "    '<Statement>': 0,\n",
    "    '<Understanding>': 0, \n",
    "    '<Uninterpretable>': 0, \n",
    "    '<Other1>': 1,\n",
    "    '<Other2>': 1,\n",
    "    '<Other3>': 1,\n",
    "    '<Question>': 0, \n",
    "    '<Agreement>': 1, \n",
    "    '<Apology>': 2, \n",
    "    '<Greeting>': 3, \n",
    "    '<Other>': 4, \n",
    "    '<Directive>': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DA_to_ID:\n",
    "    \n",
    "    def __init__(self, config, X_DA, Y_DA, name):\n",
    "        self.word2id = None\n",
    "        self.id2word = None\n",
    "        self.config = config\n",
    "        self.X_DA = X_DA\n",
    "        self.Y_DA = Y_DA\n",
    "        self.name = name\n",
    "        self.construct()\n",
    "        \n",
    "    def construct(self):\n",
    "#        vocab = {'<PAD>': 0}\n",
    "        vocab = {}\n",
    "        vocab_count = {}\n",
    "        \n",
    "        for x,y in zip(self.X_DA, self.Y_DA):\n",
    "            for token in x:\n",
    "                if token in vocab_count:\n",
    "                    vocab_count[token] += 1\n",
    "                else:\n",
    "                    vocab_count[token] = 1\n",
    "                    \n",
    "            for token in y:\n",
    "                if token in vocab_count:\n",
    "                    vocab_count[token] += 1\n",
    "                else:\n",
    "                    vocab_count[token] = 1\n",
    "                    \n",
    "        for k, _ in sorted(vocab_count.items(), key=lambda x: -x[1]):\n",
    "            vocab[k] = len(vocab)\n",
    "            if len(vocab) >= self.config[self.name]['MAX_VOCAB']: break\n",
    "        self.word2id = vocab\n",
    "        self.id2word = {v : k for k, v in vocab.items()}\n",
    "        return vocab\n",
    "        \n",
    "    def tokenize(self, X_tensor, Y_tensor):\n",
    "        X_Tensor = [[self.word2id[token] for token in sentence] for sentence in X_tensor]\n",
    "        Y_Tensor = [[self.word2id[token] for token in sentence] for sentence in Y_tensor]\n",
    "        return X_Tensor, Y_Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DA_to_ID_hire:\n",
    "    \n",
    "    def __init__(self, config, X_DA, Y_DA, name):\n",
    "        self.word2id = None\n",
    "        self.id2word = None\n",
    "        self.config = config\n",
    "        self.X_DA = X_DA\n",
    "        self.Y_DA = Y_DA\n",
    "        self.name = name\n",
    "        self.construct()\n",
    "        \n",
    "    def construct(self):\n",
    "\n",
    "        vocab = {}\n",
    "        vocab_count = {}\n",
    "        \n",
    "        for x,y in zip(self.X_DA, self.Y_DA):\n",
    "            for token in x:\n",
    "                vocab[token] = tagu_to_ids[token]\n",
    "            for token in y:\n",
    "                vocab[token] = tagu_to_ids[token]\n",
    "                    \n",
    "        self.word2id = vocab\n",
    "        self.id2word = {v : k for k, v in vocab.items()}\n",
    "        return vocab\n",
    "        \n",
    "    def tokenize(self, X_tensor, Y_tensor):\n",
    "        X_Tensor = [[self.word2id[token] for token in sentence] for sentence in X_tensor]\n",
    "        Y_Tensor = [[self.word2id[token] for token in sentence] for sentence in Y_tensor]\n",
    "        return X_Tensor, Y_Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 発話のID化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UTT_to_ID:\n",
    "    \n",
    "    def __init__(self, config, X_UTT, Y_UTT, name):\n",
    "        self.word2id = None\n",
    "        self.id2word = None\n",
    "        self.config = config\n",
    "        self.X_UTT = X_UTT\n",
    "        self.Y_UTT = Y_UTT\n",
    "        self.name = name\n",
    "        self.construct()\n",
    "        \n",
    "    def construct(self):\n",
    "        \n",
    "        vocab = {'<UNK>': 0, '<EOS>': 1, '<BOS>': 2, '<UttPAD>': 3, '<ConvPAD>': 4}\n",
    "        vocab_count = {}\n",
    "        \n",
    "        for x,y in zip(self.X_UTT, self.Y_UTT):\n",
    "            for seq in x:\n",
    "                for word in seq:\n",
    "                    if word in vocab_count:\n",
    "                        vocab_count[word] += 1\n",
    "                    else:\n",
    "                        vocab_count[word] = 1\n",
    "            for seq in y:\n",
    "                for word in seq:\n",
    "                    if word in vocab_count:\n",
    "                        vocab_count[word] += 1\n",
    "                    else:\n",
    "                        vocab_count[word] = 1\n",
    "                        \n",
    "        for k, _ in sorted(vocab_count.items(), key=lambda x: -x[1]):\n",
    "            vocab[k] = len(vocab)\n",
    "            if len(vocab) >= self.config[self.name]['UTT_MAX_VOCAB']: break\n",
    "        self.word2id = vocab\n",
    "        self.id2word = {v : k for k, v in vocab.items()}\n",
    "\n",
    "        return vocab\n",
    "        \n",
    "    def tokenize(self, X_tensor, Y_tensor):\n",
    "        \n",
    "        X_Tensor = [[[self.word2id[token] if token in self.word2id else self.word2id['<UNK>'] for token in seq] for seq in dialogue] for dialogue in X_tensor]\n",
    "        Y_Tensor = [[[self.word2id[token] if token in self.word2id else self.word2id['<UNK>'] for token in seq] for seq in dialogue] for dialogue in Y_tensor]\n",
    "        return X_Tensor, Y_Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニングデータ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_traindata(config, name):\n",
    "    files = [f for f in os.listdir(config[name]['train_path']) if file_pattern.match(f)]\n",
    "    # print(\"files:\" , files)\n",
    "    da_x, da_y, utt_x, utt_y, turn = [], [], [], [], []\n",
    "    da_x1, da_x2, da_x3, da_x4 = [], [], [], []\n",
    "    da_y1, da_y2, da_y3, da_y4 = [], [], [], []\n",
    "    # 1file 1conversation\n",
    "    for filename in files:\n",
    "        # print(os.path.join(config['train_path'], filename))\n",
    "        with open(os.path.join(config[name]['train_path'], filename), 'r') as f:\n",
    "            data = f.read().split('\\n')\n",
    "            data.remove('')\n",
    "            da_seq, utt_seq, turn_seq = [], [], []\n",
    "            da1_seq, da2_seq, da3_seq, da4_seq = [], [], [], []\n",
    "            # 1line 1turn\n",
    "            for idx, line in enumerate(data, 1):\n",
    "                jsondata = json.loads(line)\n",
    "                # single-turn multi dialogue case\n",
    "                for da, utt in zip(jsondata['DA'], jsondata['sentence']):\n",
    "                    da_seq.append(da)\n",
    "                    utt_seq.append(utt.split(' '))\n",
    "                    turn_seq.append(0)\n",
    "                for da1, da2, da3, da4 in zip(jsondata['DA1'], jsondata['DA2'], jsondata['DA3'], jsondata['DA4']):\n",
    "                    da1_seq.append(da1)\n",
    "                    da2_seq.append(da2)\n",
    "                    da3_seq.append(da3)\n",
    "                    da4_seq.append(da4)\n",
    "                        \n",
    "                turn_seq[-1] = 1\n",
    "            da_seq = [easy_damsl(da) for da in da_seq]\n",
    "            \n",
    "        if config[name]['state']:\n",
    "            for i in range(max(1, len(da_seq) - 1 - config[name]['window_size'])):\n",
    "                ## 発話の対話行為\n",
    "                da_x.append(da_seq[i:min(len(da_seq)-1, i + config[name]['window_size'])])\n",
    "                da_x1.append(da1_seq[i:min(len(da1_seq)-1, i + config[name]['window_size'])])\n",
    "                da_x2.append(da2_seq[i:min(len(da2_seq)-1, i + config[name]['window_size'])])\n",
    "                da_x3.append(da3_seq[i:min(len(da3_seq)-1, i + config[name]['window_size'])])\n",
    "                da_x4.append(da4_seq[i:min(len(da4_seq)-1, i + config[name]['window_size'])])\n",
    "                \n",
    "                ## 応答の対話行為\n",
    "                da_y.append(da_seq[1 + i:min(len(da_seq), 1 + i + config[name]['window_size'])])\n",
    "                da_y1.append(da1_seq[1 + i:min(len(da1_seq), 1 + i + config[name]['window_size'])])\n",
    "                da_y2.append(da2_seq[1 + i:min(len(da2_seq), 1 + i + config[name]['window_size'])])\n",
    "                da_y3.append(da3_seq[1 + i:min(len(da3_seq), 1 + i + config[name]['window_size'])])\n",
    "                da_y4.append(da4_seq[1 + i:min(len(da4_seq), 1 + i + config[name]['window_size'])])\n",
    "                \n",
    "                ## 対話文\n",
    "                utt_x.append(utt_seq[i:min(len(da_seq)-1, i + config[name]['window_size'])])\n",
    "                utt_y.append(utt_seq[1 + i:min(len(da_seq), 1 + i + config[name]['window_size'])])\n",
    "                \n",
    "                ## ターン制\n",
    "                turn.append(turn_seq[i:min(len(da_seq), i + config[name]['window_size'])])\n",
    "    \n",
    "    return da_x, da_x1, da_x2, da_x3, da_x4, da_y, da_y1, da_y2, da_y3, da_y4, utt_x, utt_y, turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def easy_damsl(tag):\n",
    "    easy_tag = [k for k, v in swda_tagu.items() if tag in v]\n",
    "    return easy_tag[0] if not len(easy_tag) < 1 else tag\n",
    "\n",
    "def separate_data(x, y, turn):\n",
    "    split_size = round(len(x) / 10)\n",
    "    if split_size == 0: split_size = 1\n",
    "    X_train, Y_train, Tturn = x[split_size * 2:], y[split_size * 2:], turn[split_size * 2:]\n",
    "    X_valid, Y_valid, Vturn = x[split_size: split_size * 2], y[split_size: split_size * 2], turn[split_size: split_size * 2]\n",
    "    X_test, Y_test, Testturn = x[:split_size], y[:split_size], turn[:split_size]\n",
    "    assert len(X_train) == len(Y_train), 'Unexpect to separate train data'\n",
    "    return X_train, Y_train, X_valid, Y_valid, X_test, Y_test, Tturn, Vturn, Testturn\n",
    "\n",
    "\n",
    "def separate_data_da(x, y):\n",
    "    split_size = round(len(x) / 10)\n",
    "    if split_size == 0: split_size = 1\n",
    "    X_train, Y_train = x[split_size * 2:], y[split_size * 2:]\n",
    "    X_valid, Y_valid = x[split_size: split_size * 2], y[split_size: split_size * 2]\n",
    "    X_test, Y_test = x[:split_size], y[:split_size]\n",
    "    assert len(X_train) == len(Y_train), 'Unexpect to separate train data'\n",
    "    return X_train, Y_train, X_valid, Y_valid, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_env(name):\n",
    "    config = pyhocon.ConfigFactory.parse_file('./dialogue.conf')\n",
    "    config['log_dirs'] = os.path.join(config[name]['log_dir'])\n",
    "    if not os.path.exists(config['log_dirs']):\n",
    "        os.mkdir(config['log_dirs'])\n",
    "     \n",
    "    return config\n",
    "\n",
    "def create_DAdata(config, name):\n",
    "    da_x, da_x1, da_x2, da_x3, da_x4, da_y, da_y1, da_y2, da_y3, da_y4, _, _, _ = create_traindata(config, name)\n",
    "    return da_x, da_x1, da_x2, da_x3, da_x4, da_y, da_y1, da_y2, da_y3, da_y4\n",
    "\n",
    "def create_Uttdata(config, name):\n",
    "    _, _, _, _, _, _, _, _, _, _, posts, cmnts, turn = create_traindata(config, name)\n",
    "    X_train, Y_train, X_valid, Y_valid, X_test, Y_test, Tturn, Vturn, Testturn = separate_data(posts, cmnts, turn)\n",
    "    return X_train, Y_train, X_valid, Y_valid, X_test, Y_test, Tturn, Vturn, Testturn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"CmbAttention\"\n",
    "loss_name=\"HireCE_All\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "write = SummaryWriter(\"./logs\")\n",
    "config = initialize_env(model_name+loss_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_x, da_x1, da_x2, da_x3, da_x4, da_y, da_y1, da_y2, da_y3, da_y4 = create_DAdata(config, model_name+loss_name)\n",
    "XDA_train, YDA_train, XDA_valid, YDA_valid, _, _ = separate_data_da(da_x, da_y)\n",
    "XDA1_train, YDA1_train, XDA1_valid, YDA1_valid, _, _ = separate_data_da(da_x1, da_y1)\n",
    "XDA2_train, YDA2_train, XDA2_valid, YDA2_valid, _, _ = separate_data_da(da_x2, da_y2)\n",
    "XDA3_train, YDA3_train, XDA3_valid, YDA3_valid, _, _ = separate_data_da(da_x3, da_y3)\n",
    "XDA4_train, YDA4_train, XDA4_valid, YDA4_valid, _, _ = separate_data_da(da_x4, da_y4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DA_vocab = DA_to_ID(config, XDA_train+XDA_valid, YDA_train+YDA_valid, model_name+loss_name)\n",
    "DA1_vocab = DA_to_ID_hire(config, XDA1_train+XDA1_valid, YDA1_train+YDA1_valid, model_name+loss_name)\n",
    "DA2_vocab = DA_to_ID_hire(config, XDA2_train+XDA2_valid, YDA2_train+YDA2_valid, model_name+loss_name)\n",
    "DA3_vocab = DA_to_ID_hire(config, XDA3_train+XDA3_valid, YDA3_train+YDA3_valid, model_name+loss_name)\n",
    "DA4_vocab = DA_to_ID_hire(config, XDA4_train+XDA4_valid, YDA4_train+YDA4_valid, model_name+loss_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "XUtt_train, YUtt_train, XUtt_valid, YUtt_valid, _, _, Tturn, Vturn, _ = create_Uttdata(config, model_name+loss_name)\n",
    "Utt_vocab = UTT_to_ID(config, XUtt_train+XUtt_valid, YUtt_train+YUtt_valid, model_name+loss_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 対話行為\n",
    "XDA_train, YDA_train = DA_vocab.tokenize(XDA_train, YDA_train)\n",
    "XDA1_train, YDA1_train = DA1_vocab.tokenize(XDA1_train, YDA1_train)\n",
    "XDA2_train, YDA2_train = DA2_vocab.tokenize(XDA2_train, YDA2_train)\n",
    "XDA3_train, YDA3_train = DA3_vocab.tokenize(XDA3_train, YDA3_train)\n",
    "XDA4_train, YDA4_train = DA4_vocab.tokenize(XDA4_train, YDA4_train)\n",
    "\n",
    "XDA_valid, YDA_valid = DA_vocab.tokenize(XDA_valid, YDA_valid)\n",
    "XDA1_valid, YDA1_valid = DA1_vocab.tokenize(XDA1_valid, YDA1_valid)\n",
    "XDA2_valid, YDA2_valid = DA2_vocab.tokenize(XDA2_valid, YDA2_valid)\n",
    "XDA3_valid, YDA3_valid = DA3_vocab.tokenize(XDA3_valid, YDA3_valid)\n",
    "XDA4_valid, YDA4_valid = DA4_vocab.tokenize(XDA4_valid, YDA4_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 発話文\n",
    "XUtt_train, YUtt_train = Utt_vocab.tokenize(XUtt_train, YUtt_train)\n",
    "XUtt_valid, YUtt_valid = Utt_vocab.tokenize(XUtt_valid, YUtt_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "XDA_valid = list(filter(None, XDA_valid))\n",
    "XDA1_valid = list(filter(None, XDA1_valid))\n",
    "XDA2_valid = list(filter(None, XDA2_valid))\n",
    "XDA3_valid = list(filter(None, XDA3_valid))\n",
    "XDA4_valid = list(filter(None, XDA4_valid))\n",
    "XUtt_valid = list(filter(None, XUtt_valid))\n",
    "\n",
    "YDA_valid = list(filter(None, YDA_valid))\n",
    "YDA1_valid = list(filter(None, YDA1_valid))\n",
    "YDA2_valid = list(filter(None, YDA2_valid))\n",
    "YDA3_valid = list(filter(None, YDA3_valid))\n",
    "YDA4_valid = list(filter(None, YDA4_valid))\n",
    "YUtt_valid = list(filter(None, YUtt_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish preparing dataset...\n"
     ]
    }
   ],
   "source": [
    "print('Finish preparing dataset...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CmbAttentionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_name, utt_vocab, da_vocab, config, device):\n",
    "        super(CmbAttentionModel, self).__init__()\n",
    "        \n",
    "        self.total_layer = config[model_name]['total_layer']\n",
    "        \n",
    "        self.loss_fun = nn.CrossEntropyLoss().cuda()\n",
    "        \n",
    "        self.utter_encoder = UtteraceEncoder(len(utt_vocab.word2id), config[model_name]['UTT_EMBED'], config[model_name]['UTT_HIDDEN'])\n",
    "\n",
    "        self.context_encoder = RNNContextAwareEncoder(config[model_name]['CON_EMBED'], config[model_name]['CON_HIDDEN'])\n",
    "\n",
    "        self.da_encoder = RNNDAAwareEncoder(len(utt_vocab.word2id), config[model_name]['DA_EMBED'], config[model_name]['DA_HIDDEN'])\n",
    "        \n",
    "        self.classify = HierarchicelClassification(config[model_name]['DA_HIDDEN'] + config[model_name]['CON_HIDDEN'])\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, X_utter, X_da, Yda, mask, context_hidden, da_hidden, turn):\n",
    "\n",
    "        utter_output, utter_weights = self.utter_encoder(X_utter, mask)\n",
    "\n",
    "        turn_output = torch.cat((utter_output, turn), dim=2)\n",
    "\n",
    "        context_output, context_weights, context_hidden = self.context_encoder(turn_output, mask, context_hidden)\n",
    "\n",
    "        da_output, da_weights, da_hidden = self.da_encoder(X_da, mask, da_hidden)\n",
    "\n",
    "        x_output = torch.cat((context_output, da_output), dim=2)\n",
    "        \n",
    "        pred1, pred2, pred3, pred4 = self.classify(x_output, Yda) \n",
    "        \n",
    "        return pred1, pred2, pred3, pred4, context_hidden, da_hidden\n",
    "    \n",
    "    def validtion(self, X_utter, X_da, Yda, mask, context_hidden, da_hidden, turn):\n",
    "\n",
    "        utter_output, utter_weights = self.utter_encoder(X_utter, mask)\n",
    "\n",
    "        turn_output = torch.cat((utter_output, turn), dim=2)\n",
    "\n",
    "        context_output, context_weights, context_hidden = self.context_encoder(turn_output, mask, context_hidden)\n",
    "\n",
    "        da_output, da_weights, da_hidden = self.da_encoder(X_da, mask, da_hidden)\n",
    "\n",
    "        x_output = torch.cat((context_output, da_output), dim=2)\n",
    "        \n",
    "        pred1, pred2, pred3, pred4 = self.classify(x_output, Yda)\n",
    "        \n",
    "        return pred1, pred2, pred3, pred4, context_hidden, da_hidden\n",
    "\n",
    "\n",
    "    def prediction(self, X_utter, X_da, mask, context_hidden, da_hidden, turn):\n",
    "\n",
    "        utter_output, _ = self.utter_encoder(X_utter, mask)\n",
    "\n",
    "        turn_output = torch.cat((utter_output, turn), dim=2)\n",
    "\n",
    "        context_output, context_weights, context_hidden = self.context_encoder(turn_output, mask, context_hidden)\n",
    "\n",
    "        da_output, da_weights, da_hidden = self.da_encoder(X_da, mask, da_hidden)\n",
    "\n",
    "        x_output = torch.cat((context_output, da_output), dim=2)\n",
    "        \n",
    "        pred1, pred2, pred3, pred4 = self.classify.test(x_output)\n",
    "\n",
    "        return pred1, pred2, pred3, pred4, context_hidden, da_hidden\n",
    "\n",
    "\n",
    "    def initDAHidden(self, batch_size):\n",
    "        \n",
    "        return self.utter_encoder.initHidden(batch_size, self.device), self.context_encoder.initHidden(batch_size, self.device), self.da_encoder.initHidden(batch_size, self.device)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Model (Hierarchical Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicelClassification(nn.Module):\n",
    "    def __init__(self, CDhidden, num_classes=[2,2,2,6]):\n",
    "        super(HierarchicelClassification, self).__init__()\n",
    "        \n",
    "        self.linear_layer1 = nn.Linear(CDhidden, num_classes[0])\n",
    "        self.linear_layer2 = nn.Linear(CDhidden, num_classes[1])\n",
    "        self.linear_layer3 = nn.Linear(CDhidden, num_classes[2])\n",
    "        self.linear_layer4 = nn.Linear(CDhidden, num_classes[3])\n",
    "        \n",
    "        self.softmax_layer1 = nn.Linear(num_classes[0], num_classes[0])\n",
    "        self.softmax_layer2 = nn.Linear(num_classes[0]+num_classes[1], num_classes[1])\n",
    "        self.softmax_layer3 = nn.Linear(num_classes[0]+num_classes[1]+num_classes[2], num_classes[2])\n",
    "        self.softmax_layer4 = nn.Linear(num_classes[0]+num_classes[1]+num_classes[2]+num_classes[3], num_classes[3])\n",
    "        \n",
    "    def forward(self, x_output, Yda):\n",
    "    \n",
    "        level1_output = self.softmax_layer1(self.linear_layer1(x_output))\n",
    "        if Yda[1] != -1:\n",
    "            level2_output = self.softmax_layer2(torch.cat([level1_output, self.linear_layer2(x_output)], dim=2))\n",
    "            if Yda[2] != -1:\n",
    "                level3_output = self.softmax_layer3(torch.cat([level1_output, level2_output, self.linear_layer3(x_output)], dim=2))\n",
    "                if Yda[3] != -1:\n",
    "                    level4_output = self.softmax_layer4(torch.cat([level1_output, level2_output, level3_output, self.linear_layer4(x_output)], dim=2))\n",
    "                    return level1_output.squeeze(1), level2_output.squeeze(1), level3_output.squeeze(1), level4_output.squeeze(1)\n",
    "                else:\n",
    "                    return level1_output.squeeze(1), level2_output.squeeze(1), level3_output.squeeze(1), None\n",
    "            else:\n",
    "                return level1_output.squeeze(1), level2_output.squeeze(1), None, None\n",
    "        else:\n",
    "            return level1_output.squeeze(1), None, None, None\n",
    "        \n",
    "    def test(self, x_output):\n",
    "        \n",
    "        level1_output = self.softmax_layer1(self.linear_layer1(x_output))\n",
    "        pred1 = torch.argmax(level1_output.squeeze(1))\n",
    "        if pred1==0:\n",
    "            return pred1, None, None, None\n",
    "        else:\n",
    "            level2_output = self.softmax_layer2(torch.cat([level1_output, self.linear_layer2(x_output)], dim=2))\n",
    "            pred2 = torch.argmax(level2_output.squeeze(1))\n",
    "            if pred2==0:\n",
    "                return pred1, pred2, None, None\n",
    "            else:\n",
    "                level3_output = self.softmax_layer3(torch.cat([level1_output, level2_output, self.linear_layer3(x_output)], dim=2))\n",
    "                pred3 = torch.argmax(level3_output.squeeze(1))\n",
    "                if pred3==0:\n",
    "                    return pred1, pred2, pred3, None\n",
    "                else:\n",
    "                    level4_output = self.softmax_layer4(torch.cat([level1_output, level2_output, level3_output, self.linear_layer4(x_output)], dim=2))\n",
    "                    pred4 = torch.argmax(level4_output.squeeze(1))\n",
    "                    return pred1, pred2, pred3, pred4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utterance Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UtteraceEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_dim, d_model):\n",
    "        super(UtteraceEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = WordEmbedding(vocab_size, emb_dim, self.d_model)\n",
    "        self.pe = PositinalEncoding(self.d_model, 200)\n",
    "        self.att = Attention(self.d_model)\n",
    "        self.ffn = FeedForward(d_model, emb_dim)\n",
    "        \n",
    "    def forward(self, x_utter, mask):\n",
    "\n",
    "        emb_output = self.embedding(x_utter)\n",
    "\n",
    "        pos_output = self.pe(emb_output)\n",
    "\n",
    "        att_output, att_weights = self.att(pos_output, pos_output, pos_output, mask)\n",
    "\n",
    "        ffn_output = self.ffn(att_output)\n",
    "\n",
    "        seq_len = ffn_output.size()[1]\n",
    "\n",
    "        avg_output = F.avg_pool2d(ffn_output, (seq_len, 1)) # => (128, 1, 512)\n",
    "\n",
    "        return avg_output, att_weights  # 発話ベクトル(128, 1, 512)\n",
    "\n",
    "    def initHidden(self, batch_size, device):\n",
    "        return torch.zeros(1, batch_size, self.d_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.linear_1(x)\n",
    "\n",
    "        x = self.dropout(F.relu(x))\n",
    "\n",
    "        x = self.linear_2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model):\n",
    "        super(Attention, self).__init__()\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        self.d_k = d_model\n",
    "       \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # 全結合層で特徴量を変換\n",
    "        k = self.k_linear(k)\n",
    "        q = self.q_linear(q)\n",
    "        v = self.v_linear(v)\n",
    "\n",
    "        # Attentionの値を計算する\n",
    "        # 各値を足し算すると大きくなりすぎるので、root(d_k)で割って調整\n",
    "        weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # ここでmaskを計算\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            weights = weights.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # softmaxで規格化をする\n",
    "        attention_weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        # AttentionをValueとかけ算\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "\n",
    "        # 全結合層で特徴量を変換\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositinalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len, dropout=0.1):\n",
    "        super(PositinalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, w_model):\n",
    "        super(WordEmbedding, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.linear = nn.Linear(embed_size, w_model)\n",
    "\n",
    "    def forward(self, x_word):\n",
    "        return torch.tanh(self.linear(self.word_embedding(x_word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNContextAwareEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_dim, d_model):\n",
    "        super(RNNContextAwareEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(self.d_model+1, self.d_model)\n",
    "        self.rnn = nn.GRU(self.d_model, self.d_model, batch_first=True)\n",
    "        self.attention = ContextAttention(self.d_model, self.d_model, self.d_model)\n",
    "        self.ffn = FeedForward(self.d_model, self.d_model)\n",
    "\n",
    "    def forward(self, x, mask, hidden):\n",
    "\n",
    "        lin_output = self.linear(x)\n",
    "\n",
    "        att_output, att_weights = self.attention(lin_output, mask, hidden.transpose(0,1))        \n",
    "\n",
    "        rnn_output, rnn_hidden = self.rnn(att_output, hidden)\n",
    "\n",
    "        ffn_output = self.ffn(rnn_output)\n",
    "\n",
    "        return ffn_output, att_weights, rnn_hidden\n",
    "\n",
    "    def initHidden(self, batch_size, device):\n",
    "        return torch.zeros(1, batch_size, self.d_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, hidden_size, att_size):\n",
    "        super(ContextAttention, self).__init__()\n",
    "        self.q_linear = nn.Linear(att_size, att_size)\n",
    "        self.v_linear = nn.Linear(att_size, att_size)\n",
    "        self.k_linear = nn.Linear(att_size, att_size)\n",
    "\n",
    "        self.fc_1 = nn.Linear(d_model, d_model)\n",
    "        self.fc_3 = nn.Linear(hidden_size, d_model, bias=True)\n",
    "        self.fc_2 = nn.Linear(d_model, att_size)\n",
    "\n",
    "        self.fc_out = nn.Linear(att_size, hidden_size, bias=True)\n",
    "        self.d_k = att_size\n",
    "\n",
    "    def forward(self, x, mask, hidden):\n",
    "        \n",
    "        x = self.fc_2(torch.tanh(self.fc_1(x) + self.fc_3(hidden)))\n",
    "\n",
    "        q = self.q_linear(x)\n",
    "        v = self.v_linear(x)\n",
    "        k = self.k_linear(x)\n",
    "\n",
    "        weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # ここでmaskを計算\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            weights = weights.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attention_weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        att_output = torch.matmul(attention_weights, v)\n",
    "\n",
    "        output = self.fc_out(att_output)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNDAAwareEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDAAwareEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, da_size, emb_dim, d_model):\n",
    "        super(RNNDAAwareEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = DAEmbedding(da_size, emb_dim, self.d_model)\n",
    "        self.rnn = nn.GRU(self.d_model, self.d_model, batch_first=True)\n",
    "        self.attention = ContextAttention(self.d_model, self.d_model, self.d_model)\n",
    "        self.ffn = FeedForward(self.d_model, emb_dim)\n",
    "\n",
    "    def forward(self, X_da, mask, hidden):\n",
    "\n",
    "        emb_output = self.embedding(X_da)\n",
    "\n",
    "        att_output, att_weights = self.attention(emb_output, mask, hidden.transpose(0,1))        \n",
    "\n",
    "        rnn_output, rnn_hidden = self.rnn(att_output, hidden)\n",
    "\n",
    "        ffn_output = self.ffn(rnn_output)\n",
    "\n",
    "        return ffn_output, att_weights, rnn_hidden\n",
    "\n",
    "    def initHidden(self, batch_size, device):\n",
    "        return torch.zeros(1, batch_size, self.d_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, da_size, embed_size, d_model):\n",
    "        super(DAEmbedding, self).__init__()\n",
    "        self.da_embedding = nn.Embedding(da_size, embed_size)\n",
    "        self.linear = nn.Linear(embed_size, d_model)\n",
    "\n",
    "    def forward(self, x_da):\n",
    "        return torch.tanh(self.linear(self.da_embedding(x_da)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 損失関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalLossNetwork:\n",
    "    '''Logics to calculate the loss of the model.\n",
    "    '''\n",
    "    def __init__(self, device='cpu', total_level=4, alpha=1, beta=0.8, p_loss=3):\n",
    "        '''Param init.\n",
    "        '''\n",
    "        self.total_level = total_level\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.p_loss = p_loss\n",
    "        self.device = device\n",
    "\n",
    "    def calculate_lloss(self, predictions, true_labels):\n",
    "        '''Calculates the layer loss.\n",
    "        '''\n",
    "        lloss = 0\n",
    "        for i in range(self.total_level):\n",
    "            \n",
    "            if predictions[i]==None:\n",
    "                return lloss\n",
    "            if i != 3:\n",
    "                true_onehot = one_hot(true_labels[i], num_classes=2)[0]\n",
    "                pred_vec = predictions[i][0]\n",
    "                \n",
    "                lloss += nn.BCELoss()(nn.Sigmoid()(pred_vec), true_onehot.to(torch.float))\n",
    "            else:\n",
    "                lloss += nn.CrossEntropyLoss()(predictions[i], true_labels[i])\n",
    "            \n",
    "        return lloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = config[model_name+loss_name]['lr']\n",
    "config[model_name+loss_name]['BATCH_SIZE']=1\n",
    "batch_size = config[model_name+loss_name]['BATCH_SIZE']\n",
    "\n",
    "model = CmbAttentionModel(model_name+loss_name, Utt_vocab, DA_vocab, config, device).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr)\n",
    "\n",
    "total_layer = config[model_name+loss_name]['total_layer']\n",
    "HLN = HierarchicalLossNetwork(device=device, total_level=total_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(model_name, loss_name, models, optims):\n",
    "\n",
    "    plot_train_losses = []\n",
    "    plot_valid_losses = []\n",
    "    print_total_loss = 0\n",
    "    plot_total_loss = 0\n",
    "    plot_total_acc = 0\n",
    "    _valid_loss = None\n",
    "    config[model_name+loss_name]['BATCH_SIZE']=1\n",
    "    batch_size = config[model_name+loss_name]['BATCH_SIZE']\n",
    "    \n",
    "    print('{} start TRAINING'.format(model_name+loss_name))\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(config[model_name+loss_name]['EPOCH']):\n",
    "\n",
    "        tmp_time = time.time()\n",
    "        print('Epoch {} start'.format(epoch+1))\n",
    "        index = [i for i in range(len(XDA_train))]\n",
    "        random.shuffle(index)\n",
    "        idx=0\n",
    "\n",
    "        while idx < len(index):\n",
    "            step_size = min(batch_size, len(index)-idx)\n",
    "            batch_idx = index[idx:idx+step_size]\n",
    "            utter_hidden, context_hidden, da_hidden = models.initDAHidden(step_size)\n",
    "            classification = HierarchicelClassification(640)\n",
    "\n",
    "            print('\\rConversation {}/{} training...'.format(idx + step_size, len(XDA_train)), end='')\n",
    "            Xda_seq = [XDA_train[seq_idx] for seq_idx in batch_idx]\n",
    "            Yda_seq = [YDA_train[seq_idx] for seq_idx in batch_idx]\n",
    "            Yda1_seq = [YDA1_train[seq_idx] for seq_idx in batch_idx]\n",
    "            Yda2_seq = [YDA2_train[seq_idx] for seq_idx in batch_idx]\n",
    "            Yda3_seq = [YDA3_train[seq_idx] for seq_idx in batch_idx]\n",
    "            Yda4_seq = [YDA4_train[seq_idx] for seq_idx in batch_idx]\n",
    "            turn_seq = [Tturn[seq_idx] for seq_idx in batch_idx]\n",
    "            max_conv_len = max(len(s) for s in Xda_seq) \n",
    "\n",
    "            ## \n",
    "            XUtt_seq = [XUtt_train[seq_idx] for seq_idx in batch_idx]\n",
    "            YUtt_seq = [YUtt_train[seq_idx] for seq_idx in batch_idx]\n",
    "\n",
    "            for i in range(len(XUtt_seq)):\n",
    "                XUtt_seq[i] = XUtt_seq[i] + [[Utt_vocab.word2id['<ConvPAD>']]] * (max_conv_len - len(XUtt_seq[i]))\n",
    "                YUtt_seq[i] = YUtt_seq[i] + [[Utt_vocab.word2id['<ConvPAD>']]] * (max_conv_len - len(YUtt_seq[i]))\n",
    "\n",
    "            for ci in range(len(Xda_seq)):\n",
    "\n",
    "                turn_seq[ci] = turn_seq[ci] + [0] * (max_conv_len - len(turn_seq[ci]))\n",
    "                Xda_seq[ci] = Xda_seq[ci] + [0] * (max_conv_len - len(Xda_seq[ci]))\n",
    "                Yda_seq[ci] = Yda_seq[ci] + [0] * (max_conv_len - len(Yda_seq[ci]))\n",
    "                Yda1_seq[ci] = Yda1_seq[ci] + [0] * (max_conv_len - len(Yda1_seq[ci]))\n",
    "                Yda2_seq[ci] = Yda2_seq[ci] + [0] * (max_conv_len - len(Yda2_seq[ci]))\n",
    "                Yda3_seq[ci] = Yda3_seq[ci] + [0] * (max_conv_len - len(Yda3_seq[ci]))\n",
    "                Yda4_seq[ci] = Yda4_seq[ci] + [0] * (max_conv_len - len(Yda4_seq[ci]))\n",
    "\n",
    "            for i in range(0, max_conv_len):\n",
    "                \n",
    "                last = True if i == max_conv_len - 1 else False\n",
    "                \n",
    "                Xda_tensor = torch.tensor([[X[i]] for X in Xda_seq]).to(device)\n",
    "                Yda_tensor = torch.tensor([[Y[i]] for Y in Yda_seq]).to(device)\n",
    "                Yda1_tensor = torch.tensor([[Y[i]] for Y in Yda1_seq]).to(device)\n",
    "                Yda2_tensor = torch.tensor([[Y[i]] for Y in Yda2_seq]).to(device)\n",
    "                Yda3_tensor = torch.tensor([[Y[i]] for Y in Yda3_seq]).to(device)\n",
    "                Yda4_tensor = torch.tensor([[Y[i]] for Y in Yda4_seq]).to(device)\n",
    "                \n",
    "                turn_tensor = torch.tensor([[t[i]] for t in turn_seq]).to(device)\n",
    "                turn_tensor = turn_tensor.float()\n",
    "                turn_tensor = turn_tensor.unsqueeze(1)    \n",
    "\n",
    "                ### \n",
    "                max_seq_len = max(len(XU[i]) + 1 for XU in XUtt_seq)\n",
    "                \n",
    "                ### Padding処理\n",
    "                for ci in range(len(XUtt_seq)):\n",
    "                    XUtt_seq[ci][i] = XUtt_seq[ci][i] + [Utt_vocab.word2id['<UttPAD>']] * (max_seq_len - len(XUtt_seq[ci][i]))\n",
    "                    YUtt_seq[ci][i] = YUtt_seq[ci][i] + [Utt_vocab.word2id['<UttPAD>']] * (max_seq_len - len(YUtt_seq[ci][i]))\n",
    "                XUtt_tensor = torch.tensor([XU[i] for XU in XUtt_seq]).to(device)\n",
    "                YUtt_tensor = None\n",
    "                level1_pred, level2_pred, level3_pred, level4_pred, context_hidden, da_hidden = models(XUtt_tensor, Xda_tensor, [Yda1_tensor.squeeze(1), Yda2_tensor.squeeze(1), Yda3_tensor.squeeze(1), Yda4_tensor.squeeze(1)], None, context_hidden, da_hidden, turn_tensor)\n",
    "             \n",
    "                lloss = HLN.calculate_lloss([level1_pred, level2_pred, level3_pred, level4_pred], [Yda1_tensor.squeeze(1), Yda2_tensor.squeeze(1), Yda3_tensor.squeeze(1), Yda4_tensor.squeeze(1)])\n",
    "                optims.zero_grad()\n",
    "                lloss.backward(retain_graph=True)\n",
    "                print_total_loss += lloss.item()    \n",
    "                \n",
    "                if last:\n",
    "                    optims.step()\n",
    "                    \n",
    "            print_total_loss/=max_conv_len  \n",
    "            idx += step_size\n",
    "\n",
    "        \n",
    "        valid_loss = validation(XDA_valid, YDA1_valid, YDA2_valid, YDA3_valid, YDA4_valid, XUtt_valid, models, device, config, Vturn)\n",
    "\n",
    "\n",
    "        def save_model(filename):\n",
    "            torch.save(models.state_dict(), os.path.join(config[model_name+loss_name]['log_dir'], config[model_name+loss_name]['SAVE_NAME'] + \"_v1\" + str(config[model_name+loss_name]['window_size']) + \"_v\" + \".model\".format(filename)))\n",
    "\n",
    "        print(\"steps %d\\tloss %.4f\\tvalid loss %.4f | exec time %.4f\" % (epoch+1, print_total_loss, valid_loss, time.time()-tmp_time))\n",
    "        plot_train_losses.append(print_total_loss)\n",
    "        plot_valid_losses.append(valid_loss)\n",
    "        print_total_loss = 0\n",
    "\n",
    "        if _valid_loss == None:\n",
    "            save_model(\"model_save\")\n",
    "            print(\"Model Saved\")\n",
    "            _valid_loss = valid_loss\n",
    "        else:\n",
    "            if valid_loss<_valid_loss:\n",
    "                _valid_loss = valid_loss\n",
    "                save_model(\"model_save\")\n",
    "                print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validation(X_valid, Y1_valid, Y2_valid, Y3_valid, Y4_valid, XU_valid, model, device, config, turn):\n",
    "\n",
    "    total_loss = 0\n",
    "    idx = 0\n",
    "    \n",
    "    for seq_idx in range(len(X_valid)):\n",
    "        print('\\r{}/{} conversation evaluating'.format(seq_idx+1, len(X_valid)), end='')\n",
    "        utter_hidden, context_hidden, da_hidden = model.initDAHidden(1)\n",
    "        \n",
    "        X_seq = X_valid[seq_idx]\n",
    "        \n",
    "        Y1_seq = Y1_valid[seq_idx]\n",
    "        Y2_seq = Y2_valid[seq_idx]\n",
    "        Y3_seq = Y3_valid[seq_idx]\n",
    "        Y4_seq = Y4_valid[seq_idx]\n",
    "        \n",
    "        turn[seq_idx] = turn[seq_idx] + [0] * (len(X_seq) - len(turn[seq_idx]))\n",
    "        turn_seq = turn[seq_idx]\n",
    "        XU_seq = XU_valid[seq_idx]\n",
    "        \n",
    "\n",
    "        assert len(X_seq) == len(Y1_seq), 'Unexpect sequence len in evaluate {} != {}'.format(len(X_seq), len(Y1_seq))\n",
    "        \n",
    "        for i in range(0, len(X_seq)):\n",
    "            X_tensor = torch.tensor([[X_seq[i]]]).to(device)\n",
    "            Y1_tensor = torch.tensor([[Y1_seq[i]]]).to(device)\n",
    "            Y2_tensor = torch.tensor([[Y2_seq[i]]]).to(device)\n",
    "            Y3_tensor = torch.tensor([[Y3_seq[i]]]).to(device)\n",
    "            Y4_tensor = torch.tensor([[Y4_seq[i]]]).to(device)\n",
    "            turn_tensor = torch.tensor([[turn_seq[i]]]).to(device)\n",
    "            turn_tensor = turn_tensor.float()\n",
    "            turn_tensor = turn_tensor.unsqueeze(1)   \n",
    "            XU_tensor = torch.tensor([XU_seq[i]]).to(device)\n",
    "            \n",
    "            level1_pred, level2_pred, level3_pred, level4_pred, context_hidden, da_hidden = model.validtion(XU_tensor, X_tensor, [Y1_tensor.squeeze(1), Y2_tensor.squeeze(1), Y3_tensor.squeeze(1), Y4_tensor.squeeze(1)], None, context_hidden, da_hidden, turn_tensor)\n",
    "            lloss = HLN.calculate_lloss([level1_pred, level2_pred, level3_pred, level4_pred], [Y1_tensor.squeeze(1), Y2_tensor.squeeze(1), Y3_tensor.squeeze(1), Y4_tensor.squeeze(1) ])\n",
    "            total_loss += lloss.item()\n",
    "        \n",
    "        total_loss/=len(X_seq)\n",
    "        \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CmbAttentionHireCE_All start TRAINING\n",
      "Epoch 1 start\n",
      "20003/20003 conversation evaluatingsteps 1\tloss 1.3653\tvalid loss 2.1535 | exec time 9779.9181\n",
      "Model Saved\n",
      "Epoch 2 start\n",
      "20003/20003 conversation evaluatingsteps 2\tloss 1.6230\tvalid loss 2.1977 | exec time 9113.1708\n",
      "Epoch 3 start\n",
      "20003/20003 conversation evaluatingsteps 3\tloss 1.7531\tvalid loss 2.1828 | exec time 9101.4102\n",
      "Epoch 4 start\n",
      "20003/20003 conversation evaluatingsteps 4\tloss 2.0747\tvalid loss 2.2899 | exec time 9111.7532\n",
      "Epoch 5 start\n",
      "20003/20003 conversation evaluating...steps 5\tloss 1.8689\tvalid loss 2.1725 | exec time 9441.9608\n",
      "Epoch 6 start\n",
      "20003/20003 conversation evaluatingsteps 6\tloss 1.3530\tvalid loss 2.1219 | exec time 9139.9438\n",
      "Model Saved\n",
      "Epoch 7 start\n",
      "20003/20003 conversation evaluatingsteps 7\tloss 1.8298\tvalid loss 2.0835 | exec time 9117.3484\n",
      "Model Saved\n",
      "Epoch 8 start\n",
      "20003/20003 conversation evaluatingsteps 8\tloss 2.3718\tvalid loss 2.1801 | exec time 9219.8426\n",
      "Epoch 9 start\n",
      "20003/20003 conversation evaluatingsteps 9\tloss 1.1459\tvalid loss 2.4559 | exec time 9243.3131\n",
      "Epoch 10 start\n",
      "20003/20003 conversation evaluatingsteps 10\tloss 1.8675\tvalid loss 2.2297 | exec time 9112.8206\n",
      "Epoch 11 start\n",
      "Conversation 74416/160028 training..."
     ]
    }
   ],
   "source": [
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "Train(model_name, loss_name, model, opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルノ評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_x, da_x1, da_x2, da_x3, da_x4, da_y, da_y1, da_y2, da_y3, da_y4 = create_DAdata(config, model_name+loss_name)\n",
    "_, _, _, _, XDA_test, YDA_test = separate_data_da(da_x, da_y)\n",
    "_, _, _, _, XDA1_test, YDA1_test = separate_data_da(da_x1, da_y1)\n",
    "_, _, _, _, XDA2_test, YDA2_test = separate_data_da(da_x2, da_y2)\n",
    "_, _, _, _, XDA3_test, YDA3_test = separate_data_da(da_x3, da_y3)\n",
    "_, _, _, _, XDA4_test, YDA4_test = separate_data_da(da_x4, da_y4)\n",
    "\n",
    "XDA_test, YDA_test = DA_vocab.tokenize(XDA_test, YDA_test)\n",
    "XDA1_test, YDA1_test = DA1_vocab.tokenize(XDA1_test, YDA1_test)\n",
    "XDA2_test, YDA2_test = DA2_vocab.tokenize(XDA2_test, YDA2_test)\n",
    "XDA3_test, YDA3_test = DA3_vocab.tokenize(XDA3_test, YDA3_test)\n",
    "XDA4_test, YDA4_test = DA4_vocab.tokenize(XDA4_test, YDA4_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, _, XUtt_test, YUtt_test, _, _, turn = create_Uttdata(config, model_name+loss_name)\n",
    "\n",
    "XUtt_test, _ = Utt_vocab.tokenize(XUtt_test, YUtt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        os.path.join(config[model_name+loss_name]['log_dir'], \n",
    "                     config[model_name+loss_name]['SAVE_NAME'] + \"_v1\" + str(config[model_name+loss_name]['window_size']) + \"_v2\" + \".model\".format('model_save'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推定結果の評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "\n",
    "def ModelTest(models):\n",
    "    result=[]\n",
    "    for seq_idx in range(0, len(XDA_test)):\n",
    "        print('\\r{}/{} conversation evaluating'.format(seq_idx+1, len(XDA_test)), end='')\n",
    "        \n",
    "        X_seq = XDA_test[seq_idx]\n",
    "        Y1_seq = YDA1_test[seq_idx]\n",
    "        Y2_seq = YDA2_test[seq_idx]\n",
    "        Y3_seq = YDA3_test[seq_idx]\n",
    "        Y4_seq = YDA4_test[seq_idx]\n",
    "        \n",
    "        turn[seq_idx] = turn[seq_idx] + [0] * (len(X_seq) - len(turn[seq_idx]))\n",
    "        turn_seq = turn[seq_idx]\n",
    "        XU_seq = XUtt_test[seq_idx]\n",
    "\n",
    "        pred1_seq = []\n",
    "        true1_seq = []\n",
    "        pred2_seq = []\n",
    "        true2_seq = []\n",
    "        pred3_seq = []\n",
    "        true3_seq = []\n",
    "        pred4_seq = []\n",
    "        true4_seq = []\n",
    "        \n",
    "        utter_hidden, context_hidden, da_hidden = models.initDAHidden(1)\n",
    "\n",
    "        for i in range(0, len(X_seq)):\n",
    "            X_tensor = torch.tensor([[X_seq[i]]]).to(device)\n",
    "            Y1_tensor = torch.tensor([[Y1_seq[i]]]).to(device)\n",
    "            Y2_tensor = torch.tensor([[Y2_seq[i]]]).to(device)\n",
    "            Y3_tensor = torch.tensor([[Y3_seq[i]]]).to(device)\n",
    "            Y4_tensor = torch.tensor([[Y4_seq[i]]]).to(device)\n",
    "            turn_tensor = torch.tensor([[turn_seq[i]]]).to(device)\n",
    "            turn_tensor = turn_tensor.float()\n",
    "            turn_tensor = turn_tensor.unsqueeze(1)   \n",
    "            XU_tensor = torch.tensor([XU_seq[i]]).to(device)\n",
    "\n",
    "            level1_pred, level2_pred, level3_pred, level4_pred, context_hidden, da_hidden = model.prediction(XU_tensor, X_tensor, None, context_hidden, da_hidden, turn_tensor)\n",
    "            \n",
    "        pred_list=[level1_pred, level2_pred, level3_pred, level4_pred]\n",
    "        pred_index=[torch.tensor(-1)]*4\n",
    "        true_list=[Y1_tensor, Y2_tensor, Y3_tensor, Y4_tensor]\n",
    "        true_index=[torch.tensor(-1)]*4\n",
    "        \n",
    "        for i in range(0,4):\n",
    "            if pred_list[i]==None:\n",
    "                break\n",
    "            pred_index[i] = torch.argmax(pred_list[i])\n",
    "            true_index[i] = true_list[i].squeeze(1)\n",
    "    \n",
    "        pred1_seq.append(pred_index[0].item())\n",
    "        pred2_seq.append(pred_index[1].item())\n",
    "        pred3_seq.append(pred_index[2].item())\n",
    "        pred4_seq.append(pred_index[3].item())\n",
    "        \n",
    "        true1_seq.append(true_index[0].item())\n",
    "        true2_seq.append(true_index[1].item())\n",
    "        true3_seq.append(true_index[2].item())\n",
    "        true4_seq.append(true_index[3].item())\n",
    "        \n",
    "        result.append({\n",
    "            'true1': true1_seq,\n",
    "            'true2': true2_seq,\n",
    "            'true3': true3_seq,\n",
    "            'true4': true4_seq,\n",
    "            'pred1': pred1_seq,\n",
    "            'pred2': pred2_seq,\n",
    "            'pred3': pred3_seq,\n",
    "            'pred4': pred4_seq\n",
    "        })\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クラスタリング結果の評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "\n",
    "def ModelTestClustering(models):\n",
    "    result=[]\n",
    "    for seq_idx in range(0, len(XDA_test)):\n",
    "        print('\\r{}/{} conversation evaluating'.format(seq_idx+1, len(XDA_test)), end='')\n",
    "        XDA_seq = XDA_test[seq_idx]\n",
    "        YDA_seq = YDA_test[seq_idx]\n",
    "        DAturn[seq_idx] = DAturn[seq_idx] + [0] * (len(XDA_seq) - len(DAturn[seq_idx]))\n",
    "        DAturn_seq = DAturn[seq_idx]\n",
    "        XUtt_seq = XUtt_test[seq_idx]\n",
    "\n",
    "        pred_seq = []\n",
    "        true_seq = []\n",
    "        turn_seq = []\n",
    "        utter_hidden, context_hidden, da_hidden = models.initDAHidden(1)\n",
    "\n",
    "        for i in range(0, len(XDA_seq)):\n",
    "            XDA_tensor = torch.tensor([[XDA_seq[i]]]).to(device)\n",
    "            YDA_tensor = torch.tensor(YDA_seq[i]).to(device)\n",
    "            DAturn_tensor = torch.tensor([[DAturn_seq[i]]]).to(device)\n",
    "            DAturn_tensor = DAturn_tensor.float()\n",
    "            DAturn_tensor = DAturn_tensor.unsqueeze(1)\n",
    "            XUtt_tensor = torch.tensor([XUtt_seq[i]]).to(device)\n",
    "\n",
    "            output, utter_hidden, context_hidden, da_hidden, att_weights = model.prediction(XUtt_tensor, XDA_tensor, None, utter_hidden, context_hidden, da_hidden, DAturn_tensor)\n",
    "\n",
    "            \n",
    "        x_numpy = output[-1].to('cpu').detach().numpy().copy()\n",
    "        result.append(x_numpy)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=ModelTest(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results=ModelTestClustering(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_average(y_true, y_pred):\n",
    "    p = precision_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    r = recall_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    f = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    acc = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    print('p: {} | r: {} | f: {} | acc: {}'.format(p, r, f, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evelu(result, loss_name):\n",
    "    true = [label for line in result for label in line['true']]\n",
    "    pred = [label for line in result for label in line['pred']]\n",
    "\n",
    "    calc_average(y_true=true, y_pred=pred)\n",
    "    f = f1_score(y_true=true, y_pred=pred, average=None)\n",
    "    r = recall_score(y_true=true, y_pred=pred, average=None)\n",
    "    p = precision_score(y_true=true, y_pred=pred, average=None)\n",
    "    \n",
    "    print(\"Recall\")\n",
    "    [print(DA_vocab.id2word[idx], score) for idx, score in zip(sorted(set(true)),r)]\n",
    "    print(\"Precision\")\n",
    "    [print(DA_vocab.id2word[idx], score) for idx, score in zip(sorted(set(true)),p)]\n",
    "    print(\"F-Score\")\n",
    "    [print(DA_vocab.id2word[idx], score) for idx, score in zip(sorted(set(true)),f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=[]\n",
    "trues=[]\n",
    "for re in result:\n",
    "    if re['true1'][0]==0:\n",
    "        trues.append(0)\n",
    "    else:\n",
    "        if re['true2'][0]==0:\n",
    "            trues.append(1)\n",
    "        else:\n",
    "            if re['true3'][0]==0:\n",
    "                trues.append(2)\n",
    "            else:\n",
    "                if re['true4'][0]!=-1:\n",
    "                    trues.append(re['true4'][0]+3)\n",
    "                    \n",
    "for re in result:\n",
    "    if re['pred1'][0]==0:\n",
    "        preds.append(0)\n",
    "    else:\n",
    "        if re['pred2'][0]==0:\n",
    "            preds.append(1)\n",
    "        else:\n",
    "            if re['pred3'][0]==0:\n",
    "                preds.append(2)\n",
    "            else:\n",
    "                if re['pred4'][0]!=-1:\n",
    "                    preds.append(re['pred4'][0]+3)                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Evelu(result, loss_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 混同行列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "confusion_matrix_path=\"./data/img/confusionmatrix/LabelCM.png\"\n",
    "\n",
    "def CreateConfusionMatrix(result):\n",
    "    \n",
    "    y_true = [label for line in result for label in line['true']]\n",
    "    y_pred = [label for line in result for label in line['pred']]\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize=\"pred\")\n",
    "    plt.figure(figsize=(40, 40))\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues')\n",
    "    plt.savefig(confusion_matrix_path)\n",
    "    \n",
    "\n",
    "# result=ModelTest(model)\n",
    "CreateConfusionMatrix(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 階層型クラスタリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_hierarchical_path=\"./data/img/clustering/hierarchical/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [xDA[-1] for xDA in XDA_test]\n",
    "\n",
    "output=np.array(results)\n",
    "target=np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_output = list(set(targets))\n",
    "swda_lists = [DA_vocab.id2word[i] for i in set_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_uniq = np.unique(target)\n",
    "target_result = []\n",
    "for label in label_uniq:\n",
    "    target_result.append(output[target==label].mean(axis=0))\n",
    "    \n",
    "methods = [\"single\", \"complete\", \"average\", \"weighted\",\"centroid\", \"median\", \"ward\"]\n",
    "\n",
    "for method in methods:\n",
    "    S = pdist(target_result)\n",
    "    Z = linkage(S, method=method)\n",
    "    fig = plt.figure(figsize=(30, 18))\n",
    "    ax = fig.add_subplot(1, 1, 1, title=\"Dendrogram\")\n",
    "    dendrogram(Z, labels=np.array(swda_lists))\n",
    "    c, d = cophenet(Z,S)\n",
    "    print(\"{0} {1:.3f}\".format(method, c))\n",
    "    # x 軸のラベルを設定する。\n",
    "    ax.set_xlabel(\"Dialogue ACT Label\")\n",
    "    # y 軸のラベルを設定する。\n",
    "    ax.set_ylabel(\"threshold\")\n",
    "    plt.show()\n",
    "    plt.savefig(clustering_hierarchical_path + \"SwDA_hierarchy_dendrogram_Responce_Mean_{}_AllLabel.png\".format(method))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tSNEクラスタリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_tSNE_path=\"./data/img/clustering/tSNE/\"\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "targets = [xDA[-1] for xDA in XDA_test]\n",
    "output=np.array(results)\n",
    "target=np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tSNE_Visualization(output, y_target, loss_name):\n",
    "    \n",
    "    if not os.path.isdir(clustering_tSNE_path):\n",
    "        os.makedirs(clustering_tSNE_path)\n",
    "        \n",
    "    tsne = TSNE(n_components=2, random_state=41, n_iter=10000, perplexity=50.0, early_exaggeration=20.0, init='pca')\n",
    "    X_reduced = tsne.fit_transform(output)\n",
    "\n",
    "    f, ax = plt.subplots(1, 1, figsize=(13, 7))\n",
    "    for idx in range(len(DA_vocab.word2id)):\n",
    "        targets=X_reduced[target==idx]\n",
    "        plt.scatter(targets[:, 0], targets[:, 1],\n",
    "                    label=DA_vocab.id2word[idx],\n",
    "                    cmap='jet',\n",
    "                    s=15, alpha=0.5)\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left')  \n",
    "    \n",
    "    plt.savefig(clustering_tSNE_path + \"SwDA_LF{0}_Hierarchical_Label.png\".format(loss_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tSNE_Visualization(output, target, loss_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 各対話行為タグをtSNEによる次元削減"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_tSNE_path=\"./data/img/clustering/tSNE/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tSNE_Visualization_Utterance(output, target, da_name):\n",
    "    \n",
    "    if not os.path.isdir(clustering_tSNE_path):\n",
    "        os.makedirs(clustering_tSNE_path)\n",
    "        \n",
    "    tsne = TSNE(n_components=2, random_state=41, n_iter=10000, perplexity=50.0, early_exaggeration=20.0, init='pca')\n",
    "    X_reduced = tsne.fit_transform(output)\n",
    "\n",
    "    f, ax = plt.subplots(1, 1, figsize=(13, 7))\n",
    "    \n",
    "    idx = DA_vocab.word2id[da_name]\n",
    "    \n",
    "    targets=X_reduced[target==idx]\n",
    "    plt.scatter(targets[:, 0], targets[:, 1],\n",
    "                label=da_name,\n",
    "                cmap='jet',\n",
    "                s=15, alpha=0.5)\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left')  \n",
    "    \n",
    "    plt.savefig(clustering_tSNE_path + \"SwDA_DA_{0}.png\".format(da_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for d in DA_vocab.word2id:\n",
    "#     if DA_vocab.word2id[d] in target:\n",
    "    tSNE_Visualization_Utterance(output, target, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "loss = nn.BCELoss()\n",
    "input = torch.randn(3, requires_grad=True)\n",
    "target = torch.empty(3).random_(2)\n",
    "print(input, target)\n",
    "output = loss(m(input), target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m(input),target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
